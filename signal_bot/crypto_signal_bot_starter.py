# -*- coding: utf-8 -*-
"""crypto_signal_bot_starter.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ZA-Uz99AC5p0iMDhDETOkmx_eqS4xSa5

# ðŸš€ Crypto Signal Bot â€” Google Colab Starter Notebook
"""

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive')

# Set data path
DATA_DIR = '/content/drive/MyDrive/crypto_signal_bot/data'

# Create directory
import os
os.makedirs(DATA_DIR, exist_ok=True)

# Run the Streamlit dashboard again
!streamlit run signal_bot/dashboard.py

# Run the Streamlit dashboard again with updated code
!streamlit run signal_bot/dashboard.py

# Execute the main script again to ensure data files are up to date
!python -m signal_bot.main

# Install Streamlit
!pip install streamlit

# Install dependencies
!pip install pandas ta scikit-learn

# Imports
import pandas as pd
import requests
import ta

"""## ðŸª™ Top 10 Coin Data Fetch"""

def fetch_top10_market_data():
    url = "https://api.coingecko.com/api/v3/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": 10,
        "page": 1,
        "sparkline": "true"
    }
    response = requests.get(url, params=params)
    data = response.json()
    df = pd.DataFrame(data)
    df["timestamp"] = pd.Timestamp.utcnow()
    return df

df_top10 = fetch_top10_market_data()
df_top10.to_csv(f"{DATA_DIR}/top10_market_data.csv", index=False)
df_top10.head()

"""## ðŸ“Š Technical Indicators"""

def compute_indicators(df):
    df["close"] = df["current_price"]
    df["rsi"] = ta.momentum.RSIIndicator(df["close"]).rsi()
    df["ema_20"] = ta.trend.EMAIndicator(df["close"], window=20).ema_indicator()
    df["macd_diff"] = ta.trend.MACD(df["close"]).macd_diff()
    bb = ta.volatility.BollingerBands(df["close"], window=20)
    df["bb_upper"] = bb.bollinger_hband()
    df["bb_lower"] = bb.bollinger_lband()
    return df

df_ind = compute_indicators(df_top10)
df_ind.to_csv(f"{DATA_DIR}/top10_with_indicators.csv", index=False)
df_ind[["symbol", "rsi", "ema_20", "macd_diff", "bb_upper", "bb_lower"]].head()

"""## ðŸ“Ž Gemini Prompt Example

Paste this in Gemini:

> Here's my function that fetches top 10 coin data and adds RSI, MACD, EMA, and Bollinger Bands. Can you suggest optimizations or extra indicators for high-volatility detection?

## ðŸ“ˆ Signal Finder
"""

def generate_signals(df):
    signals = []
    for _, row in df.iterrows():
        sig = []
        if row["rsi"] < 30:
            sig.append("BUY_RSI")
        if row["rsi"] > 70:
            sig.append("SELL_RSI")
        if row["current_price"] < row["bb_lower"]:
            sig.append("BREAKOUT_WATCH")
        signals.append(", ".join(sig) if sig else "HOLD")
    df["signal"] = signals
    return df

df_signals = generate_signals(df_ind)
df_signals.to_csv(f"{DATA_DIR}/signals.csv", index=False)
df_signals[["symbol", "rsi", "macd_diff", "signal"]].head()

"""## ðŸ§  ML Logger"""

# Append features to ML training log
def log_ml_features(df, path):
    features = df[[
        "id", "timestamp", "current_price",
        "rsi", "ema_20", "macd_diff",
        "bb_upper", "bb_lower", "signal"
    ]]
    if os.path.exists(path):
        df_old = pd.read_csv(path)
        df_new = pd.concat([df_old, features]).drop_duplicates()
    else:
        df_new = features
    df_new.to_csv(path, index=False)

log_ml_features(df_signals, f"{DATA_DIR}/ml_training.csv")

# Execute the main script
!python signal_bot/main.py

# Modify the signals/generate_signals.py file to return a DataFrame, removing the shell command.
with open('signal_bot/signals/generate_signals.py', 'w') as f:
    f.write("""# signals/generate_signals.py
import pandas as pd
import ta

def generate_signal(df):
    \"\"\"
    Generates trading signals based on technical indicators.

    Args:
        df (pd.DataFrame): DataFrame with technical indicators.

    Returns:
        pd.DataFrame: DataFrame with 'signal' column added.
    \"\"\"
    # Ensure the DataFrame is not empty and has required columns
    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']
    if df.empty or not all(col in df.columns for col in required_cols):
        df['signal'] = 'HOLD'
        return df[['signal']]

    # Initialize signal column
    df['signal'] = 'HOLD'

    # Apply RSI signals
    df.loc[df['rsi'] < 30, 'signal'] = df.loc[df['rsi'] < 30, 'signal'].apply(lambda x: (x + ', BUY_RSI').strip(', ') if x != 'HOLD' else 'BUY_RSI')
    df.loc[df['rsi'] > 70, 'signal'] = df.loc[df['rsi'] > 70, 'signal'].apply(lambda x: (x + ', SELL_RSI').strip(', ') if x != 'HOLD' else 'SELL_RSI')

    # Apply Bollinger Bands signals
    df.loc[df['close'] < df['bb_lower'], 'signal'] = df.loc[df['close'] < df['bb_lower'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_LOWER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_LOWER')
    df.loc[df['close'] > df['bb_upper'], 'signal'] = df.loc[df['close'] > df['bb_upper'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_UPPER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_UPPER')

    # Simplified MACD signal (crossing above/below 0) - requires previous data
    if len(df) > 1:
        df['macd_diff_prev'] = df['macd_diff'].shift(1)
        df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'] = df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'].apply(lambda x: (x + ', BUY_MACD_CROSS').strip(', ') if x != 'HOLD' else 'BUY_MACD_CROSS')
        df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'] = df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'].apply(lambda x: (x + ', SELL_MACD_CROSS').strip(', ') if x != 'HOLD' else 'SELL_MACD_CROSS')
        df = df.drop(columns=['macd_diff_prev'])


    return df[['signal']] # Return only the signal column as a DataFrame
"""
)

# Modify the signals/generate_signals.py file to return a DataFrame with corrected syntax.
with open('signal_bot/signals/generate_signals.py', 'w') as f:
    f.write("""# signals/generate_signals.py
import pandas as pd
import ta

def generate_signal(df):
    \"\"\"
    Generates trading signals based on technical indicators.

    Args:
        df (pd.DataFrame): DataFrame with technical indicators.

    Returns:
        pd.DataFrame: DataFrame with 'signal' column added.
    \"\"\"
    # Ensure the DataFrame is not empty and has required columns
    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']
    if df.empty or not all(col in df.columns for col in required_cols):
        df['signal'] = 'HOLD'
        return df[['signal']]

    # Initialize signal column
    df['signal'] = 'HOLD'

    # Apply RSI signals
    df.loc[df['rsi'] < 30, 'signal'] = df.loc[df['rsi'] < 30, 'signal'].apply(lambda x: (x + ', BUY_RSI').strip(', ') if x != 'HOLD' else 'BUY_RSI')
    df.loc[df['rsi'] > 70, 'signal'] = df.loc[df['rsi'] > 70, 'signal'].apply(lambda x: (x + ', SELL_RSI').strip(', ') if x != 'HOLD' else 'SELL_RSI')

    # Apply Bollinger Bands signals
    df.loc[df['close'] < df['bb_lower'], 'signal'] = df.loc[df['close'] < df['bb_lower'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_LOWER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_LOWER')
    df.loc[df['close'] > df['bb_upper'], 'signal'] = df.loc[df['close'] > df['bb_upper'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_UPPER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_UPPER')

    # Simplified MACD signal (crossing above/below 0) - requires previous data
    if len(df) > 1:
        df['macd_diff_prev'] = df['macd_diff'].shift(1)
        df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'] = df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'].apply(lambda x: (x + ', BUY_MACD_CROSS').strip(', ') if x != 'HOLD' else 'BUY_MACD_CROSS')
        df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'] = df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'].apply(lambda x: (x + ', SELL_MACD_CROSS').strip(', ') if x != 'HOLD' else 'SELL_MACD_CROSS')
        df = df.drop(columns=['macd_diff_prev'])


    return df[['signal']] # Return only the signal column as a DataFrame

# Execute the main script again to test the fix
!python signal_bot/main.py
"""
)

# Execute the main script
!python signal_bot/main.py

# Modify the signals/generate_signals.py file to return a DataFrame, removing the shell command.
with open('signal_bot/signals/generate_signals.py', 'w') as f:
    f.write("""# signals/generate_signals.py
import pandas as pd
import ta

def generate_signal(df):
    \"\"\"
    Generates trading signals based on technical indicators.

    Args:
        df (pd.DataFrame): DataFrame with technical indicators.

    Returns:
        pd.DataFrame: DataFrame with 'signal' column added.
    \"\"\"
    # Ensure the DataFrame is not empty and has required columns
    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']
    if df.empty or not all(col in df.columns for col in required_cols):
        df['signal'] = 'HOLD'
        return df[['signal']]

    # Initialize signal column
    df['signal'] = 'HOLD'

    # Apply RSI signals
    df.loc[df['rsi'] < 30, 'signal'] = df.loc[df['rsi'] < 30, 'signal'].apply(lambda x: (x + ', BUY_RSI').strip(', ') if x != 'HOLD' else 'BUY_RSI')
    df.loc[df['rsi'] > 70, 'signal'] = df.loc[df['rsi'] > 70, 'signal'].apply(lambda x: (x + ', SELL_RSI').strip(', ') if x != 'HOLD' else 'SELL_RSI')

    # Apply Bollinger Bands signals
    df.loc[df['close'] < df['bb_lower'], 'signal'] = df.loc[df['close'] < df['bb_lower'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_LOWER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_LOWER')
    df.loc[df['close'] > df['bb_upper'], 'signal'] = df.loc[df['close'] > df['bb_upper'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_UPPER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_UPPER')

    # Simplified MACD signal (crossing above/below 0) - requires previous data
    if len(df) > 1:
        df['macd_diff_prev'] = df['macd_diff'].shift(1)
        df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'] = df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'].apply(lambda x: (x + ', BUY_MACD_CROSS').strip(', ') if x != 'HOLD' else 'BUY_MACD_CROSS')
        df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'] = df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'].apply(lambda x: (x + ', SELL_MACD_CROSS').strip(', ') if x != 'HOLD' else 'SELL_MACD_CROSS')
        df = df.drop(columns=['macd_diff_prev'])


    return df[['signal']] # Return only the signal column as a DataFrame
""")

# Execute the main script again to test the fix
# Removed: !python signal_bot/main.py

# Run the Streamlit dashboard
!streamlit run signal_bot/dashboard.py

# List files in the data directory to verify outputs
import os

DATA_DIR = 'signal_bot/data'

if os.path.exists(DATA_DIR):
    print(f"Files in {DATA_DIR}:")
    for filename in os.listdir(DATA_DIR):
        print(filename)
else:
    print(f"Error: Data directory not found at {DATA_DIR}.")

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the dashboard.py file
with open('signal_bot/dashboard.py', 'w') as f:
    f.write('''# dashboard.py
import streamlit as st
import pandas as pd
import os # Import os for path handling

# Define DATA_DIR
DATA_DIR = 'signal_bot/data'

st.title("Crypto Signal Dashboard")

tab1, tab2, tab3 = st.tabs(["Signals", "Anomalies", "Indicators"])

# Ensure files exist before trying to read
signals_path = os.path.join(DATA_DIR, "top10_signals.csv") # Use top10_signals.csv
anomalies_dir = DATA_DIR # Anomalies are saved with timestamp, need to find the latest
indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv") # Use top10_with_indicators.csv

with tab1:
    st.subheader("Signal Feed (Top 10)")
    if os.path.exists(signals_path):
        try:
            signals = pd.read_csv(signals_path)
            st.dataframe(signals)
        except Exception as e:
            st.error(f"Error loading signals data: {e}")
    else:
        st.info(f"Signal data not found at {signals_path}. Run the main pipeline first.")


with tab2:
    st.subheader("Anomalies")
    # Find the latest anomaly file
    anomaly_files = [f for f in os.listdir(anomalies_dir) if f.startswith('anomalies_') and f.endswith('.csv')]
    anomaly_files.sort(reverse=True) # Sort to get the latest first

    if anomaly_files:
        latest_anomaly_file = os.path.join(anomalies_dir, anomaly_files[0])
        st.info(f"Displaying latest anomalies from: {latest_anomaly_file}")
        try:
            anomalies = pd.read_csv(latest_anomaly_file)
            st.dataframe(anomalies)
        except Exception as e:
             st.error(f"Error loading anomalies data from {latest_anomaly_file}: {e}")
    else:
        st.info(f"No anomaly files found in {anomalies_dir}. Run the main pipeline with anomaly detection enabled.")


with tab3:
    st.subheader("Top 10 Indicators")
    if os.path.exists(indicators_path):
        try:
            indicators = pd.read_csv(indicators_path)
            # Ensure required columns exist and are numeric before charting
            chart_cols = ["rsi", "macd_diff"]
            available_chart_cols = [col for col in chart_cols if col in indicators.columns and pd.api.types.is_numeric_dtype(indicators[col])]

            if available_chart_cols:
                 st.subheader("Indicator Trends (RSI, MACD Diff)")
                 # Check if there's enough data or if all values are NaN for charting
                 if not indicators[available_chart_cols].dropna().empty:
                      st.line_chart(indicators[available_chart_cols])
                 else:
                      st.info(f"Indicator data for charting ({available_chart_cols}) contains only NaN values or is empty. Cannot display chart.")
            elif chart_cols:
                 st.warning(f"Required numeric columns for charting not found or are not numeric: {chart_cols}")
            else:
                 st.info("No indicator data available for charting.")

            # Display the full indicator data
            st.subheader("Full Indicator Data")
            st.dataframe(indicators)


        except Exception as e:
            st.error(f"Error loading indicators data: {e}")
    else:
        st.info(f"Indicator data not found at {indicators_path}. Run the main pipeline first.")

# Instructions on how to run the dashboard
st.sidebar.subheader("How to run the dashboard:")
st.sidebar.markdown("1. Ensure you have Streamlit installed (`pip install streamlit`).")
st.sidebar.markdown("2. Run the main pipeline (`python -m signal_bot.main`) to generate data files.")
st.sidebar.markdown("3. Open a terminal in your project's root directory (`/content/` in Colab).")
st.sidebar.markdown("4. Run the command: `streamlit run signal_bot/dashboard.py`")
st.sidebar.markdown("5. A local URL will be provided to view the dashboard.")

''')

import pandas as pd
import os

# Define the path to the backtest results file
DATA_DIR = 'signal_bot/data'
backtest_results_path = os.path.join(DATA_DIR, "signal_backtest.csv")

# Check if the file exists before attempting to read
if os.path.exists(backtest_results_path):
    try:
        # Read the backtest results file
        backtest_results_df = pd.read_csv(backtest_results_path)

        # Display the head and some basic analysis of the results
        print(f"Content of {backtest_results_path}:")
        display(backtest_results_df.head().to_markdown(index=False))

        print("\nBasic Backtest Analysis:")
        total_signals = len(backtest_results_df)
        successful_trades = backtest_results_df['success'].sum()
        success_rate = (successful_trades / total_signals) * 100 if total_signals > 0 else 0

        print(f"Total BUY signals backtested: {total_signals}")
        print(f"Successful BUY signals (price increased by >= threshold): {successful_trades}")
        print(f"Success Rate: {success_rate:.2f}%")
        print(f"Average Return (%): {backtest_results_df['return_pct'].mean() * 100:.2f}%")


    except FileNotFoundError:
        print(f"Error: {backtest_results_path} not found. Please run main.py first to generate this file.")
    except Exception as e:
        print(f"Error reading or processing {backtest_results_path}: {e}")
else:
    print(f"Error: {backtest_results_path} not found. Please run main.py first to generate this file.")

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot', exist_ok=True)
os.makedirs('signal_bot/data', exist_ok=True) # Ensure data directory exists

# Write the content to the file
with open('signal_bot/data_collector.py', 'w') as f:
    f.write('''# signal_bot/data_collector.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins # Import get_top_coins

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def collect_data(limit=250):
    """
    Fetches a snapshot of market data for a specified number of top coins.

    Args:
        limit (int): The number of top coins to fetch data for.

    Returns:
        pd.DataFrame or None: DataFrame with market data if successful, None otherwise.
    """
    print(f"Fetching market data for top {limit} coins...")
    try:
        data = get_top_coins(limit=limit)
        if data is not None:
            df = pd.DataFrame(data)
            df["timestamp"] = pd.Timestamp.utcnow()
            full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
            df.to_csv(full_snapshot_path, index=False)
            print(f"Full market snapshot fetched and saved to {full_snapshot_path}.")
            return df
        else:
            print("Failed to fetch market data for full snapshot.")
            return None
    except Exception as e:
        print(f"Error during market data collection: {e}")
        return None

if __name__ == '__main__':
    # Example of how to run the data collector as a script
    collect_data(limit=250) # Fetch data for top 250 coins for the snapshot
''')

# Execute the main script
!python -m signal_bot.main

# Write the content of signal_bot/main.py piece by piece to avoid string literal issues.
main_py_content_part1 = """# main.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins, get_coin_history
from .indicators.ta_utils import compute_indicators
from .signals.signal_finder import find_signals
from .anomaly_detector import detect_anomalies
from .ml_logger import log_ml_features
from .backtester import backtest_signals

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Starting bot pipeline...")

    # --- Data Collection (Top 10 Snapshot) ---
    print("Fetching top 10 coin data...")
    try:
        data_top10 = get_top_coins(limit=10)
        if data_top10 is not None:
            df_top10 = pd.DataFrame(data_top10)
            df_top10["timestamp"] = pd.Timestamp.utcnow()
            top10_market_data_path = os.path.join(DATA_DIR, "top10_market_data.csv")
            df_top10.to_csv(top10_market_data_path, index=False)
            print(f"Top 10 data fetched and saved to {top10_market_data_path}.")

            # --- Indicator Computation for Top 10 ---
            print("Computing technical indicators for top 10 data...")
            df_top10["close"] = df_top10["current_price"]
            df_ind_top10 = compute_indicators(df_top10.copy())
            top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
            df_ind_top10.to_csv(top10_indicators_path, index=False)
            print(f"Indicators computed and saved for top 10 data to {top10_indicators_path}.")

            # --- Signal Generation for Top 10 ---
            print("Generating signals for top 10 snapshot data...")
            df_signals_top10 = find_signals(df_ind_top10.copy())
            top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
            df_signals_top10.to_csv(top10_signals_path, index=False)
            print(f"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.")

            # --- ML Logging for Top 10 ---
            print("Logging ML features for top 10 data...")
            ml_log_path = os.path.join(DATA_DIR, "ml_training.csv")
            log_ml_features(top10_indicators_path, ml_log_path)
            print(f"ML features logged to {ml_log_path}.")

        else:
            print("Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.")
            df_signals_top10 = pd.DataFrame()

    except Exception as e:
        print(f"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.")
        df_signals_top10 = pd.DataFrame()

    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---
    print("\\nFetching full market snapshot for Anomaly Detection...")
    try:
        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot
        if data_full is not None:
            df_full_snapshot = pd.DataFrame(data_full)
            df_full_snapshot["timestamp"] = pd.Timestamp.utcnow()
            full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
            df_full_snapshot.to_csv(full_snapshot_path, index=False)
            print(f"Full market snapshot fetched and saved to {full_snapshot_path}.")
        else:
            print("Failed to fetch full market snapshot.")
    except Exception as e:
        print(f"Error fetching or saving full market snapshot: {e}.")


    # --- Anomaly Detection ---
    print("\\nAttempting to run Anomaly Detection...")
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if os.path.exists(full_snapshot_path):
        try:
            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
            print(f"Anomaly detection completed. Anomalies saved to {path}.")
            print(f"Found {len(anomalies)} anomalies.")
            if not anomalies.empty:
                 print("Sample anomalies:")
                 print(anomalies.head().to_markdown(index=False))
            else:
                 print("No anomalies detected.")
        except Exception as e:
            print(f"Error during Anomaly Detection: {e}.")
    else:
        print(f"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.")

    # --- Historical Data Processing ---
    coin_id = "bitcoin"
    history_df_signals_display = pd.DataFrame()
    historical_price_data_path = os.path.join(DATA_DIR, f"{coin_id}_historical_price.csv") # Define path for historical price data
    historical_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv") # Define path for historical signals

    print(f"\\nAttempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")
        if history_data is not None and "prices" in history_data:
            print("Historical data fetched.")
            history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "current_price"]) # Use current_price for consistency
            history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data
            print("Historical data processed and saved.")

            print(f"Computing technical indicators for {coin_id} historical data...")
            history_df["close"] = history_df["current_price"] # Use current_price as close for indicators
            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly
            history_historical_indicators_path = os.path.join(DATA_DIR, f"{coin_id}_historical_with_indicators.csv")
            history_df_ind.to_csv(history_historical_indicators_path, index=False)
            print(f"Indicators computed and saved for historical data to {history_historical_indicators_path}.")

            print(f"Generating signals for {coin_id} historical data...")
            history_df_signals = find_signals(history_df_ind.copy())
            history_df_signals.to_csv(historical_signals_path, index=False)
            print(f"Historical signals generated and saved to {historical_signals_path}.")
            history_df_signals_display = history_df_signals

            # --- Backtesting Historical Signals ---
            print(f"\\nAttempting to backtest historical signals for {coin_id}...")
            # backtest_signals expects paths to signal CSV and price CSV
            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)
            print("Backtesting completed.")
            if not backtest_results_df.empty:
                print("Backtest Results (head):")
                print(backtest_results_df.head().to_markdown(index=False))
            else:
                print("No backtest results to display.")


        else:
             print("Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.")

    except Exception as e:
        print(f"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.")


    # --- Display Results (for Top 10 snapshot) ---
    print("\\n--- Results (Top 10 Snapshot) ---")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False))
    else:
        print("No top 10 snapshot signals generated due to processing errors or no data.")

    # --- Display Results (for Historical, if processed) ---
    print("\\n--- Results (Historical Data) ---")
    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:
         print("Historical signals (tail):")
         print(history_df_signals_display.tail().to_markdown(index=False))
         print("\nHistorical signals (sample):")
         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))
    else:
         print("No historical signals generated or historical data processing skipped.")

    # Ensure the final print statement is complete
    print("\\nBot pipeline finished.")


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

# Execute the main script to test the fix and verify full snapshot fetching.
!python -m signal_bot.main

# Rewrite the content of signal_bot/main.py again, double-checking all string literals.
main_py_content = """# main.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins, get_coin_history
from .indicators.ta_utils import compute_indicators
from .signals.signal_finder import find_signals
from .anomaly_detector import detect_anomalies
from .ml_logger import log_ml_features
from .backtester import backtest_signals

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Starting bot pipeline...")

    # --- Data Collection (Top 10 Snapshot) ---
    print("Fetching top 10 coin data...")
    try:
        data_top10 = get_top_coins(limit=10)
        if data_top10 is not None:
            df_top10 = pd.DataFrame(data_top10)
            df_top10["timestamp"] = pd.Timestamp.utcnow()
            top10_market_data_path = os.path.join(DATA_DIR, "top10_market_data.csv")
            df_top10.to_csv(top10_market_data_path, index=False)
            print(f"Top 10 data fetched and saved to {top10_market_data_path}.")

            # --- Indicator Computation for Top 10 ---
            print("Computing technical indicators for top 10 data...")
            df_top10["close"] = df_top10["current_price"]
            df_ind_top10 = compute_indicators(df_top10.copy())
            top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
            df_ind_top10.to_csv(top10_indicators_path, index=False)
            print(f"Indicators computed and saved for top 10 data to {top10_indicators_path}.")

            # --- Signal Generation for Top 10 ---
            print("Generating signals for top 10 snapshot data...")
            df_signals_top10 = find_signals(df_ind_top10.copy())
            top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
            df_signals_top10.to_csv(top10_signals_path, index=False)
            print(f"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.")

            # --- ML Logging for Top 10 ---
            print("Logging ML features for top 10 data...")
            ml_log_path = os.path.join(DATA_DIR, "ml_training.csv")
            log_ml_features(top10_indicators_path, ml_log_path)
            print(f"ML features logged to {ml_log_path}.")

        else:
            print("Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.")
            df_signals_top10 = pd.DataFrame()

    except Exception as e:
        print(f"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.")
        df_signals_top10 = pd.DataFrame()

    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---
    print("\\nFetching full market snapshot for Anomaly Detection...")
    try:
        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot
        if data_full is not None:
            df_full_snapshot = pd.DataFrame(data_full)
            df_full_snapshot["timestamp"] = pd.Timestamp.utcnow()
            full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
            df_full_snapshot.to_csv(full_snapshot_path, index=False)
            print(f"Full market snapshot fetched and saved to {full_snapshot_path}.")
        else:
            print("Failed to fetch full market snapshot.")
    except Exception as e:
        print(f"Error fetching or saving full market snapshot: {e}.")


    # --- Anomaly Detection ---
    print("\\nAttempting to run Anomaly Detection...")
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if os.path.exists(full_snapshot_path):
        try:
            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
            print(f"Anomaly detection completed. Anomalies saved to {path}.")
            print(f"Found {len(anomalies)} anomalies.")
            if not anomalies.empty:
                 print("Sample anomalies:")
                 print(anomalies.head().to_markdown(index=False))
            else:
                 print("No anomalies detected.")
        except Exception as e:
            print(f"Error during Anomaly Detection: {e}.")
    else:
        print(f"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.")

    # --- Historical Data Processing ---
    coin_id = "bitcoin"
    history_df_signals_display = pd.DataFrame()
    historical_price_data_path = os.path.join(DATA_DIR, f"{coin_id}_historical_price.csv") # Define path for historical price data
    historical_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv") # Define path for historical signals

    print(f"\\nAttempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")
        if history_data is not None and "prices" in history_data:
            print("Historical data fetched.")
            history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "current_price"]) # Use current_price for consistency
            history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data
            print("Historical data processed and saved.")

            print(f"Computing technical indicators for {coin_id} historical data...")
            history_df["close"] = history_df["current_price"] # Use current_price as close for indicators
            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly
            history_historical_indicators_path = os.path.join(DATA_DIR, f"{coin_id}_historical_with_indicators.csv")
            history_df_ind.to_csv(history_historical_indicators_path, index=False)
            print(f"Indicators computed and saved for historical data to {history_historical_indicators_path}.")

            print(f"Generating signals for {coin_id} historical data...")
            history_df_signals = find_signals(history_df_ind.copy())
            history_df_signals.to_csv(historical_signals_path, index=False)
            print(f"Historical signals generated and saved to {historical_signals_path}.")
            history_df_signals_display = history_df_signals

            # --- Backtesting Historical Signals ---
            print(f"\\nAttempting to backtest historical signals for {coin_id}...")
            # backtest_signals expects paths to signal CSV and price CSV
            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)
            print("Backtesting completed.")
            if not backtest_results_df.empty:
                print("Backtest Results (head):")
                print(backtest_results_df.head().to_markdown(index=False))
            else:
                print("No backtest results to display.")


        else:
             print("Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.")

    except Exception as e:
        print(f"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.")


    # --- Display Results (for Top 10 snapshot) ---
    print("\\n--- Results (Top 10 Snapshot) ---")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False))
    else:
        print("No top 10 snapshot signals generated due to processing errors or no data.")

    # --- Display Results (for Historical, if processed) ---
    print("\\n--- Results (Historical Data) ---")
    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:
         print("Historical signals (tail):")
         print(history_df_signals_display.tail().to_markdown(index=False))
         print("\nHistorical signals (sample):")
         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))
    else:
         print("No historical signals generated or historical data processing skipped.")


    print("\\nBot pipeline finished.")


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

# Execute the main script to test the fix and verify full snapshot fetching.
!python -m signal_bot.main

# Rewrite the content of signal_bot/main.py with the complete and correct string literal.
main_py_content = """# main.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins, get_coin_history
from .indicators.ta_utils import compute_indicators
from .signals.signal_finder import find_signals
from .anomaly_detector import detect_anomalies
from .ml_logger import log_ml_features
from .backtester import backtest_signals

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Starting bot pipeline...")

    # --- Data Collection (Top 10 Snapshot) ---
    print("Fetching top 10 coin data...")
    try:
        data_top10 = get_top_coins(limit=10)
        if data_top10 is not None:
            df_top10 = pd.DataFrame(data_top10)
            df_top10["timestamp"] = pd.Timestamp.utcnow()
            top10_market_data_path = os.path.join(DATA_DIR, "top10_market_data.csv")
            df_top10.to_csv(top10_market_data_path, index=False)
            print(f"Top 10 data fetched and saved to {top10_market_data_path}.")

            # --- Indicator Computation for Top 10 ---
            print("Computing technical indicators for top 10 data...")
            df_top10["close"] = df_top10["current_price"]
            df_ind_top10 = compute_indicators(df_top10.copy())
            top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
            df_ind_top10.to_csv(top10_indicators_path, index=False)
            print(f"Indicators computed and saved for top 10 data to {top10_indicators_path}.")

            # --- Signal Generation for Top 10 ---
            print("Generating signals for top 10 snapshot data...")
            df_signals_top10 = find_signals(df_ind_top10.copy())
            top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
            df_signals_top10.to_csv(top10_signals_path, index=False)
            print(f"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.")

            # --- ML Logging for Top 10 ---
            print("Logging ML features for top 10 data...")
            ml_log_path = os.path.join(DATA_DIR, "ml_training.csv")
            log_ml_features(top10_indicators_path, ml_log_path)
            print(f"ML features logged to {ml_log_path}.")

        else:
            print("Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.")
            df_signals_top10 = pd.DataFrame()

    except Exception as e:
        print(f"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.")
        df_signals_top10 = pd.DataFrame()

    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---
    print("\\nFetching full market snapshot for Anomaly Detection...")
    try:
        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot
        if data_full is not None:
            df_full_snapshot = pd.DataFrame(data_full)
            df_full_snapshot["timestamp"] = pd.Timestamp.utcnow()
            full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
            df_full_snapshot.to_csv(full_snapshot_path, index=False)
            print(f"Full market snapshot fetched and saved to {full_snapshot_path}.")
        else:
            print("Failed to fetch full market snapshot.")
    except Exception as e:
        print(f"Error fetching or saving full market snapshot: {e}.")


    # --- Anomaly Detection ---
    print("\\nAttempting to run Anomaly Detection...")
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if os.path.exists(full_snapshot_path):
        try:
            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
            print(f"Anomaly detection completed. Anomalies saved to {path}.")
            print(f"Found {len(anomalies)} anomalies.")
            if not anomalies.empty:
                 print("Sample anomalies:")
                 print(anomalies.head().to_markdown(index=False))
            else:
                 print("No anomalies detected.")
        except Exception as e:
            print(f"Error during Anomaly Detection: {e}.")
    else:
        print(f"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.")

    # --- Historical Data Processing ---
    coin_id = "bitcoin"
    history_df_signals_display = pd.DataFrame()
    historical_price_data_path = os.path.join(DATA_DIR, f"{coin_id}_historical_price.csv") # Define path for historical price data
    historical_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv") # Define path for historical signals

    print(f"\\nAttempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")
        if history_data is not None and "prices" in history_data:
            print("Historical data fetched.")
            history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "current_price"]) # Use current_price for consistency
            history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data
            print("Historical data processed and saved.")

            print(f"Computing technical indicators for {coin_id} historical data...")
            history_df["close"] = history_df["current_price"] # Use current_price as close for indicators
            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly
            history_historical_indicators_path = os.path.join(DATA_DIR, f"{coin_id}_historical_with_indicators.csv")
            history_df_ind.to_csv(history_historical_indicators_path, index=False)
            print(f"Indicators computed and saved for historical data to {history_historical_indicators_path}.")

            print(f"Generating signals for {coin_id} historical data...")
            history_df_signals = find_signals(history_df_ind.copy())
            history_df_signals.to_csv(historical_signals_path, index=False)
            print(f"Historical signals generated and saved to {historical_signals_path}.")
            history_df_signals_display = history_df_signals

            # --- Backtesting Historical Signals ---
            print(f"\\nAttempting to backtest historical signals for {coin_id}...")
            # backtest_signals expects paths to signal CSV and price CSV
            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)
            print("Backtesting completed.")
            if not backtest_results_df.empty:
                print("Backtest Results (head):")
                print(backtest_results_df.head().to_markdown(index=False))
            else:
                print("No backtest results to display.")


        else:
             print("Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.")

    except Exception as e:
        print(f"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.")


    # --- Display Results (for Top 10 snapshot) ---
    print("\\n--- Results (Top 10 Snapshot) ---")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False))
    else:
        print("No top 10 snapshot signals generated due to processing errors or no data.")

    # --- Display Results (for Historical, if processed) ---
    print("\\n--- Results (Historical Data) ---")
    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:
         print("Historical signals (tail):")
         print(history_df_signals_display.tail().to_markdown(index=False))
         print("\nHistorical signals (sample):")
         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))
    else:
         print("No historical signals generated or historical data processing skipped.")


    print("\\nBot pipeline finished.")


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

# Read the content of main.py to inspect the syntax error.
with open('signal_bot/main.py', 'r') as f:
    main_content = f.read()

print(main_content)

# Execute the main script
!python -m signal_bot.main

# Rewrite the content of signal_bot/main.py to include fetching and saving the full market snapshot.
main_py_content = """# main.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins, get_coin_history
from .indicators.ta_utils import compute_indicators
from .signals.signal_finder import find_signals
from .anomaly_detector import detect_anomalies
from .ml_logger import log_ml_features
from .backtester import backtest_signals

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Starting bot pipeline...")

    # --- Data Collection (Top 10 Snapshot) ---
    print("Fetching top 10 coin data...")
    try:
        data_top10 = get_top_coins(limit=10)
        if data_top10 is not None:
            df_top10 = pd.DataFrame(data_top10)
            df_top10["timestamp"] = pd.Timestamp.utcnow()
            top10_market_data_path = os.path.join(DATA_DIR, "top10_market_data.csv")
            df_top10.to_csv(top10_market_data_path, index=False)
            print(f"Top 10 data fetched and saved to {top10_market_data_path}.")

            # --- Indicator Computation for Top 10 ---
            print("Computing technical indicators for top 10 data...")
            df_top10["close"] = df_top10["current_price"]
            df_ind_top10 = compute_indicators(df_top10.copy())
            top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
            df_ind_top10.to_csv(top10_indicators_path, index=False)
            print(f"Indicators computed and saved for top 10 data to {top10_indicators_path}.")

            # --- Signal Generation for Top 10 ---
            print("Generating signals for top 10 snapshot data...")
            df_signals_top10 = find_signals(df_ind_top10.copy())
            top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
            df_signals_top10.to_csv(top10_signals_path, index=False)
            print(f"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.")

            # --- ML Logging for Top 10 ---
            print("Logging ML features for top 10 data...")
            ml_log_path = os.path.join(DATA_DIR, "ml_training.csv")
            log_ml_features(top10_indicators_path, ml_log_path)
            print(f"ML features logged to {ml_log_path}.")

        else:
            print("Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.")
            df_signals_top10 = pd.DataFrame()

    except Exception as e:
        print(f"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.")
        df_signals_top10 = pd.DataFrame()

    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---
    print("\\nFetching full market snapshot for Anomaly Detection...")
    try:
        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot
        if data_full is not None:
            df_full_snapshot = pd.DataFrame(data_full)
            df_full_snapshot["timestamp"] = pd.Timestamp.utcnow()
            full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
            df_full_snapshot.to_csv(full_snapshot_path, index=False)
            print(f"Full market snapshot fetched and saved to {full_snapshot_path}.")
        else:
            print("Failed to fetch full market snapshot.")
    except Exception as e:
        print(f"Error fetching or saving full market snapshot: {e}.")


    # --- Anomaly Detection ---
    print("\\nAttempting to run Anomaly Detection...")
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if os.path.exists(full_snapshot_path):
        try:
            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
            print(f"Anomaly detection completed. Anomalies saved to {path}.")
            print(f"Found {len(anomalies)} anomalies.")
            if not anomalies.empty:
                 print("Sample anomalies:")
                 print(anomalies.head().to_markdown(index=False))
            else:
                 print("No anomalies detected.")
        except Exception as e:
            print(f"Error during Anomaly Detection: {e}.")
    else:
        print(f"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.")

    # --- Historical Data Processing ---
    coin_id = "bitcoin"
    history_df_signals_display = pd.DataFrame()
    historical_price_data_path = os.path.join(DATA_DIR, f"{coin_id}_historical_price.csv") # Define path for historical price data
    historical_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv") # Define path for historical signals

    print(f"\\nAttempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")
        if history_data is not None and "prices" in history_data:
            print("Historical data fetched.")
            history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "current_price"]) # Use current_price for consistency
            history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data
            print("Historical data processed and saved.")

            print(f"Computing technical indicators for {coin_id} historical data...")
            history_df["close"] = history_df["current_price"] # Use current_price as close for indicators
            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly
            history_historical_indicators_path = os.path.join(DATA_DIR, f"{coin_id}_historical_with_indicators.csv")
            history_df_ind.to_csv(history_historical_indicators_path, index=False)
            print(f"Indicators computed and saved for historical data to {history_historical_indicators_path}.")

            print(f"Generating signals for {coin_id} historical data...")
            history_df_signals = find_signals(history_df_ind.copy())
            history_df_signals.to_csv(historical_signals_path, index=False)
            print(f"Historical signals generated and saved to {historical_signals_path}.")
            history_df_signals_display = history_df_signals

            # --- Backtesting Historical Signals ---
            print(f"\\nAttempting to backtest historical signals for {coin_id}...")
            # backtest_signals expects paths to signal CSV and price CSV
            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)
            print("Backtesting completed.")
            if not backtest_results_df.empty:
                print("Backtest Results (head):")
                print(backtest_results_df.head().to_markdown(index=False))
            else:
                print("No backtest results to display.")


        else:
             print("Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.")

    except Exception as e:
        print(f"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.")


    # --- Display Results (for Top 10 snapshot) ---
    print("\\n--- Results (Top 10 Snapshot) ---")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False))
    else:
        print("No top 10 snapshot signals generated due to processing errors or no data.")

    # --- Display Results (for Historical, if processed) ---
    print("\\n--- Results (Historical Data) ---")
    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:
         print("Historical signals (tail):")
         print(history_df_signals_display.tail().to_markdown(index=False))
         print("\nHistorical signals (sample):")
         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))
    else:
         print("No historical signals generated or historical data processing skipped.")


    print("\\nBot pipeline finished.")


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

import pandas as pd
import os

# Define the path to the historical signals file
DATA_DIR = 'signal_bot/data'
historical_signals_path = os.path.join(DATA_DIR, "bitcoin_historical_signals.csv")

# Check if the file exists before attempting to read
if os.path.exists(historical_signals_path):
    try:
        # Read the historical signals file
        historical_signals_df = pd.read_csv(historical_signals_path)

        # Display the head and value counts of the 'signal' column
        print(f"Content of {historical_signals_path}:")
        display(historical_signals_df.head().to_markdown(index=False))

        print("\nValue counts of 'signal' column:")
        display(historical_signals_df['signal'].value_counts().to_markdown())

        # Optionally, display a sample of rows with specific signals if needed
        # print("\nSample of BUY signals (if any):")
        # buy_signals_sample = historical_signals_df[historical_signals_df['signal'].str.contains('BUY', na=False)].sample(min(5, len(historical_signals_df[historical_signals_df['signal'].str.contains('BUY', na=False)])))
        # display(buy_signals_sample.to_markdown(index=False))


    except FileNotFoundError:
        print(f"Error: {historical_signals_path} not found.")
    except Exception as e:
        print(f"Error reading or processing {historical_signals_path}: {e}")
else:
    print(f"Error: {historical_signals_path} not found. Please run main.py first to generate this file.")

# Execute the main script
!python -m signal_bot.main

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot/models', exist_ok=True)

# Write the content to the file
with open('signal_bot/ml_model_trainer.py', 'w') as f:
    f.write('''# ml_model_trainer.py
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report

def train_ml_model(training_csv):
    df = pd.read_csv(training_csv)
    df = df.dropna()

    # Define success label (this would come from backtest or manual labeling)
    if "success" not in df.columns:
        raise ValueError("Expected 'success' column for supervised training.")

    X = df[["rsi", "ema_20", "macd_diff", "bb_upper", "bb_lower"]]
    y = df["success"].astype(int)

    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)
    model = RandomForestClassifier(random_state=42)
    model.fit(X_train, y_train)

    y_pred = model.predict(X_test)
    report = classification_report(y_test, y_pred, output_dict=True)
    return model, report
''')

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the file
with open('signal_bot/backtester.py', 'w') as f:
    f.write('''# backtester.py
import pandas as pd
import os # Import os for path handling

def backtest_signals(signal_csv, price_csv, threshold=0.05, window=6):
    """
    Backtests trading signals against historical price data.

    Args:
        signal_csv (str): Path to the CSV file containing signals.
        price_csv (str): Path to the CSV file containing historical price data.
        threshold (float): The percentage price increase considered a successful BUY signal.
        window (int): The number of future price points to consider after a signal.

    Returns:
        pd.DataFrame: DataFrame containing backtest results.
    """
    try:
        signals = pd.read_csv(signal_csv, parse_dates=["timestamp"])
        prices = pd.read_csv(price_csv, parse_dates=["timestamp"])
    except FileNotFoundError as e:
        print(f"Error loading data for backtesting: {e}")
        return pd.DataFrame() # Return empty DataFrame on error
    except Exception as e:
        print(f"Error reading CSV files for backtesting: {e}")
        return pd.DataFrame() # Return empty DataFrame on other errors

    print("\\n--- Inside backtest_signals ---") # Debug print
    print(f"Read signals CSV: {signal_csv}") # Debug print
    print("Signals DataFrame dtypes:\\n", signals.dtypes) # Debug print
    print("Signals DataFrame head:\\n", signals.head().to_markdown(index=False)) # Debug print


    results = []
    # Ensure data is sorted for correct future price lookup
    signals = signals.sort_values("timestamp")
    prices = prices.sort_values("timestamp")

    # Filter for BUY signals using a more explicit method
    # Ensure the 'signal' column is treated as string for explicit checking
    signals['signal'] = signals['signal'].astype(str)
    # Use a boolean mask with apply and lambda or list comprehension for filtering
    buy_signals = signals[signals['signal'].apply(lambda x: 'BUY' in x)].copy()


    print(f"Found {len(buy_signals)} BUY signals for backtesting after filtering.") # Debug print


    if buy_signals.empty:
        print("No BUY signals found for backtesting.")
        print("--- Exiting backtest_signals ---\\n") # Debug print
        return pd.DataFrame()

    for index, signal in buy_signals.iterrows(): # Iterate through BUY signals
        print(f"Backtesting signal: {signal['signal']} at {signal['timestamp']}") # Debug print
        coin_id = signal["id"]
        signal_time = signal["timestamp"]
        # Ensure 'current_price' column exists in the signals DataFrame if needed,
        # or get the price at the signal time from the prices DataFrame
        # Assuming signals DataFrame has 'current_price' for simplicity based on user code structure
        if "current_price" not in signal:
             # Fallback to prices DataFrame if current_price is not in signals
             # Need to ensure the prices DataFrame also has 'id' for lookup
             if "id" not in prices.columns:
                  print("Error: 'id' column not found in prices DataFrame for lookup.")
                  continue # Skip this signal
             price_at_signal_row = prices[
                 (prices["id"] == coin_id) &
                 (prices["timestamp"] == signal_time)
             ]
             if price_at_signal_row.empty:
                  print(f"Warning: Price at signal time not found for {coin_id} at {signal_time}. Skipping.")
                  continue
             price_at_signal = price_at_signal_row.iloc[0]["current_price"]
        else:
            price_at_signal = signal["current_price"]


        # Filter for future prices of this coin within the window
        future_prices = prices[
            (prices["id"] == coin_id) &
            (prices["timestamp"] > signal_time)
        ].sort_values("timestamp").head(window)

        print(f"Found {len(future_prices)} future price points within window.") # Debug print


        if not future_prices.empty:
            # Find the maximum price within the future window
            # Ensure 'current_price' column exists in future_prices
            if "current_price" not in future_prices.columns:
                 print("Error: 'current_price' column not found in future prices DataFrame.")
                 continue # Skip this signal
            max_price = future_prices["current_price"].max()
            return_pct = (max_price - price_at_signal) / price_at_signal

            result = {
                "id": coin_id,
                "timestamp": signal_time,
                "signal": signal["signal"], # Log the specific BUY signal
                "price_at_signal": price_at_signal, # Add price at signal for context
                "max_future_price": max_price, # Add max future price
                "return_pct": return_pct,
                "success": return_pct >= threshold
            }
            results.append(result)
        else:
             # Handle cases where no future price data is available within the window
             print(f"Warning: No future price data found within window for {coin_id} at {signal_time}. Skipping backtest for this signal.")
             # Optional: Log this signal with a specific status

    if not results:
         print("No backtest results generated (either no BUY signals processed or no successful trades).") # Refine message
         print("--- Exiting backtest_signals ---\\n") # Debug print
         return pd.DataFrame()

    result_df = pd.DataFrame(results)
    output_path = "data/signal_backtest.csv" # Define output path
    os.makedirs(os.path.dirname(output_path), exist_ok=True) # Ensure directory exists
    result_df.to_csv(output_path, index=False)
    print(f"Backtest results saved to {output_path}")
    print("--- Exiting backtest_signals ---\\n") # Debug print
    return result_df
''')

# Execute the scheduler script
!python -m signal_bot.scheduler

# Add apscheduler to requirements.txt
with open('signal_bot/requirements.txt', 'a') as f:
    f.write('apscheduler\n')

# Install dependencies from requirements.txt
!pip install -r signal_bot/requirements.txt

# Execute the scheduler script
!python -m signal_bot.scheduler

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python -m signal_bot.main

# Execute the main script
!python signal_bot/main.py

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the file - Temporarily modify to run only full_data_collection_job once
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


# print("Starting scheduler...")
# scheduler.start() # Comment out to prevent immediate blocking execution

# Manual test execution of full_data_collection_job once
setup_logger() # Ensure logger is setup
full_data_collection_job()
''')

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the file
with open('signal_bot/dataset_manager.py', 'w') as f:
    f.write('''# dataset_manager.py
import pandas as pd

def clean_and_normalize(df):
    """Clean and normalize DataFrame columns and data types."""
    df.columns = [col.lower().replace(" ", "_") for col in df.columns]
    df = df.dropna(subset=["id", "current_price"])
    df["timestamp"] = pd.to_datetime(df["timestamp"])
    df = df.sort_values(by=["id", "timestamp"])
    return df''')

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot/signals', exist_ok=True)

# Write the content to the file
with open('signal_bot/signals/signal_finder.py', 'w') as f:
    f.write('''# signal_finder.py
import pandas as pd
import numpy as np # Import numpy for NaN checks

def find_signals(df):
    """Detect signals based on indicator thresholds."""
    print("\\n--- Inside find_signals ---") # Debug print
    print("Input DataFrame columns:", df.columns.tolist()) # Debug print
    print("Input DataFrame head:\\n", df.head().to_markdown(index=False)) # Debug print


    signals_list = [] # Use a different name to avoid conflict with signals variable inside loop

    # Ensure DataFrame has required columns. Add checks for NaN values.
    required_cols = ["rsi", "macd_diff", "current_price", "bb_upper", "bb_lower"]
    if not all(col in df.columns for col in required_cols):
        print(f"Warning: Missing required columns for signal generation: {required_cols}. Returning HOLD signals.")
        df["signal"] = "HOLD"
        return df

    # Initialize signal column
    df["signal"] = "HOLD"

    # Add a column for previous MACD diff to simplify check
    df["macd_diff_prev"] = df["macd_diff"].shift(1)

    # Check if there are any rows with valid indicator values
    if df[required_cols].dropna().empty:
         print("Warning: All required indicator values are NaN. Returning HOLD signals.")
         # Drop the temporary column before returning
         if 'macd_diff_prev' in df.columns:
              df = df.drop(columns=["macd_diff_prev"])
         print("--- Exiting find_signals ---\\n") # Debug print
         return df # Return the DataFrame with all HOLD signals


    for index, row in df.iterrows():
        sigs = []

        # Skip rows with NaN indicators for signal generation
        if np.isnan(row["rsi"]) or np.isnan(row["macd_diff"]) or np.isnan(row["current_price"]) or np.isnan(row["bb_upper"]) or np.isnan(row["bb_lower"]):
            # print(f"Skipping row {index} due to NaN indicators") # Debug print
            # signals_list.append("HOLD") # No longer need signals_list, updating df directly
            continue # Skip to the next row if indicators are NaN

        # RSI signals
        if row["rsi"] < 30:
            sigs.append("BUY_RSI_OVERSOLD")
        if row["rsi"] > 70:
            sigs.append("SELL_RSI_OVERBOUGHT")

        # MACD signals - check for cross above 0
        # Ensure previous macd_diff is not NaN for cross check
        if not np.isnan(row["macd_diff_prev"]):
            if row["macd_diff"] > 0 and row["macd_diff_prev"] <= 0:
                sigs.append("BUY_MACD_CROSS")
            elif row["macd_diff"] < 0 and row["macd_diff_prev"] >= 0:
                sigs.append("SELL_MACD_CROSS")
        # Simplified MACD trending check (can be adjusted)
        # if row["macd_diff"] > 0 and row["macd_diff"] > row.get("macd_diff_prev", -float('inf')): # Use -inf to handle first row
        #     sigs.append("BUY_MACD_TRENDING")


        # Bollinger Bands signals
        if row["current_price"] > row["bb_upper"]:
            sigs.append("OVERBOUGHT_VOLATILE")
        if row["current_price"] < row["bb_lower"]:
            sigs.append("POTENTIAL_BREAKOUT")

        # Combine signals for the current row
        df.loc[index, "signal"] = ", ".join(sigs) if sigs else "HOLD"

    # Drop the temporary column
    df = df.drop(columns=["macd_diff_prev"])
    print("--- Exiting find_signals ---\\n") # Debug print

    return df # Return the DataFrame with the added signal column
''')

# signals/generate_signals.py
import pandas as pd
import ta
import numpy as np

def generate_signal(df):
    """
    Generates trading signals based on technical indicators for time-series data.

    Args:
        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).
                           Assumes DataFrame is sorted by timestamp.

    Returns:
        pd.DataFrame: DataFrame with 'signal' column added.
    """
    # Ensure the DataFrame has required columns
    required_cols = ['rsi', 'ema_20', 'macd_diff', 'close', 'bb_upper', 'bb_lower']
    if not all(col in df.columns for col in required_cols):
        print(f"Warning: Missing required columns for signal generation. Required: {required_cols}, Found: {df.columns.tolist()}. Returning HOLD signals.")
        df['signal'] = 'HOLD'
        return df[['signal']]

    if df.empty:
         print("Warning: Empty DataFrame provided for signal generation. Returning HOLD signals.")
         df['signal'] = 'HOLD'
         return df[['signal']]

    # Initialize signal column
    df['signal'] = 'HOLD'

    # Find the first index where all required indicators are not NaN
    df_cleaned = df.dropna(subset=required_cols)

    if df_cleaned.empty:
         print("Warning: No rows with valid indicator values found after dropping NaNs. Returning HOLD signals.")
         # Set all signals to HOLD if no valid indicator rows are found
         df['signal'] = 'HOLD'
         return df[['signal']]

    first_valid_index = df_cleaned.index.min()
    first_valid_iloc = df.index.get_loc(first_valid_index)


    # Apply signal logic based on technical indicators, starting from the first valid index
    # Iterate over the original DataFrame to set signals, but use df_cleaned for valid rows
    for i in range(first_valid_iloc, len(df)):
        current_row = df.iloc[i]

        # Ensure previous row exists and has valid indicator values
        prev_row_iloc = i - 1
        if prev_row_iloc < first_valid_iloc:
             # Previous row does not have valid indicators
             df.loc[current_row.name, 'signal'] = 'HOLD'
             continue

        prev_row = df.iloc[prev_row_iloc]
        signals = []

        # RSI signals (using current and previous crossing thresholds)
        if current_row['rsi'] < 30 and prev_row['rsi'] >= 30:
            signals.append("BUY_RSI_CROSS")
        elif current_row['rsi'] > 70 and prev_row['rsi'] <= 70:
            signals.append("SELL_RSI_CROSS")

        # RSI extreme conditions (may add to existing signals)
        if current_row['rsi'] < 20:
             signals.append("EXTREME_OVERSOLD")
        elif current_row['rsi'] > 80:
             signals.append("EXTREME_OVERBOUGHT")

        # MACD signals (using macd_diff crossing 0)
        if current_row['macd_diff'] > 0 and prev_row['macd_diff'] <= 0:
            signals.append("BUY_MACD_CROSS")
        elif current_row['macd_diff'] < 0 and prev_row['macd_diff'] >= 0:
            signals.append("SELL_MACD_CROSS")

        # Bollinger Bands signals (breakouts)
        if current_row['close'] > current_row['bb_upper'] and prev_row['close'] <= prev_row['bb_upper']:
            signals.append("BREAKOUT_UPPER")
        elif current_row['close'] < current_row['bb_lower'] and prev_row['close'] >= prev_row['bb_lower']:
            signals.append("BREAKOUT_LOWER")

        # Update the signal for the current row
        df.loc[current_row.name, 'signal'] = ", ".join(signals) if signals else "HOLD"


    # For rows before the first valid indicator index, signal is already HOLD

    return df[['signal']] # Return only the signal column as a DataFrame

# Execute the main script to test the updated signal generation logic after removing the strict length check.
!python signal_bot/main.py

# signals/generate_signals.py
import pandas as pd
import ta
import numpy as np

def generate_signal(df):
    """
    Generates trading signals based on technical indicators for time-series data.

    Args:
        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).
                           Assumes DataFrame is sorted by timestamp.

    Returns:
        pd.DataFrame: DataFrame with 'signal' column added.
    """
    # Ensure the DataFrame has required columns
    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']
    if not all(col in df.columns for col in required_cols):
        # This should not happen if compute_indicators ran successfully, but as a safeguard
        print("Error: Missing required columns for signal generation.")
        df['signal'] = 'HOLD'
        return df[['signal']]

    # Initialize signal column
    df['signal'] = 'HOLD'

    # Find the first index where all required indicators are not NaN
    first_valid_index = df.dropna(subset=required_cols).index.min()

    if pd.isna(first_valid_index):
         print("Warning: All required indicator values are NaN. Returning HOLD signals.")
         # Set all signals to HOLD if no valid indicator rows are found
         df['signal'] = 'HOLD'
         return df[['signal']]

    first_valid_iloc = df.index.get_loc(first_valid_index)

    # Apply signal logic based on technical indicators, starting from the first valid index
    for i in range(first_valid_iloc, len(df)):
        current_row = df.iloc[i]
        # Get previous row if available and if previous indicator values are also valid
        prev_row = df.iloc[i-1] if i > 0 and not df.iloc[i-1][required_cols].isnull().any() else None

        if prev_row is None:
             # Cannot compute signals requiring valid previous values
             df.loc[current_row.name, 'signal'] = 'HOLD'
             continue

        signals = []

        # RSI signals (using current and previous crossing thresholds)
        if current_row['rsi'] < 30 and prev_row['rsi'] >= 30:
            signals.append("BUY_RSI_CROSS")
        elif current_row['rsi'] > 70 and prev_row['rsi'] <= 70:
            signals.append("SELL_RSI_CROSS")

        # RSI extreme conditions (may add to existing signals)
        if current_row['rsi'] < 20:
             signals.append("EXTREME_OVERSOLD")
        elif current_row['rsi'] > 80:
             signals.append("EXTREME_OVERBOUGHT")

        # MACD signals (using macd_diff crossing 0)
        if current_row['macd_diff'] > 0 and prev_row['macd_diff'] <= 0:
            signals.append("BUY_MACD_CROSS")
        elif current_row['macd_diff'] < 0 and prev_row['macd_diff'] >= 0:
            signals.append("SELL_MACD_CROSS")

        # Bollinger Bands signals (breakouts)
        if current_row['close'] > current_row['bb_upper'] and prev_row['close'] <= prev_row['bb_upper']:
            signals.append("BREAKOUT_UPPER")
        elif current_row['close'] < current_row['bb_lower'] and prev_row['close'] >= prev_row['bb_lower']:
            signals.append("BREAKOUT_LOWER")

        # Update the signal for the current row
        df.loc[current_row.name, 'signal'] = ", ".join(signals) if signals else "HOLD"


    # For rows before the first valid indicator index, set signal to HOLD
    if first_valid_iloc > 0:
        df.loc[df.index[:first_valid_iloc], 'signal'] = 'HOLD'


    return df[['signal']] # Return only the signal column as a DataFrame

# Execute the main script to test the updated signal generation logic with NaN handling.
!python signal_bot/main.py

# signals/generate_signals.py
import pandas as pd
import ta
import numpy as np

def generate_signal(df):
    """
    Generates trading signals based on technical indicators for time-series data.

    Args:
        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).
                           Assumes DataFrame is sorted by timestamp.

    Returns:
        pd.DataFrame: DataFrame with 'signal' column added.
    """
    # Ensure the DataFrame has required columns
    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']
    if not all(col in df.columns for col in required_cols):
        # This should not happen if compute_indicators ran successfully, but as a safeguard
        print("Error: Missing required columns for signal generation.")
        df['signal'] = 'HOLD'
        return df[['signal']]

    # Initialize signal column
    df['signal'] = 'HOLD'

    # Check if there are enough rows for indicator calculation to be potentially valid
    # RSI needs at least 14 periods, others need 20 based on default window sizes
    # Check for at least the largest window size for meaningful signals
    min_data_points = 20 # Based on BB and EMA window size

    if len(df) < min_data_points:
         print(f"Warning: Not enough data points ({len(df)}) for meaningful signal generation (requires at least {min_data_points}). Returning HOLD signals.")
         return df[['signal']]

    # Apply signal logic based on technical indicators, starting from where indicators are likely valid
    # Indicators like RSI, MACD, BB will have NaNs for the first period(s) based on their window.
    # We can start applying logic from the first row where all required indicators are not NaN.
    first_valid_index = df.dropna(subset=required_cols).index.min()

    if pd.isna(first_valid_index):
         print("Warning: All indicator values are NaN. Returning HOLD signals.")
         return df[['signal']]

    first_valid_iloc = df.index.get_loc(first_valid_index)


    for i in range(first_valid_iloc, len(df)):
        current_row = df.iloc[i]
        prev_row = df.iloc[i-1] if i > 0 else None # Get previous row if available

        if prev_row is None:
             # Cannot compute signals requiring previous values for the very first row
             continue

        signals = []

        # RSI signals (using current and previous crossing thresholds)
        if current_row['rsi'] < 30 and prev_row['rsi'] >= 30:
            signals.append("BUY_RSI_CROSS")
        elif current_row['rsi'] > 70 and prev_row['rsi'] <= 70:
            signals.append("SELL_RSI_CROSS")

        # RSI extreme conditions (may add to existing signals)
        if current_row['rsi'] < 20:
             signals.append("EXTREME_OVERSOLD")
        elif current_row['rsi'] > 80:
             signals.append("EXTREME_OVERBOUGHT")


        # MACD signals (using macd_diff crossing 0)
        if current_row['macd_diff'] > 0 and prev_row['macd_diff'] <= 0:
            signals.append("BUY_MACD_CROSS")
        elif current_row['macd_diff'] < 0 and prev_row['macd_diff'] >= 0:
            signals.append("SELL_MACD_CROSS")

        # Bollinger Bands signals (breakouts)
        if current_row['close'] > current_row['bb_upper'] and prev_row['close'] <= prev_row['bb_upper']:
            signals.append("BREAKOUT_UPPER")
        elif current_row['close'] < current_row['bb_lower'] and prev_row['close'] >= prev_row['bb_lower']:
            signals.append("BREAKOUT_LOWER")

        # Update the signal for the current row
        df.loc[current_row.name, 'signal'] = ", ".join(signals) if signals else "HOLD"


    # For rows before the first valid indicator index, set signal to HOLD
    if first_valid_iloc > 0:
        df.loc[df.index[:first_valid_iloc], 'signal'] = 'HOLD'


    return df[['signal']] # Return only the signal column as a DataFrame

# Modify main.py to print DataFrame info before generating signals
main_py_content = """# main.py
import pandas as pd
import os
from utils.coingecko_api import get_top_coins, get_coin_history
from indicators.ta_utils import compute_indicators
from signals.generate_signals import generate_signal

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Fetching top 10 coin data...")
    try:
        data = get_top_coins()
        df_top10 = pd.DataFrame(data)
        df_top10["timestamp"] = pd.Timestamp.utcnow()
        df_top10.to_csv(f"{DATA_DIR}/top10_market_data.csv", index=False)
        print("Top 10 data fetched and saved.")

        print("Computing technical indicators for top 10 data...")
        # For now, we'll use the current price as 'close' for indicator calculation
        df_top10["close"] = df_top10["current_price"]
        df_ind_top10 = compute_indicators(df_top10.copy())
        df_ind_top10.to_csv(f"{DATA_DIR}/top10_with_indicators.csv", index=False)
        print("Indicators computed and saved for top 10 data.")

        # Add print statements to inspect df_ind_top10
        print("\\n--- Inspecting df_ind_top10 before signal generation ---")
        print("Columns:", df_ind_top10.columns.tolist())
        print("Head:\\n", df_ind_top10.head().to_markdown(index=False))
        print("------------------------------------------------------\\n")


        print("Generating signals for top 10 snapshot data...")
        # Note: The current generate_signal function expects a single row or a dataframe
        # with sufficient historical data. For this initial test with only current data,
        # the signals will likely be 'HOLD' or based on limited info.
        df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data
        df_signals_top10.to_csv(f"{DATA_DIR}/top10_signals.csv", index=False)
        print("Signals generated and saved for top 10 snapshot data.")

    except Exception as e:
        print(f"Error processing top 10 data: {e}. Skipping top 10 data processing.")
        df_signals_top10 = pd.DataFrame() # Ensure df_signals_top10 is defined even on error

    # --- Historical Data Processing with Error Handling ---
    coin_id = "bitcoin"
    history_df_signals = pd.DataFrame() # Initialize as empty DataFrame

    print(f"Attempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")

        if history_data is not None: # Check if data fetching was successful
            print("Historical data fetched.")

            # Process historical price data into a DataFrame
            if "prices" not in history_data:
                print(f"Warning: 'prices' key not found in historical data for {coin_id}. Response keys: {history_data.keys()}")
            else:
                history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])
                history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
                print("Historical data processed into DataFrame.")

                # Compute indicators on the historical data DataFrame
                print(f"Computing technical indicators for {coin_id} historical data...")
                history_df_ind = compute_indicators(history_df.copy())
                history_df_ind.to_csv(f"{DATA_DIR}/{coin_id}_historical_with_indicators.csv", index=False)
                print("Indicators computed and saved for historical data.")

                # Add print statements to inspect history_df_ind
                print("\\n--- Inspecting history_df_ind before signal generation ---")
                print("Columns:", history_df_ind.columns.tolist())
                print("Head:\\n", history_df_ind.head().to_markdown(index=False))
                print("--------------------------------------------------------\\n")


                # Generate signals on the historical data
                print(f"Generating signals for {coin_id} historical data...")
                history_df_signals = generate_signal(history_df_ind)
                history_df_signals.to_csv(f"{DATA_DIR}/{coin_id}_historical_signals.csv", index=False)
                print("Historical signals generated and saved.")
        else:
             print("Historical data fetching failed. Skipping historical data processing.")


    except Exception as e:
        print(f"Error processing historical data for {coin_id}: {e}. Skipping historical signals.")
        # history_df_signals is already initialized as empty

    # --- Display Results ---
    print("\\n--- Results ---")
    print("Historical signals (if generated):")
    if not history_df_signals.empty:
        print(history_df_signals.head().to_markdown(index=False)) # Use to_markdown for better printing
    else:
        print("No historical signals generated.")

    print("\\nTop 10 snapshot signals:")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False)) # Use to_markdown for better printing
    else:
        print("No top 10 snapshot signals generated.")


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

# Execute the script to inspect the DataFrames
!python signal_bot/main.py

# Execute the main script to test the updated signal generation logic.
!python signal_bot/main.py

# Modify the signals/generate_signals.py file to implement signal logic for time-series data and return a DataFrame.
with open('signal_bot/signals/generate_signals.py', 'w') as f:
    f.write("""# signals/generate_signals.py
import pandas as pd
import ta

def generate_signal(df):
    \"\"\"
    Generates trading signals based on technical indicators for time-series data.

    Args:
        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).
                           Assumes DataFrame is sorted by timestamp.

    Returns:
        pd.DataFrame: DataFrame with 'signal' column added.
    \"\"\"
    # Ensure the DataFrame has required columns and is not empty
    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']
    if df.empty or not all(col in df.columns for col in required_cols):
        # If data is insufficient or missing columns, return a DataFrame with 'HOLD' signals
        print("Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.")
        df['signal'] = 'HOLD'
        return df[['signal']]

    # Initialize signal column
    df['signal'] = 'HOLD'

    # Apply signal logic based on technical indicators
    for i in range(1, len(df)): # Start from the second row to check previous values
        # RSI signals
        if df['rsi'].iloc[i] < 30 and df['rsi'].iloc[i-1] >= 30:
            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', BUY_RSI_CROSS').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'BUY_RSI_CROSS'
        elif df['rsi'].iloc[i] > 70 and df['rsi'].iloc[i-1] <= 70:
            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', SELL_RSI_CROSS').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'SELL_RSI_CROSS'
        # RSI extreme conditions (may add to existing signals)
        elif df['rsi'].iloc[i] < 20:
             df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', EXTREME_OVERSOLD').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'EXTREME_OVERSOLD'
        elif df['rsi'].iloc[i] > 80:
             df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', EXTREME_OVERBOUGHT').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'EXTREME_OVERBOUGHT'


        # MACD signals (using macd_diff crossing 0)
        if df['macd_diff'].iloc[i] > 0 and df['macd_diff'].iloc[i-1] <= 0:
            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', BUY_MACD_CROSS').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'BUY_MACD_CROSS'
        elif df['macd_diff'].iloc[i] < 0 and df['macd_diff'].iloc[i-1] >= 0:
            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', SELL_MACD_CROSS').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'SELL_MACD_CROSS'

        # Bollinger Bands signals (breakouts)
        if df['close'].iloc[i] > df['bb_upper'].iloc[i] and df['close'].iloc[i-1] <= df['bb_upper'].iloc[i-1]:
            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', BREAKOUT_UPPER').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'BREAKOUT_UPPER'
        elif df['close'].iloc[i] < df['bb_lower'].iloc[i] and df['close'].iloc[i-1] >= df['bb_lower'].iloc[i-1]:
            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', BREAKOUT_LOWER').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'BREAKOUT_LOWER'

    return df[['signal']] # Return only the signal column as a DataFrame
""")

# Execute the main script
!python signal_bot/main.py

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the file
with open('signal_bot/main.py', 'w') as f:
    f.write('''# main.py
import pandas as pd
import os
from utils.coingecko_api import get_top_coins, get_coin_history
from indicators.ta_utils import compute_indicators
from signals.generate_signals import generate_signal

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Fetching top 10 coin data...")
    data = get_top_coins()
    df_top10 = pd.DataFrame(data)
    df_top10["timestamp"] = pd.Timestamp.utcnow()
    df_top10.to_csv(f"{DATA_DIR}/top10_market_data.csv", index=False)
    print("Top 10 data fetched and saved.")

    print("Computing technical indicators for top 10 data...")
    # For now, we'll use the current price as 'close' for indicator calculation
    df_top10["close"] = df_top10["current_price"]
    df_ind_top10 = compute_indicators(df_top10.copy())
    df_ind_top10.to_csv(f"{DATA_DIR}/top10_with_indicators.csv", index=False)
    print("Indicators computed and saved for top 10 data.")

    # --- Historical Data Processing ---
    coin_id = "bitcoin"
    print(f"Attempting to fetch and process historical data for {coin_id}...")
    history_data = get_coin_history(coin_id, days="30")
    print("Historical data fetched.")

    # Process historical price data into a DataFrame
    # The 'prices' key contains a list of [timestamp, price]
    if "prices" not in history_data:
        print(f"Warning: 'prices' key not found in historical data for {coin_id}.")
        history_df_signals = pd.DataFrame() # Ensure history_df_signals is defined even if historical data is missing
    else:
        history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])
        # Convert timestamp to datetime and set as index (or keep as column)
        history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
        # Optional: Set timestamp as index for time-series analysis
        # history_df = history_df.set_index("timestamp")
        print("Historical data processed into DataFrame.")

        # Compute indicators on the historical data DataFrame
        print(f"Computing technical indicators for {coin_id} historical data...")
        history_df_ind = compute_indicators(history_df.copy())
        history_df_ind.to_csv(f"{DATA_DIR}/{coin_id}_historical_with_indicators.csv", index=False)
        print("Indicators computed and saved for historical data.")

        # Generate signals on the historical data
        print(f"Generating signals for {coin_id} historical data...")
        history_df_signals = generate_signal(history_df_ind)
        history_df_signals.to_csv(f"{DATA_DIR}/{coin_id}_historical_signals.csv", index=False)
        print("Historical signals generated and saved.")

    print("Generating signals for top 10 snapshot data...")
    # Note: The current generate_signal function expects a single row or a dataframe
    # with sufficient historical data. For this initial test with only current data,
    # the signals will likely be 'HOLD' or based on limited info.
    df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data
    df_signals_top10.to_csv(f"{DATA_DIR}/top10_signals.csv", index=False)
    print("Signals generated and saved for top 10 snapshot data.")


    # --- Display Results ---
    print("\\n--- Results ---")
    print("Historical signals (if generated):")
    if not history_df_signals.empty:
        print(history_df_signals.head().to_markdown(index=False)) # Use to_markdown for better printing
    else:
        print("No historical signals generated.")

    print("\\nTop 10 snapshot signals:")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False)) # Use to_markdown for better printing
    else:
        print("No top 10 snapshot signals generated.")


if __name__ == "__main__":
    main()
''')

# Modify ml_logger.py to check for required columns before selecting them.
import pandas as pd
import os
from datetime import datetime

def log_ml_features(indicator_csv, output_csv):
    """Append new features to ML training dataset."""
    try:
        df = pd.read_csv(indicator_csv)
        print(f"Successfully read {indicator_csv} for ML logging.")
        print(f"Columns in {indicator_csv}: {df.columns.tolist()}") # Add logging

        required_features = [
            "id", "timestamp", "current_price",
            "rsi", "ema_20", "macd_diff",
            "bb_upper", "bb_lower"
        ]

        # Check if all required features are in the DataFrame
        missing_features = [col for col in required_features if col not in df.columns]
        if missing_features:
            print(f"Warning: Missing required features for ML logging in {indicator_csv}: {missing_features}. Skipping logging for this file.")
            # Optionally, return an empty DataFrame or None to indicate failure
            return pd.DataFrame()

        features = df[required_features].copy()

        if os.path.exists(output_csv):
            existing = pd.read_csv(output_csv)
            combined = pd.concat([existing, features]).drop_duplicates(subset=["id", "timestamp"])
        else:
            combined = features

        combined.to_csv(output_csv, index=False)
        print(f"ML features logged to {output_csv}.")
        return combined

    except FileNotFoundError:
        print(f"Error: Input file for ML logging not found at {indicator_csv}. Skipping logging.")
        return pd.DataFrame() # Return empty DataFrame on error
    except Exception as e:
        print(f"Error during ML feature logging for {indicator_csv}: {e}. Skipping logging.")
        return pd.DataFrame() # Return empty DataFrame on error

# Overwrite the existing ml_logger.py file
with open('signal_bot/ml_logger.py', 'w') as f:
    f.write('''# ml_logger.py
import pandas as pd
import os
from datetime import datetime

def log_ml_features(indicator_csv, output_csv):
    """Append new features to ML training dataset."""
    try:
        df = pd.read_csv(indicator_csv)
        # print(f"Successfully read {indicator_csv} for ML logging.") # Removed verbose logging
        # print(f"Columns in {indicator_csv}: {df.columns.tolist()}") # Removed verbose logging

        required_features = [
            "id", "timestamp", "current_price",
            "rsi", "ema_20", "macd_diff",
            "bb_upper", "bb_lower"
        ]

        # Check if all required features are in the DataFrame
        missing_features = [col for col in required_features if col not in df.columns]
        if missing_features:
            print(f"Warning: Missing required features for ML logging in {indicator_csv}: {missing_features}. Skipping logging for this file.")
            # Optionally, return an empty DataFrame or None to indicate failure
            return pd.DataFrame()

        features = df[required_features].copy()

        if os.path.exists(output_csv):
            existing = pd.read_csv(output_csv)
            combined = pd.concat([existing, features]).drop_duplicates(subset=["id", "timestamp"])
        else:
            combined = features

        combined.to_csv(output_csv, index=False)
        # print(f"ML features logged to {output_csv}.") # Removed verbose logging
        return combined

    except FileNotFoundError:
        print(f"Error: Input file for ML logging not found at {indicator_csv}. Skipping logging.")
        return pd.DataFrame() # Return empty DataFrame on error
    except Exception as e:
        print(f"Error during ML feature logging for {indicator_csv}: {e}. Skipping logging.")
        return pd.DataFrame() # Return empty DataFrame on error
''')

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the file
with open('signal_bot/anomaly_detector.py', 'w') as f:
    f.write('''# anomaly_detector.py
import pandas as pd
from sklearn.ensemble import IsolationForest
from datetime import datetime
import os

def load_market_snapshot(snapshot_csv):
    """Load full market snapshot from CSV."""
    df = pd.read_csv(snapshot_csv, parse_dates=["timestamp"])
    return df

def prepare_features(df):
    """Create features for anomaly detection (e.g., price % change)."""
    df["pct_change_24h"] = df["price_change_percentage_24h"].fillna(0)
    return df[["id", "symbol", "name", "pct_change_24h", "timestamp"]].copy()

def run_isolation_forest(df, contamination=0.01):
    """Apply Isolation Forest to detect anomalies based on % change."""
    model = IsolationForest(contamination=contamination, random_state=42)
    df["anomaly_score"] = model.fit_predict(df[["pct_change_24h"]])
    return df

def save_anomalies(df, output_dir="data"):
    """Save anomalies to timestamped CSV."""
    timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    anomalies = df[df["anomaly_score"] == -1]
    path = os.path.join(output_dir, f"anomalies_{timestamp}.csv")
    os.makedirs(output_dir, exist_ok=True)
    anomalies.to_csv(path, index=False)
    return path, anomalies

def detect_anomalies(snapshot_csv, output_dir="data"):
    """End-to-end anomaly detection workflow."""
    df = load_market_snapshot(snapshot_csv)
    features = prepare_features(df)
    with_scores = run_isolation_forest(features)
    path, anomalies = save_anomalies(with_scores, output_dir)
    return path, anomalies
''')

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot/data', exist_ok=True) # Ensure the data directory exists

# Write the content to the file
with open('signal_bot/indicator_engine.py', 'w') as f:
    f.write('''# indicator_engine.py
import pandas as pd
from datetime import datetime
import os
import ta  # pip install ta

def compute_indicators(input_csv, output_csv):
    df = pd.read_csv(input_csv, parse_dates=["timestamp"])

    # Ensure close price exists
    df["close"] = df["current_price"]
    df = df.sort_values("timestamp")

    df["rsi"] = ta.momentum.RSIIndicator(df["close"]).rsi()
    df["ema_20"] = ta.trend.EMAIndicator(df["close"], window=20).ema_indicator()
    df["macd_diff"] = ta.trend.MACD(df["close"]).macd_diff()

    # Bollinger Bands (20 window, 2 std)
    bb = ta.volatility.BollingerBands(df["close"], window=20, window_dev=2)
    df["bb_upper"] = bb.bollinger_hband()
    df["bb_lower"] = bb.bollinger_lband()

    df["processed_timestamp"] = datetime.utcnow().isoformat()
    os.makedirs(os.path.dirname(output_csv), exist_ok=True)
    df.to_csv(output_csv, index=False)
    return df

# Run engine
# Using the DATA_DIR defined earlier in the notebook
DATA_DIR = '/content/drive/MyDrive/crypto_signal_bot/data'
input_path = f"{DATA_DIR}/top10_market_data.csv"
output_path = f"{DATA_DIR}/top10_with_indicators.csv"

# Check if the input file exists before attempting to read
if os.path.exists(input_path):
    df_ind = compute_indicators(input_path, output_path)
    import ace_tools as tools; tools.display_dataframe_to_user(name="Top 10 with Indicators", dataframe=df_ind)
else:
    print(f"Error: Input file not found at {input_path}. Please ensure data has been fetched.")

''')

# Rewrite the content of signal_bot/main.py with correct indentation for the historical data handling
main_py_content = """# main.py
import pandas as pd
import os
from utils.coingecko_api import get_top_coins, get_coin_history
from indicators.ta_utils import compute_indicators
from signals.generate_signals import generate_signal

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Fetching top 10 coin data...")
    try:
        data = get_top_coins()
        if data is not None:
            df_top10 = pd.DataFrame(data)
            df_top10["timestamp"] = pd.Timestamp.utcnow()
            df_top10.to_csv(f"{DATA_DIR}/top10_market_data.csv", index=False)
            print("Top 10 data fetched and saved.")

            print("Computing technical indicators for top 10 data...")
            # For now, we'll use the current price as 'close' for indicator calculation
            df_top10["close"] = df_top10["current_price"]
            df_ind_top10 = compute_indicators(df_top10.copy())
            df_ind_top10.to_csv(f"{DATA_DIR}/top10_with_indicators.csv", index=False)
            print("Indicators computed and saved for top 10 data.")

            print("Generating signals for top 10 snapshot data...")
            df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data
            df_signals_top10.to_csv(f"{DATA_DIR}/top10_signals.csv", index=False)
            print("Signals generated and saved for top 10 snapshot data.")
        else:
            print("Failed to fetch top 10 coin data.")
            df_signals_top10 = pd.DataFrame() # Ensure df_signals_top10 is defined

    except Exception as e:
        print(f"Error processing top 10 data: {e}. Skipping top 10 data processing.")
        df_signals_top10 = pd.DataFrame() # Ensure df_signals_top10 is defined even on error


    # --- Historical Data Processing with Error Handling ---
    coin_id = "bitcoin"
    history_df_signals = pd.DataFrame() # Initialize as empty DataFrame

    print(f"Attempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")

        if history_data is not None:
            print("Historical data fetched.")

            # Process historical price data into a DataFrame
            if "prices" not in history_data:
                print(f"Warning: 'prices' key not found in historical data for {coin_id}. Response keys: {history_data.keys()}")
            else:
                history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])
                history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
                print("Historical data processed into DataFrame.")

                # Compute indicators on the historical data DataFrame
                print(f"Computing technical indicators for {coin_id} historical data...")
                history_df_ind = compute_indicators(history_df.copy())
                history_df_ind.to_csv(f"{DATA_DIR}/{coin_id}_historical_with_indicators.csv", index=False)
                print("Indicators computed and saved for historical data.")

                # Generate signals on the historical data
                print(f"Generating signals for {coin_id} historical data...")
                history_df_signals = generate_signal(history_df_ind)
                history_df_signals.to_csv(f"{DATA_DIR}/{coin_id}_historical_signals.csv", index=False)
                print("Historical signals generated and saved.")
        else:
             print("Historical data fetching failed. Skipping historical data processing.")


    except Exception as e:
        print(f"Error processing historical data for {coin_id}: {e}. Skipping historical signals.")
        # history_df_signals is already initialized as empty

    # --- Display Results ---
    print("\\n--- Results ---")
    print("Historical signals (if generated):")
    if not history_df_signals.empty:
        print(history_df_signals.head().to_markdown(index=False)) # Use to_markdown for better printing
    else:
        print("No historical signals generated.")

    print("\\nTop 10 snapshot signals:")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False)) # Use to_markdown for better printing
    else:
        print("No top 10 snapshot signals generated.")


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

# Execute the script
!python signal_bot/main.py

# Modify the main.py script to handle None return from get_coin_history without using continue
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

modified_lines = []
in_historical_processing_block = False
for line in lines:
    if 'history_data = get_coin_history(coin_id, days="30")' in line:
        modified_lines.append(line)
        modified_lines.append('        if history_data is not None:\n') # Start the conditional block
        in_historical_processing_block = True
    elif in_historical_processing_block and line.strip() == '': # End the conditional block at the end of the block
         modified_lines.append('        # End of historical data processing block\n')
         in_historical_processing_block = False
         modified_lines.append(line) # Keep the blank line
    elif in_historical_processing_block and line.strip().startswith('except'): # End the conditional block before except
         modified_lines.append('        # End of historical data processing block\n')
         in_historical_processing_block = False
         modified_lines.append(line)
    elif in_historical_processing_block:
        # Indent the lines within the historical data processing block
        modified_lines.append(f'            {line.lstrip()}')
    elif 'coin_id = "ethereum"' in line:
         modified_lines.append('    coin_id = "bitcoin" # Revert to bitcoin\n') # Revert coin_id
    else:
        modified_lines.append(line)

with open('signal_bot/main.py', 'w') as f:
    f.writelines(modified_lines)

# Execute the main script again to test the changes
!python signal_bot/main.py

# Modify the get_coin_history function to remove the 'interval' parameter
with open('signal_bot/utils/coingecko_api.py', 'r') as f:
    lines = f.readlines()

modified_lines = []
for line in lines:
    if 'params = {"vs_currency": "usd", "days": days, "interval": "hourly"}' in line:
        modified_lines.append('    params = {"vs_currency": "usd", "days": days}\n') # Remove interval parameter
    else:
        modified_lines.append(line)

with open('signal_bot/utils/coingecko_api.py', 'w') as f:
    f.writelines(modified_lines)

# Modify the main.py script to handle None return from get_coin_history and try a different coin
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

modified_lines = []
for line in lines:
    if 'history_data = get_coin_history(coin_id, days="30")' in line:
        modified_lines.append(line)
        modified_lines.append('        if history_data is None:\n') # Add check for None
        modified_lines.append('            print("Historical data fetching failed. Skipping historical data processing.")\n')
        modified_lines.append('            history_df_signals = pd.DataFrame() # Ensure history_df_signals is defined\n')
        modified_lines.append('            continue # Skip the rest of the historical data processing block\n')
    elif 'coin_id = "bitcoin"' in line:
         modified_lines.append('    coin_id = "ethereum" # Try fetching historical data for Ethereum\n') # Change coin_id
    else:
        modified_lines.append(line)

with open('signal_bot/main.py', 'w') as f:
    f.writelines(modified_lines)

# Execute the main script again to test the changes
!python signal_bot/main.py

import requests
import pandas as pd
import os

BASE_URL = "https://api.coingecko.com/api/v3"

def get_top_coins(limit=10):
    url = f"{BASE_URL}/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": limit,
        "page": 1,
        "sparkline": "true"
    }
    try:
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching top coins: {e}")
        return None

def get_coin_history(coin_id, days="30"):
    url = f"{BASE_URL}/coins/{coin_id}/market_chart"
    params = {"vs_currency": "usd", "days": days} # Removed 'interval': 'hourly'
    print(f"Fetching historical data from URL: {url} with params: {params}") # Log URL and params
    try:
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes
        data = response.json()
        print(f"Historical data response keys: {data.keys()}") # Log response keys
        return data
    except requests.exceptions.RequestException as e:
        print(f"Error fetching historical data for {coin_id}: {e}")
        if 'response' in locals() and response is not None:
             print(f"Response status code: {response.status_code}")
             print(f"Response text: {response.text}") # Log response text
        return None

# Overwrite the existing file with the modified content
with open('signal_bot/utils/coingecko_api.py', 'w') as f:
    f.write('''# utils/coingecko_api.py
import requests

BASE_URL = "https://api.coingecko.com/api/v3"

def get_top_coins(limit=10):
    url = f"{BASE_URL}/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": limit,
        "page": 1,
        "sparkline": "true"
    }
    try:
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching top coins: {e}")
        return None

def get_coin_history(coin_id, days="30"):
    url = f"{BASE_URL}/coins/{coin_id}/market_chart"
    params = {"vs_currency": "usd", "days": days} # Removed 'interval': 'hourly'
    print(f"Fetching historical data from URL: {url} with params: {params}") # Log URL and params
    try:
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes
        data = response.json()
        print(f"Historical data response keys: {data.keys()}") # Log response keys
        return data
    except requests.exceptions.RequestException as e:
        print(f"Error fetching historical data for {coin_id}: {e}")
        if 'response' in locals() and response is not None:
             print(f"Response status code: {response.status_code}")
             print(f"Response text: {response.text}") # Log response text
        return None''')

# Modify main.py to handle None from get_coin_history
main_py_content = """# main.py
import pandas as pd
import os
from utils.coingecko_api import get_top_coins, get_coin_history
from indicators.ta_utils import compute_indicators
from signals.generate_signals import generate_signal

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Fetching top 10 coin data...")
    try:
        data = get_top_coins()
        df_top10 = pd.DataFrame(data)
        df_top10["timestamp"] = pd.Timestamp.utcnow()
        df_top10.to_csv(f"{DATA_DIR}/top10_market_data.csv", index=False)
        print("Top 10 data fetched and saved.")

        print("Computing technical indicators for top 10 data...")
        # For now, we'll use the current price as 'close' for indicator calculation
        df_top10["close"] = df_top10["current_price"]
        df_ind_top10 = compute_indicators(df_top10.copy())
        df_ind_top10.to_csv(f"{DATA_DIR}/top10_with_indicators.csv", index=False)
        print("Indicators computed and saved for top 10 data.")

        print("Generating signals for top 10 snapshot data...")
        # Note: The current generate_signal function expects a single row or a dataframe
        # with sufficient historical data. For this initial test with only current data,
        # the signals will likely be 'HOLD' or based on limited info.
        df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data
        df_signals_top10.to_csv(f"{DATA_DIR}/top10_signals.csv", index=False)
        print("Signals generated and saved for top 10 snapshot data.")

    except Exception as e:
        print(f"Error processing top 10 data: {e}. Skipping top 10 data processing.")
        df_signals_top10 = pd.DataFrame() # Ensure df_signals_top10 is defined even on error

    # --- Historical Data Processing with Error Handling ---
    coin_id = "bitcoin"
    history_df_signals = pd.DataFrame() # Initialize as empty DataFrame

    print(f"Attempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")

        if history_data is not None: # Check if data fetching was successful
            print("Historical data fetched.")

            # Process historical price data into a DataFrame
            if "prices" not in history_data:
                print(f"Warning: 'prices' key not found in historical data for {coin_id}. Response keys: {history_data.keys()}")
            else:
                history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])
                history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
                print("Historical data processed into DataFrame.")

                # Compute indicators on the historical data DataFrame
                print(f"Computing technical indicators for {coin_id} historical data...")
                history_df_ind = compute_indicators(history_df.copy())
                history_df_ind.to_csv(f"{DATA_DIR}/{coin_id}_historical_with_indicators.csv", index=False)
                print("Indicators computed and saved for historical data.")

                # Generate signals on the historical data
                print(f"Generating signals for {coin_id} historical data...")
                history_df_signals = generate_signal(history_df_ind)
                history_df_signals.to_csv(f"{DATA_DIR}/{coin_id}_historical_signals.csv", index=False)
                print("Historical signals generated and saved.")
        else:
             print("Historical data fetching failed. Skipping historical data processing.")


    except Exception as e:
        print(f"Error processing historical data for {coin_id}: {e}. Skipping historical signals.")
        # history_df_signals is already initialized as empty

    # --- Display Results ---
    print("\\n--- Results ---")
    print("Historical signals (if generated):")
    if not history_df_signals.empty:
        print(history_df_signals.head().to_markdown(index=False)) # Use to_markdown for better printing
    else:
        print("No historical signals generated.")

    print("\\nTop 10 snapshot signals:")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False)) # Use to_markdown for better printing
    else:
        print("No top 10 snapshot signals generated.")


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)


# Execute the main script to trigger the historical data fetch with added logging
!python signal_bot/main.py

# Execute the main script
!python signal_bot/main.py

# Rewrite the content of signal_bot/main.py to integrate modules and clean up for testing.
main_py_content = """# main.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins, get_coin_history
from .indicators.ta_utils import compute_indicators
# from .signals.generate_signals import generate_signal
from .signals.signal_finder import find_signals
from .anomaly_detector import detect_anomalies
from .ml_logger import log_ml_features
from .backtester import backtest_signals
from .ml_model_trainer import train_ml_model
from .logger import setup_logger, log_info, log_error
from .exporter import export_to_excel, export_to_html # Import exporter functions

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    setup_logger()
    log_info("Starting bot pipeline...")

    # --- Data Collection (using existing top 10 fetch for now) ---
    log_info("--- Top 10 Data Pipeline ---")
    print("Fetching top 10 coin data...")
    try:
        data = get_top_coins()
        if data is not None:
            df_top10 = pd.DataFrame(data)
            df_top10["timestamp"] = pd.Timestamp.utcnow()
            top10_market_data_path = os.path.join(DATA_DIR, "top10_market_data.csv")
            df_top10.to_csv(top10_market_data_path, index=False)
            log_info("Top 10 data fetched and saved.")

            log_info("Computing technical indicators for top 10 data...")
            df_top10["close"] = df_top10["current_price"]
            df_ind_top10 = compute_indicators(df_top10.copy())
            top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
            df_ind_top10.to_csv(top10_indicators_path, index=False)
            log_info("Indicators computed and saved for top 10 data.")

            log_info("Generating signals for top 10 snapshot data...")
            df_signals_top10 = find_signals(df_ind_top10.copy())
            top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
            df_signals_top10.to_csv(top10_signals_path, index=False)
            log_info("Signals generated and saved for top 10 snapshot data.")

            log_info("Logging ML features for top 10 data...")
            ml_log_path = os.path.join(DATA_DIR, "ml_training.csv")
            log_ml_features(top10_indicators_path, ml_log_path)
            log_info("ML features logged for top 10 data.")

        else:
            log_error("Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.")
            df_signals_top10 = pd.DataFrame()

    except Exception as e:
        log_error(f"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.")
        df_signals_top10 = pd.DataFrame()
    log_info("--- Top 10 Data Pipeline Finished ---")


    # --- Anomaly Detection ---
    log_info("\\n--- Anomaly Detection Pipeline ---")
    print("Attempting to run Anomaly Detection...")
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if os.path.exists(full_snapshot_path):
        try:
            log_info("Running Anomaly Detection...")
            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
            log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")
            if not anomalies.empty:
                 print("Sample anomalies:")
                 print(anomalies.head().to_markdown(index=False))
            else:
                 print("No anomalies detected.")
        except Exception as e:
            log_error(f"Error during Anomaly Detection: {e}.")
    else:
        log_info(f"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.")
    log_info("--- Anomaly Detection Pipeline Finished ---")


    # --- Historical Data Processing ---
    log_info("\\n--- Historical Data Pipeline ---")
    coin_id = "bitcoin"
    history_df_signals_display = pd.DataFrame()
    historical_price_data_path = os.path.join(DATA_DIR, f"{coin_id}_historical_price.csv")
    historical_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv")
    backtest_results_path = os.path.join(DATA_DIR, "signal_backtest.csv")

    print(f"Attempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")
        if history_data is not None and "prices" in history_data:
            log_info("Historical data fetched.")
            history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "current_price"])
            history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
            history_df.to_csv(historical_price_data_path, index=False)
            log_info("Historical data processed and saved.")

            log_info(f"Computing technical indicators for {coin_id} historical data...")
            history_df["close"] = history_df["current_price"]
            history_df_ind = compute_indicators(history_df.copy())
            history_historical_indicators_path = os.path.join(DATA_DIR, f"{coin_id}_historical_with_indicators.csv")
            history_df_ind.to_csv(history_historical_indicators_path, index=False)
            log_info("Indicators computed and saved for historical data.")

            log_info(f"Generating signals for {coin_id} historical data...")
            history_df_signals = find_signals(history_df_ind.copy())
            history_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv")
            history_df_signals.to_csv(historical_signals_path, index=False)
            log_info("Historical signals generated and saved.")
            history_df_signals_display = history_df_signals

            # --- Backtesting Historical Signals ---
            log_info(f"Attempting to backtest historical signals for {coin_id}...")
            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)
            log_info("Backtesting completed.")
            if not backtest_results_df.empty:
                print("Backtest Results (head):")
                print(backtest_results_df.head().to_markdown(index=False))

                # --- Export Backtest Results --- # Added Export step
                log_info("Exporting backtest results...")
                excel_output_path = os.path.join(DATA_DIR, "signal_backtest_report.xlsx")
                html_output_path = os.path.join(DATA_DIR, "signal_backtest_report.html")
                export_to_excel(backtest_results_path, excel_output_path)
                export_to_html(backtest_results_path, html_output_path)
                log_info(f"Backtest results exported to {excel_output_path} and {html_output_path}.")


                # --- ML Model Training ---
                log_info("Attempting to train ML model...")
                if os.path.exists(backtest_results_path):
                    try:
                        model, report = train_ml_model(backtest_results_path)
                        log_info("ML model training completed.")
                        print("Classification Report:")
                        for label, metrics in report.items():
                             if isinstance(metrics, dict):
                                  print(f"  {label}:")
                                  for metric, value in metrics.items():
                                       print(f"    {metric}: {value:.4f}")
                             else:
                                  print(f"  {label}: {metrics:.4f}")

                        # Optional: Save the trained model
                        # import joblib
                        # joblib.dump(model, os.path.join(DATA_DIR, 'ml_model.pkl'))
                        # log_info("ML model saved.")


                    except ValueError as ve:
                        log_error(f"Error during ML training: {ve}. Skipping training.")
                    except Exception as e:
                        log_error(f"Error during ML training: {e}. Skipping training.")
                else:
                    log_info(f"Warning: Backtest results file not found at {backtest_results_path}. Skipping ML model training.")


            else:
                log_info("No backtest results to display. Skipping ML model training.")


        else:
             log_info("Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.")


    except Exception as e:
        log_error(f"Error processing historical data: {e}. Skipping historical signals.")
    log_info("--- Historical Data Pipeline Finished ---")


    # --- Display Results (for Top 10 snapshot) ---
    print("\\n--- Results (Top 10 Snapshot) ---")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False))
    else:
        print("No top 10 snapshot signals generated due to processing errors or no data.")

    # --- Display Results (for Historical, if processed) ---
    print("\\n--- Results (Historical Data) ---")
    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:
         print("Historical signals (tail):")
         print(history_df_signals_display.tail().to_markdown(index=False))
         print("\nHistorical signals (sample):")
         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))
    else:
         print("No historical signals generated or historical data processing skipped.")


    log_info("Bot pipeline finished.")


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

# Rewrite the content of signal_bot/main.py with the corrected structure.
main_py_content = """# main.py
import pandas as pd
import os
from utils.coingecko_api import get_top_coins, get_coin_history
from indicators.ta_utils import compute_indicators
from signals.generate_signals import generate_signal

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Fetching top 10 coin data...")
    data = get_top_coins()
    df_top10 = pd.DataFrame(data)
    df_top10["timestamp"] = pd.Timestamp.utcnow()
    df_top10.to_csv(f"{DATA_DIR}/top10_market_data.csv", index=False)
    print("Top 10 data fetched and saved.")

    print("Computing technical indicators for top 10 data...")
    # For now, we'll use the current price as 'close' for indicator calculation
    df_top10["close"] = df_top10["current_price"]
    df_ind_top10 = compute_indicators(df_top10.copy())
    df_ind_top10.to_csv(f"{DATA_DIR}/top10_with_indicators.csv", index=False)
    print("Indicators computed and saved for top 10 data.")

    # --- Historical Data Processing with Error Handling ---
    coin_id = "bitcoin"
    history_df_signals = pd.DataFrame() # Initialize as empty DataFrame

    print(f"Attempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")
        print("Historical data fetched.")

        # Process historical price data into a DataFrame
        if "prices" not in history_data:
            print(f"Warning: 'prices' key not found in historical data for {coin_id}.")
        else:
            history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])
            history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
            print("Historical data processed into DataFrame.")

            # Compute indicators on the historical data DataFrame
            print(f"Computing technical indicators for {coin_id} historical data...")
            history_df_ind = compute_indicators(history_df.copy())
            history_df_ind.to_csv(f"{DATA_DIR}/{coin_id}_historical_with_indicators.csv", index=False)
            print("Indicators computed and saved for historical data.")

            # Generate signals on the historical data
            print(f"Generating signals for {coin_id} historical data...")
            history_df_signals = generate_signal(history_df_ind)
            history_df_signals.to_csv(f"{DATA_DIR}/{coin_id}_historical_signals.csv", index=False)
            print("Historical signals generated and saved.")

    except Exception as e:
        print(f"Error processing historical data for {coin_id}: {e}. Skipping historical signals.")
        # history_df_signals is already initialized as empty

    # --- Top 10 Signal Generation (always executed) ---
    print("Generating signals for top 10 snapshot data...")
    # Note: The current generate_signal function expects a single row or a dataframe
    # with sufficient historical data. For this initial test with only current data,
    # the signals will likely be 'HOLD' or based on limited info.
    df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data
    df_signals_top10.to_csv(f"{DATA_DIR}/top10_signals.csv", index=False)
    print("Signals generated and saved for top 10 snapshot data.")

    # --- Display Results ---
    print("\\n--- Results ---")
    print("Historical signals (if generated):")
    if not history_df_signals.empty:
        print(history_df_signals.head())
    else:
        print("No historical signals generated.")

    print("\\nTop 10 snapshot signals:")
    print(df_signals_top10.head())


if __name__ == "__main__":
    main()
"""

with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

# Open the signal_bot/main.py file to read its current content.
with open('signal_bot/main.py', 'r') as f:
    current_lines = f.readlines()

print("Current content of signal_bot/main.py:")
print("".join(current_lines))

# Now, based on the printed content, manually identify the correct lines
# to use for finding the historical data block and the top 10 signal generation.

# Assuming the lines are:
# history_fetch_line_content = '    history_data = get_coin_history(coin_id, days="30")\n'
# top10_signal_generation_line_content = '    df_signals = generate_signal(df_ind)\n' # This line needs adjustment

# Re-read the file to modify
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Find the line where historical data is fetched
    history_fetch_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'history_data = get_coin_history(coin_id, days="30")' in line:
            history_fetch_line_index = i
            break

    # Find the line where top 10 signals are generated (this marks the end of the historical processing block)
    # Based on previous executions, the line might be '    df_signals = generate_signal(df_ind)\n'
    top10_signal_generation_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'df_signals = generate_signal(df_ind)' in line:
            top10_signal_generation_line_index = i
            break


    if history_fetch_line_index != -1 and top10_signal_generation_line_index != -1:
        # Extract the historical data processing block
        historical_processing_lines = main_function_lines[history_fetch_line_index:top10_signal_generation_line_index]

        # Wrap the historical data processing block in a try-except block
        wrapped_lines = [
            '    try:\n'
        ]
        for line in historical_processing_lines:
            wrapped_lines.append(f'        {line}') # Indent the original lines
        wrapped_lines.extend([
            '    except Exception as e:\n', # Catch a general exception for robustness
            '        print(f"Error processing historical data: {e}. Skipping historical signals.")\n',
            '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\n',
            '        print("Historical signals generated (if successful):")\n', # Add print statements here
            '        if not history_df_signals.empty:\n',
            '            print(history_df_signals.head())\n',
            '        else:\n',
            '            print("No historical signals generated.")\n',
            '\n'
        ])

        # Replace the original historical processing lines with the wrapped ones
        modified_main_function_lines = (
            main_function_lines[:history_fetch_line_index] +
            wrapped_lines +
            main_function_lines[top10_signal_generation_line_index:]
        )

        # Find the line where top 10 signals are saved
        top10_signals_save_line_index = -1
        for i, line in enumerate(modified_main_function_lines):
            if 'df_signals.to_csv(f"{DATA_DIR}/signals.csv", index=False)' in line:
                top10_signals_save_line_index = i
                break

        if top10_signals_save_line_index != -1:
             # Add print statements after the top 10 signal saving
            display_lines = [
                '\n',
                '    print("Top 10 snapshot signals generated:")\n',
                '    print(df_signals.head())\n',
                '\n'
            ]

            modified_main_function_lines = (
                modified_main_function_lines[:top10_signals_save_line_index + 1] +
                display_lines +
                modified_main_function_lines[top10_signals_save_line_index + 1:]
            )

        # Reconstruct the full lines
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the historical data fetch or top 10 signal generation line in main function.")
else:
    print("Could not find the main function in the file.")

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Find the line where historical data is fetched
    history_fetch_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'history_data = get_coin_history(coin_id, days="30")' in line:
            history_fetch_line_index = i
            break

    # Find the line where top 10 signals are generated (this marks the end of the historical processing block)
    top10_signal_generation_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'df_signals = generate_signal(df_ind)' in line:
            top10_signal_generation_line_index = i
            break

    if history_fetch_line_index != -1 and top10_signal_generation_line_index != -1:
        # Extract the historical data processing block
        historical_processing_lines = main_function_lines[history_fetch_line_index:top10_signal_generation_line_index]

        # Wrap the historical data processing block in a try-except block
        wrapped_lines = [
            '    try:\n'
        ]
        for line in historical_processing_lines:
            wrapped_lines.append(f'        {line}') # Indent the original lines
        wrapped_lines.extend([
            '    except Exception as e:\n', # Catch a general exception for robustness
            '        print(f"Error processing historical data: {e}. Skipping historical signals.")\n',
            '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\n',
            '        print("Historical signals generated (if successful):")\n', # Add print statements here
            '        if not history_df_signals.empty:\n',
            '            print(history_df_signals.head())\n',
            '        else:\n',
            '            print("No historical signals generated.")\n',
            '\n'
        ])

        # Replace the original historical processing lines with the wrapped ones
        modified_main_function_lines = (
            main_function_lines[:history_fetch_line_index] +
            wrapped_lines +
            main_function_lines[top10_signal_generation_line_index:]
        )

        # Find the line where top 10 signals are saved
        top10_signals_save_line_index = -1
        for i, line in enumerate(modified_main_function_lines):
            if 'df_signals.to_csv(f"{DATA_DIR}/signals.csv", index=False)' in line:
                top10_signals_save_line_index = i
                break

        if top10_signals_save_line_index != -1:
             # Add print statements after the top 10 signal saving
            display_lines = [
                '\n',
                '    print("Top 10 snapshot signals generated:")\n',
                '    print(df_signals.head())\n',
                '\n'
            ]

            modified_main_function_lines = (
                modified_main_function_lines[:top10_signals_save_line_index + 1] +
                display_lines +
                modified_main_function_lines[top10_signals_save_line_index + 1:]
            )

        # Reconstruct the full lines
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the historical data fetch or top 10 signal generation line in main function.")
else:
    print("Could not find the main function in the file.")

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Find the try block
    try_line_index = -1
    for i, line in enumerate(main_function_lines):
        if line.strip() == 'try:':
            try_line_index = i
            break

    if try_line_index != -1:
        # Find the except block
        except_line_index = -1
        for i in range(try_line_index + 1, len(main_function_lines)):
            if line.strip().startswith('except'):
                 except_line_index = i
                 break

        # If both try and except are found, we need to re-evaluate the block
        if except_line_index != -1:
             # Find the end of the except block (before the next non-except/non-blank line)
             except_block_end_index = except_line_index + 1
             while except_block_end_index < len(main_function_lines) and (main_function_lines[except_block_end_index].strip() == '' or main_function_lines[except_block_end_index].strip().startswith('#') or main_function_lines[except_block_end_index].strip().startswith('print') or main_function_lines[except_block_end_index].strip().startswith('history_df_signals =')):
                 except_block_end_index += 1


             # Extract the lines within the try-except block (including try and except)
             try_except_block = main_function_lines[try_line_index:except_block_end_index]

             # Reconstruct the try-except block with correct indentation
             reconstructed_block = ['    try:\n'] # Ensure correct indentation for try
             in_except_block = False
             for line in try_except_block[1:]:
                  if line.strip().startswith('except'):
                     reconstructed_block.append(line)
                     in_except_block = True
                  elif in_except_block:
                      reconstructed_block.append(f'    {line.lstrip()}') # Ensure correct indentation for except block content
                  else:
                      reconstructed_block.append(f'        {line.lstrip()}') # Ensure correct indentation for try block content


             # Replace the original try-except block with the reconstructed one
             modified_main_function_lines = (
                main_function_lines[:try_line_index] +
                reconstructed_block +
                main_function_lines[except_block_end_index:]
             )

             # Reconstruct the full lines
             lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

             # Write the modified content back to the file
             with open('signal_bot/main.py', 'w') as f:
                 f.writelines(lines)
        else:
            print("Could not find the except block in main function.")
    else:
        print("Could not find the try block in main function.")
else:
    print("Could not find the main function in the file.")

!python signal_bot/main.py

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Remove the 'return' statement from the historical data check
    modified_main_function_lines = [line for line in main_function_lines if 'return # Exit the function if data is not available' not in line]

    # Find the start of the historical data processing block (after the check for 'prices')
    history_df_creation_line_index = -1
    for i, line in enumerate(modified_main_function_lines):
         if 'history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])' in line:
             history_df_creation_line_index = i
             break


    # Find the end of the historical data processing block (after saving historical signals)
    history_signals_save_line_index = -1
    for i, line in enumerate(modified_main_function_lines):
        if 'history_df_signals.to_csv(f"{DATA_DIR}/{coin_id}_historical_signals.csv", index=False)' in line:
            history_signals_save_line_index = i
            break

    if history_df_creation_line_index != -1 and history_signals_save_line_index != -1:
        # Extract the historical data processing block (including the check for 'prices' but excluding the return)
        historical_processing_lines = modified_main_function_lines[history_df_creation_line_index - 2:history_signals_save_line_index + 1] # Adjust index to include the 'if "prices" not in history_data:' and print lines

        # Wrap the historical data processing block in a try-except block
        wrapped_lines = [
            '    try:\n'
        ]
        for line in historical_processing_lines:
            wrapped_lines.append(f'    {line}') # Indent the original lines
        wrapped_lines.extend([
            '    except Exception as e:\n', # Catch a general exception for robustness
            '        print(f"Error processing historical data: {e}. Skipping historical signals.")\n',
            '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\n',
        ])

        # Replace the original historical processing lines with the wrapped ones
        modified_main_function_lines = (
            modified_main_function_lines[:history_df_creation_line_index - 2] + # Adjust index
            wrapped_lines +
            modified_main_function_lines[history_signals_save_line_index + 1:]
        )

        # Reconstruct the full lines
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the historical data creation or save line in main function.")
else:
    print("Could not find the main function in the file.")

!python signal_bot/main.py

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Find the start of the historical data processing block
    history_fetch_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'history_data = get_coin_history(coin_id, days="30")' in line:
            history_fetch_line_index = i
            break

    # Find the end of the historical data processing block (after saving historical signals)
    history_signals_save_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'history_df_signals.to_csv(f"{DATA_DIR}/{coin_id}_historical_signals.csv", index=False)' in line:
            history_signals_save_line_index = i
            break

    if history_fetch_line_index != -1 and history_signals_save_line_index != -1:
        # Extract the historical data processing block
        historical_processing_lines = main_function_lines[history_fetch_line_index:history_signals_save_line_index + 1]

        # Wrap the historical data processing block in a try-except block
        wrapped_lines = [
            '    try:\n'
        ]
        for line in historical_processing_lines:
            wrapped_lines.append(f'    {line}') # Indent the original lines
        wrapped_lines.extend([
            '    except Exception as e:\n', # Catch a general exception for robustness
            '        print(f"Error processing historical data: {e}. Skipping historical signals.")\n',
            '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\n',
        ])

        # Replace the original historical processing lines with the wrapped ones
        modified_main_function_lines = (
            main_function_lines[:history_fetch_line_index] +
            wrapped_lines +
            main_function_lines[history_signals_save_line_index + 1:]
        )

        # Find the line where top 10 signals are generated
        top10_signal_generation_line_index = -1
        for i, line in enumerate(modified_main_function_lines):
            if 'df_signals = generate_signal(df_ind)' in line:
                top10_signal_generation_line_index = i
                break

        if top10_signal_generation_line_index != -1:
             # Add print statements to confirm signal generation and display head
            display_lines = [
                '\n',
                '    print("Historical signals generated (if successful):")\n',
                '    if not history_df_signals.empty:\n',
                '        print(history_df_signals.head())\n',
                '    else:\n',
                '        print("No historical signals generated.")\n',
                '\n',
                '    print("Top 10 snapshot signals generated:")\n',
                '    print(df_signals.head())\n',
                '\n'
            ]

            # Insert the display lines after the top 10 signal generation and saving lines
            top10_signals_save_line_index = -1
            for i in range(top10_signal_generation_line_index + 1, len(modified_main_function_lines)):
                 if 'df_signals.to_csv' in modified_main_function_lines[i]:
                     top10_signals_save_line_index = i
                     break

            if top10_signals_save_line_index != -1:
                modified_main_function_lines = (
                    modified_main_function_lines[:top10_signals_save_line_index + 1] +
                    display_lines +
                    modified_main_function_lines[top10_signals_save_line_index + 1:]
                )
            else:
                 print("Could not find the top 10 signals save line in main function.")

        # Reconstruct the full lines
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the historical data fetch or save line in main function.")
else:
    print("Could not find the main function in the file.")

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1


if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Find the start of the historical data processing block
    history_fetch_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'history_data = get_coin_history(coin_id, days="30")' in line:
            history_fetch_line_index = i
            break

    if history_fetch_line_index != -1:
        # Find the end of the historical data processing block (before the original signal generation for df_ind)
        original_signal_generation_line_index = -1
        for i, line in enumerate(main_function_lines):
             if 'df_signals = generate_signal(df_ind)' in line:
                 original_signal_generation_line_index = i
                 break

        if original_signal_generation_line_index != -1:
             # Extract the historical data processing block
            historical_processing_lines = main_function_lines[history_fetch_line_index:original_signal_generation_line_index]

            # Wrap the historical data processing block in a try-except block
            wrapped_lines = [
                '    try:\n'
            ]
            for line in historical_processing_lines:
                wrapped_lines.append(f'    {line}') # Indent the original lines
            wrapped_lines.extend([
                '    except KeyError:\n',
                '        print("Error processing historical data. Skipping historical signals.")\n',
                '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\n',
            ])


            # Replace the original historical processing lines with the wrapped ones
            modified_main_function_lines = (
                main_function_lines[:history_fetch_line_index] +
                wrapped_lines +
                main_function_lines[original_signal_generation_line_index:]
            )

            # Reconstruct the full lines
            lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

            # Write the modified content back to the file
            with open('signal_bot/main.py', 'w') as f:
                f.writelines(lines)
        else:
            print("Could not find the original signal generation line in main function.")
    else:
        print("Could not find the historical data fetch line in main function.")
else:
    print("Could not find the main function in the file.")

!python signal_bot/main.py

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Find the line where historical data is fetched
    history_fetch_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'history_data = get_coin_history(coin_id, days="30")' in line:
            history_fetch_line_index = i
            break

    if history_fetch_line_index != -1:
        # Remove the inspect line if it exists
        main_function_lines = [line for line in main_function_lines if 'Keys in history_data:' not in line]

        # Find the line where the history_df DataFrame is created
        history_df_creation_line_index = -1
        for i, line in enumerate(main_function_lines):
            if 'history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])' in line:
                history_df_creation_line_index = i
                break

        if history_df_creation_line_index != -1:
            # Add a check for the 'prices' key before creating the DataFrame
            check_lines = [
                '    if "prices" not in history_data:\n',
                '        print("Error: Historical data not available.")\n',
                '        return # Exit the function if data is not available\n',
            ]

            # Insert the check lines before the history_df creation line
            modified_main_function_lines = (
                main_function_lines[:history_df_creation_line_index] +
                check_lines +
                main_function_lines[history_df_creation_line_index:]
            )

            # Reconstruct the full lines
            lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

            # Write the modified content back to the file
            with open('signal_bot/main.py', 'w') as f:
                f.writelines(lines)
        else:
             print("Could not find the history_df creation line in main function.")
    else:
        print("Could not find the historical data fetch line in main function.")
else:
    print("Could not find the main function in the file.")

!python signal_bot/main.py

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Find the line where historical data is fetched
    history_fetch_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'history_data = get_coin_history(coin_id, days="30")' in line:
            history_fetch_line_index = i
            break

    if history_fetch_line_index != -1:
        # Add a print statement to inspect the keys of history_data
        inspect_line = '    print(f"Keys in history_data: {history_data.keys()}")\n'


        # Insert the inspect line after the history data fetch line
        modified_main_function_lines = (
            main_function_lines[:history_fetch_line_index + 1] +
            [inspect_line] +
            main_function_lines[history_fetch_line_index + 1:]
        )

        # Reconstruct the full lines
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the historical data fetch line in main function.")
else:
    print("Could not find the main function in the file.")

!python signal_bot/main.py

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Add import statement for get_coin_history
import_line = 'from utils.coingecko_api import get_coin_history\n'

# Find the position to insert imports (after existing imports)
insert_pos = 0
for i, line in enumerate(lines):
    if line.strip().startswith('import') or line.strip().startswith('from'):
        insert_pos = i + 1
    elif not line.strip(): # Skip empty lines
        continue
    else:
        break # Stop if we encounter non-import code

# Insert the new import line if not already present
if import_line not in lines:
    lines.insert(insert_pos, import_line)

# Write the modified content back to the file
with open('signal_bot/main.py', 'w') as f:
    f.writelines(lines)

!python signal_bot/main.py

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Find the line where historical signals are saved
    save_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'history_df_signals.to_csv' in line:
            save_line_index = i
            break

    if save_line_index != -1:
        # Add lines to print confirmation and the head of the dataframe
        display_lines = [
            '    print("Historical signals generated:")\n',
            '    print(history_df_signals.head())\n',
            '\n'
        ]

        # Insert the new lines after the save line
        modified_main_function_lines = (
            main_function_lines[:save_line_index + 1] +
            display_lines +
            main_function_lines[save_line_index + 1:]
        )

        # Reconstruct the full lines
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the historical signals save line in main function.")
else:
    print("Could not find the main function in the file.")

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1


if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Identify the line where signals are currently generated for df_ind
    signal_generation_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'df_signals = generate_signal(df_ind)' in line:
            signal_generation_line_index = i
            break

    if signal_generation_line_index != -1:
        # Modify the line to use history_df_ind and update the output filename
        modified_line = '    history_df_signals = generate_signal(history_df_ind)\n'
        save_line = '    history_df_signals.to_csv(f"{DATA_DIR}/{coin_id}_historical_signals.csv", index=False)\n'
        print_line = '    print("Historical signals generated and saved.")\n'


        # Replace the original signal generation and saving lines with the new ones
        # Find the save line after the signal generation line
        save_line_index = -1
        for i in range(signal_generation_line_index + 1, len(main_function_lines)):
            if 'df_signals.to_csv' in main_function_lines[i]:
                save_line_index = i
                break

        if save_line_index != -1:
             # Delete the original signal generation and save lines
            del main_function_lines[save_line_index]
            del main_function_lines[signal_generation_line_index]

            # Insert the new lines
            main_function_lines.insert(signal_generation_line_index, modified_line)
            main_function_lines.insert(signal_generation_line_index + 1, save_line)
            main_function_lines.insert(signal_generation_line_index + 2, print_line)


        # Reconstruct the full lines
        lines = lines[:main_start + 1] + main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the signal generation line in main function.")
else:
    print("Could not find the main function in the file.")

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1


if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Identify the line where indicators are computed for df
    indicator_computation_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'df_ind = compute_indicators(df.copy())' in line:
            indicator_computation_line_index = i
            break

    if indicator_computation_line_index != -1:
        # New lines to insert before indicator computation
        new_lines = [
            '\n',
            '    # Fetch historical data for a specific coin (e.g., bitcoin)\n',
            '    coin_id = "bitcoin"\n',
            '    print(f"Fetching historical data for {coin_id}...")\n',
            '    history_data = get_coin_history(coin_id, days="30")\n',
            '    print("Historical data fetched.")\n',
            '\n',
            '    # Process historical price data into a DataFrame\n',
            '    # The \'prices\' key contains a list of [timestamp, price]\n',
            '    history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])\n',
            '    # Convert timestamp to datetime and set as index (or keep as column)\n',
            '    history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")\n',
            '    # Optional: Set timestamp as index for time-series analysis\n',
            '    # history_df = history_df.set_index("timestamp")\n',
            '    print("Historical data processed into DataFrame.")\n',
            '\n',
            '    # Compute indicators on the historical data DataFrame\n',
            '    print(f"Computing technical indicators for {coin_id} historical data...")\n',
            '    history_df_ind = compute_indicators(history_df.copy())\n',
            '    history_df_ind.to_csv(f"{DATA_DIR}/{coin_id}_historical_with_indicators.csv", index=False)\n',
            '    print("Indicators computed and saved for historical data.")\n',
            '\n',
            '    # Note: Signal generation logic currently needs adjustment for time-series data\n',
            '    # The next step would be to adapt generate_signal to work with history_df_ind\n',
            '    # For now, we will keep the original signal generation on the top 10 snapshot for demonstration\n',
            '\n'
        ]

        # Insert the new lines before the indicator computation line
        modified_main_function_lines = (
            main_function_lines[:indicator_computation_line_index] +
            new_lines +
            main_function_lines[indicator_computation_line_index:]
        )

        # Reconstruct the full lines
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the indicator computation line in main function.")
else:
    print("Could not find the main function in the file.")

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Identify the line where indicators are computed for df
    indicator_computation_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'df_ind = compute_indicators(df.copy())' in line:
            indicator_computation_line_index = i
            break

    if indicator_computation_line_index != -1:
        # New lines to insert before indicator computation
        new_lines = [
            '\n',
            '    # Fetch historical data for a specific coin (e.g., bitcoin)\n',
            '    coin_id = "bitcoin"\n',
            '    print(f"Fetching historical data for {coin_id}...")\n',
            '    history_data = get_coin_history(coin_id, days="30")\n',
            '    print("Historical data fetched.")\n',
            '\n',
            '    # Process historical price data into a DataFrame\n',
            '    # The 'prices' key contains a list of [timestamp, price]\n',
            '    history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])\n',
            '    # Convert timestamp to datetime and set as index (or keep as column)\n',
            '    history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")\n',
            '    # Optional: Set timestamp as index for time-series analysis\n',
            '    # history_df = history_df.set_index("timestamp")\n',
            '    print("Historical data processed into DataFrame.")\n',
            '\n',
            '    # Compute indicators on the historical data DataFrame\n',
            '    print(f"Computing technical indicators for {coin_id} historical data...")\n',
            '    history_df_ind = compute_indicators(history_df.copy())\n',
            '    history_df_ind.to_csv(f"{DATA_DIR}/{coin_id}_historical_with_indicators.csv", index=False)\n',
            '    print("Indicators computed and saved for historical data.")\n',
            '\n',
            '    # Note: Signal generation logic currently needs adjustment for time-series data\n',
            '    # The next step would be to adapt generate_signal to work with history_df_ind\n',
            '    # For now, we will keep the original signal generation on the top 10 snapshot for demonstration\n',
            '\n'
        ]

        # Insert the new lines before the indicator computation line
        modified_main_function_lines = (
            main_function_lines[:indicator_computation_line_index] +
            new_lines +
            main_function_lines[indicator_computation_line_index:]
        )

        # Reconstruct the full lines
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the indicator computation line in main function.")
else:
    print("Could not find the main function in the file.")

# Open the signal_bot/main.py file for editing.
with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

# Find the start and end of the main function
main_start = -1
main_end = -1
for i, line in enumerate(lines):
    if line.strip() == 'def main():':
        main_start = i
    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):
        main_end = i
        break
    # Handle case where main is the last function
    if main_start != -1 and i == len(lines) - 1:
        main_end = i + 1

if main_start != -1 and main_end != -1:
    # Extract the main function lines
    main_function_lines = lines[main_start + 1:main_end]

    # Identify the line where indicators are computed for df
    indicator_computation_line_index = -1
    for i, line in enumerate(main_function_lines):
        if 'df_ind = compute_indicators(df.copy())' in line:
            indicator_computation_line_index = i
            break

    if indicator_computation_line_index != -1:
        # New lines to insert before indicator computation
        new_lines = [
            '\n',
            '    # Fetch historical data for a specific coin (e.g., bitcoin)\n',
            '    coin_id = "bitcoin"\n',
            '    print(f"Fetching historical data for {coin_id}...")\n',
            '    history_data = get_coin_history(coin_id, days="30")\n',
            '    print("Historical data fetched.")\n',
            '\n',
            '    # Process historical price data into a DataFrame\n',
            "    # The 'prices' key contains a list of [timestamp, price]\n", # Corrected line
            '    history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "close"])\n',
            '    # Convert timestamp to datetime and set as index (or keep as column)\n',
            '    history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")\n',
            '    # Optional: Set timestamp as index for time-series analysis\n',
            '    # history_df = history_df\n', # Corrected line (incomplete in original)
            '\n' # Added a newline to complete the block
        ]

        # Insert the new lines before the indicator computation line
        modified_main_function_lines = (
            main_function_lines[:indicator_computation_line_index] +
            new_lines +
            main_function_lines[indicator_computation_line_index:]
        )

        # Reconstruct the full lines (assuming this code block was meant to write back to main.py)
        # This part was incomplete in the user's provided code, adding the logic to write back.
        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]

        # Write the modified content back to the file
        with open('signal_bot/main.py', 'w') as f:
            f.writelines(lines)
    else:
        print("Could not find the indicator computation line in main function.")
else:
    print("Could not find the main function in the file.")

# Execute the main script again
!python -m signal_bot.main

# Execute the main script to regenerate data files
!python -m signal_bot.main

# Regenerate the signal_bot/main.py file with its latest integrated content.
import os

# Ensure the signal_bot directory exists
os.makedirs('signal_bot', exist_ok=True)
os.makedirs('signal_bot/data', exist_ok=True) # Ensure data directory exists as main uses it

# Content for main.py (latest integrated version)
main_py_content = """# main.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins, get_coin_history
from .indicators.ta_utils import compute_indicators
from .signals.signal_finder import find_signals
from .anomaly_detector import detect_anomalies
from .ml_logger import log_ml_features
from .backtester import backtest_signals
from .ml_model_trainer import train_ml_model
from .logger import setup_logger, log_info, log_error
from .exporter import export_to_excel, export_to_html

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    setup_logger()
    log_info("Starting bot pipeline...")

    # --- Data Collection (using existing top 10 fetch for now) ---
    log_info("--- Top 10 Data Pipeline ---")
    print("Fetching top 10 coin data...")
    try:
        data = get_top_coins()
        if data is not None:
            df_top10 = pd.DataFrame(data)
            df_top10["timestamp"] = pd.Timestamp.utcnow()
            top10_market_data_path = os.path.join(DATA_DIR, "top10_market_data.csv")
            df_top10.to_csv(top10_market_data_path, index=False)
            log_info("Top 10 data fetched and saved.")

            log_info("Computing technical indicators for top 10 data...")
            df_top10["close"] = df_top10["current_price"]
            df_ind_top10 = compute_indicators(df_top10.copy())
            top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
            df_ind_top10.to_csv(top10_indicators_path, index=False)
            log_info("Indicators computed and saved for top 10 data.")

            log_info("Generating signals for top 10 snapshot data...")
            df_signals_top10 = find_signals(df_ind_top10.copy())
            top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
            df_signals_top10.to_csv(top10_signals_path, index=False)
            log_info("Signals generated and saved for top 10 snapshot data.")

            log_info("Logging ML features for top 10 data...")
            ml_log_path = os.path.join(DATA_DIR, "ml_training.csv")
            log_ml_features(top10_indicators_path, ml_log_path)
            log_info("ML features logged for top 10 data.")

        else:
            log_error("Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.")
            df_signals_top10 = pd.DataFrame()

    except Exception as e:
        log_error(f"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.")
        df_signals_top10 = pd.DataFrame()
    log_info("--- Top 10 Data Pipeline Finished ---")


    # --- Anomaly Detection ---
    log_info("\\n--- Anomaly Detection Pipeline ---")
    print("Attempting to run Anomaly Detection...")
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if os.path.exists(full_snapshot_path):
        try:
            log_info("Running Anomaly Detection...")
            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
            log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")
            if not anomalies.empty:
                 print("Sample anomalies:")
                 print(anomalies.head().to_markdown(index=False))
            else:
                 print("No anomalies detected.")
        except Exception as e:
            log_error(f"Error during Anomaly Detection: {e}.")
    else:
        log_info(f"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.")
    log_info("--- Anomaly Detection Pipeline Finished ---")


    # --- Historical Data Processing ---
    log_info("\\n--- Historical Data Pipeline ---")
    coin_id = "bitcoin"
    history_df_signals_display = pd.DataFrame()
    historical_price_data_path = os.path.join(DATA_DIR, f"{coin_id}_historical_price.csv")
    historical_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv")
    backtest_results_path = os.path.join(DATA_DIR, "signal_backtest.csv")


    print(f"Attempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")
        if history_data is not None and "prices" in history_data:
            log_info("Historical data fetched.")
            history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "current_price"])
            history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
            history_df.to_csv(historical_price_data_path, index=False)
            log_info("Historical data processed and saved.")

            log_info(f"Computing technical indicators for {coin_id} historical data...")
            history_df["close"] = history_df["current_price"]
            history_df_ind = compute_indicators(history_df.copy())
            history_historical_indicators_path = os.path.join(DATA_DIR, f"{coin_id}_historical_with_indicators.csv")
            history_df_ind.to_csv(history_historical_indicators_path, index=False)
            log_info("Indicators computed and saved for historical data.")

            log_info(f"Generating signals for {coin_id} historical data...")
            history_df_signals = find_signals(history_df_ind.copy())
            history_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv")
            history_df_signals.to_csv(history_signals_path, index=False)
            log_info("Historical signals generated and saved.")
            history_df_signals_display = history_df_signals

            # --- Backtesting Historical Signals ---
            log_info(f"Attempting to backtest historical signals for {coin_id}...")
            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)
            log_info("Backtesting completed.")
            if not backtest_results_df.empty:
                print("Backtest Results (head):")
                print(backtest_results_df.head().to_markdown(index=False))

                # --- Export Backtest Results ---
                log_info("Exporting backtest results...")
                excel_output_path = os.path.join(DATA_DIR, "signal_backtest_report.xlsx")
                html_output_path = os.path.join(DATA_DIR, "signal_backtest_report.html")
                export_to_excel(backtest_results_path, excel_output_path)
                export_to_html(backtest_results_path, html_output_path)
                log_info(f"Backtest results exported to {excel_output_path} and {html_output_path}.")


                # --- ML Model Training ---
                log_info("Attempting to train ML model...")
                if os.path.exists(backtest_results_path):
                    try:
                        model, report = train_ml_model(backtest_results_path)
                        log_info("ML model training completed.")
                        print("Classification Report:")
                        for label, metrics in report.items():
                             if isinstance(metrics, dict):
                                  print(f"  {label}:")
                                  for metric, value in metrics.items():
                                       print(f"    {metric}: {value:.4f}")
                             else:
                                  print(f"  {label}: {metrics:.4f}")

                        # Optional: Save the trained model
                        # import joblib
                        # joblib.dump(model, os.path.join(DATA_DIR, 'ml_model.pkl'))
                        # log_info("ML model saved.")


                    except ValueError as ve:
                        log_error(f"Error during ML training: {ve}. Skipping training.")
                    except Exception as e:
                        log_error(f"Error during ML training: {e}. Skipping training.")
                else:
                    log_info(f"Warning: Backtest results file not found at {backtest_results_path}. Skipping ML model training.")


            else:
                log_info("No backtest results to display. Skipping ML model training.")


        else:
             log_info("Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.")


    except Exception as e:
        log_error(f"Error processing historical data: {e}. Skipping historical signals.")
    log_info("--- Historical Data Pipeline Finished ---")


    # --- Display Results (for Top 10 snapshot) ---
    print("\\n--- Results (Top 10 Snapshot) ---")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False))
    else:
        print("No top 10 snapshot signals generated due to processing errors or no data.")

    # --- Display Results (for Historical, if processed) ---
    print("\\n--- Results (Historical Data) ---")
    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:
         print("Historical signals (tail):")
         print(history_df_signals_display.tail().to_markdown(index=False))
         print("\nHistorical signals (sample):")
         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))
    else:
         print("No historical signals generated or historical data processing skipped.")


    log_info("Bot pipeline finished.") # This print statement might be the issue. Let's check the end of the string.


if __name__ == "__main__":
    main()
""" # The string literal ends here. Ensure the last lines are correct.

# Write the content to signal_bot/main.py
with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

print("signal_bot/main.py regenerated successfully.")



# Open the signal_bot/main.py file for editing.
# Add import statements at the beginning of the file for the get_top_coins function from utils.coingecko_api,
# the compute_indicators function from indicators.ta_utils, and the generate_signal function from signals.generate_signals.

with open('signal_bot/main.py', 'r') as f:
    lines = f.readlines()

import_lines = [
    'from utils.coingecko_api import get_top_coins\n',
    'from indicators.ta_utils import compute_indicators\n',
    'from signals.generate_signals import generate_signal\n'
]

# Find the position to insert imports (after existing imports or at the beginning)
insert_pos = 0
for i, line in enumerate(lines):
    if line.strip().startswith('import') or line.strip().startswith('from'):
        insert_pos = i + 1
    elif not line.strip(): # Skip empty lines
        continue
    else:
        break # Stop if we encounter non-import code

# Insert the new import lines, avoiding duplicates
for imp_line in import_lines:
    if imp_line not in lines:
        lines.insert(insert_pos, imp_line)
        insert_pos += 1 # Adjust insertion position for the next import

# Write the modified content back to the file
with open('signal_bot/main.py', 'w') as f:
    f.writelines(lines)

# Run the Streamlit dashboard again
!streamlit run signal_bot/dashboard.py

# Step 3: Run the Streamlit dashboard
# Ensure the previous cell has finished executing before running this one.
# This cell will stay in a 'working' state while the dashboard server is running.
print("Attempting to run Streamlit dashboard...")
!python -m streamlit run signal_bot/dashboard.py

# Regenerate the signal_bot/dashboard.py file with its latest content.
import os

# Ensure the signal_bot directory exists
os.makedirs('signal_bot', exist_ok=True)
os.makedirs('signal_bot/data', exist_ok=True) # Ensure data directory exists as dashboard reads from it

# Content for dashboard.py (latest integrated version)
dashboard_py_content = """# dashboard.py
import streamlit as st
import pandas as pd
import os

# Define DATA_DIR
DATA_DIR = 'signal_bot/data'

st.title("Crypto Signal Dashboard")

tab1, tab2, tab3 = st.tabs(["Signals", "Anomalies", "Indicators"])

# Ensure files exist before trying to read
signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
anomalies_dir = DATA_DIR
indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")

with tab1:
    st.subheader("Signal Feed (Top 10)")
    if os.path.exists(signals_path):
        try:
            signals = pd.read_csv(signals_path)
            st.dataframe(signals)
        except Exception as e:
            st.error(f"Error loading signals data: {e}")
    else:
        st.info(f"Signal data not found at {signals_path}. Run the main pipeline first.")

with tab2:
    st.subheader("Anomalies")
    anomaly_files = [f for f in os.listdir(anomalies_dir) if f.startswith('anomalies_') and f.endswith('.csv')]
    anomaly_files.sort(reverse=True)

    if anomaly_files:
        latest_anomaly_file = os.path.join(anomalies_dir, anomaly_files[0])
        st.info(f"Displaying latest anomalies from: {latest_anomaly_file}")
        try:
            anomalies = pd.read_csv(latest_anomaly_file)
            st.dataframe(anomalies)
        except Exception as e:
             st.error(f"Error loading anomalies data from {latest_anomaly_file}: {e}")
    else:
        st.info(f"No anomaly files found in {anomalies_dir}. Run the main pipeline with anomaly detection enabled.")

with tab3:
    st.subheader("Top 10 Indicators")
    if os.path.exists(indicators_path):
        try:
            indicators = pd.read_csv(indicators_path)
            chart_cols = ["rsi", "macd_diff"]
            available_chart_cols = [col for col in chart_cols if col in indicators.columns and pd.api.types.is_numeric_dtype(indicators[col])]

            if available_chart_cols:
                 st.subheader("Indicator Trends (RSI, MACD Diff)")
                 if not indicators[available_chart_cols].dropna().empty:
                      st.line_chart(indicators[available_chart_cols])
                 else:
                      st.info(f"Indicator data for charting ({available_chart_cols}) contains only NaN values or is empty. Cannot display chart.")
            elif chart_cols:
                 st.warning(f"Required numeric columns for charting not found or are not numeric: {chart_cols}")
            else:
                 st.info("No indicator data available for charting.")

            st.subheader("Full Indicator Data")
            st.dataframe(indicators)

        except Exception as e:
            st.error(f"Error loading indicators data: {e}")
    else:
        st.info(f"Indicator data not found at {indicators_path}. Run the main pipeline first.")

st.sidebar.subheader("How to run the dashboard:")
st.sidebar.markdown("1. Ensure you have Streamlit installed (`pip install streamlit`).")
st.sidebar.markdown("2. Run the main pipeline (`python -m signal_bot.main`) to generate data files.")
st.sidebar.markdown("3. Open a terminal in your project's root directory (`/content/` in Colab).")
st.sidebar.markdown("4. Run the command: `streamlit run signal_bot/dashboard.py`")
st.sidebar.markdown("5. A local URL will be provided to view the dashboard.")
"""

# Write the content to signal_bot/dashboard.py
with open('signal_bot/dashboard.py', 'w') as f:
    f.write(dashboard_py_content)

print("signal_bot/dashboard.py regenerated successfully.")

# Step 2: Run the main pipeline to generate data files
# This will fetch data, compute indicators, generate signals, backtest, and log
print("Running main pipeline...")
!python -m signal_bot.main
print("Main pipeline execution completed.")

# Step 1: Install dependencies (including streamlit)
# Ensure required libraries are installed
!pip install -q -r signal_bot/requirements.txt
print("Dependencies installed.")

# Optional: Verify streamlit installation
!pip show streamlit

# Regenerate the signal_bot/main.py file with its latest integrated content (fixing syntax error).
import os

# Ensure the signal_bot directory and data directory exist
os.makedirs('signal_bot', exist_ok=True)
os.makedirs('signal_bot/data', exist_ok=True)

# Content for main.py (latest integrated version)
main_py_content = """# main.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins, get_coin_history
from .indicators.ta_utils import compute_indicators
# from .signals.generate_signals import generate_signal
from .signals.signal_finder import find_signals
from .anomaly_detector import detect_anomalies
from .ml_logger import log_ml_features
from .backtester import backtest_signals
from .ml_model_trainer import train_ml_model
from .logger import setup_logger, log_info, log_error
from .exporter import export_to_excel, export_to_html

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    setup_logger()
    log_info("Starting bot pipeline...")

    # --- Data Collection (using existing top 10 fetch for now) ---
    log_info("--- Top 10 Data Pipeline ---")
    print("Fetching top 10 coin data...")
    try:
        data = get_top_coins()
        if data is not None:
            df_top10 = pd.DataFrame(data)
            df_top10["timestamp"] = pd.Timestamp.utcnow()
            top10_market_data_path = os.path.join(DATA_DIR, "top10_market_data.csv")
            df_top10.to_csv(top10_market_data_path, index=False)
            log_info("Top 10 data fetched and saved.")

            log_info("Computing technical indicators for top 10 data...")
            df_top10["close"] = df_top10["current_price"]
            df_ind_top10 = compute_indicators(df_top10.copy())
            top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
            df_ind_top10.to_csv(top10_indicators_path, index=False)
            log_info("Indicators computed and saved for top 10 data.")

            log_info("Generating signals for top 10 snapshot data...")
            df_signals_top10 = find_signals(df_ind_top10.copy())
            top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
            df_signals_top10.to_csv(top10_signals_path, index=False)
            log_info("Signals generated and saved for top 10 snapshot data.")

            log_info("Logging ML features for top 10 data...")
            ml_log_path = os.path.join(DATA_DIR, "ml_training.csv")
            log_ml_features(top10_indicators_path, ml_log_path)
            log_info("ML features logged for top 10 data.")

        else:
            log_error("Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.")
            df_signals_top10 = pd.DataFrame()

    except Exception as e:
        log_error(f"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.")
        df_signals_top10 = pd.DataFrame()
    log_info("--- Top 10 Data Pipeline Finished ---")


    # --- Anomaly Detection ---
    log_info("\\n--- Anomaly Detection Pipeline ---")
    print("Attempting to run Anomaly Detection...")
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if os.path.exists(full_snapshot_path):
        try:
            log_info("Running Anomaly Detection...")
            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
            log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")
            if not anomalies.empty:
                 print("Sample anomalies:")
                 print(anomalies.head().to_markdown(index=False))
            else:
                 print("No anomalies detected.")
        except Exception as e:
            log_error(f"Error during Anomaly Detection: {e}.")
    else:
        log_info(f"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.")
    log_info("--- Anomaly Detection Pipeline Finished ---")


    # --- Historical Data Processing ---
    log_info("\\n--- Historical Data Pipeline ---")
    coin_id = "bitcoin"
    history_df_signals_display = pd.DataFrame()
    historical_price_data_path = os.path.join(DATA_DIR, f"{coin_id}_historical_price.csv")
    historical_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv")
    backtest_results_path = os.path.join(DATA_DIR, "signal_backtest.csv")


    print(f"Attempting to fetch and process historical data for {coin_id}...")
    try:
        history_data = get_coin_history(coin_id, days="30")
        if history_data is not None and "prices" in history_data:
            log_info("Historical data fetched.")
            history_df = pd.DataFrame(history_data["prices"], columns=["timestamp", "current_price"])
            history_df["timestamp"] = pd.to_datetime(history_df["timestamp"], unit="ms")
            history_df.to_csv(historical_price_data_path, index=False)
            log_info("Historical data processed and saved.")

            log_info(f"Computing technical indicators for {coin_id} historical data...")
            history_df["close"] = history_df["current_price"]
            history_df_ind = compute_indicators(history_df.copy())
            history_historical_indicators_path = os.path.join(DATA_DIR, f"{coin_id}_historical_with_indicators.csv")
            history_df_ind.to_csv(history_historical_indicators_path, index=False)
            log_info("Indicators computed and saved for historical data.")

            log_info(f"Generating signals for {coin_id} historical data...")
            history_df_signals = find_signals(history_df_ind.copy())
            history_signals_path = os.path.join(DATA_DIR, f"{coin_id}_historical_signals.csv")
            history_df_signals.to_csv(history_signals_path, index=False)
            log_info("Historical signals generated and saved.")
            history_df_signals_display = history_df_signals

            # --- Backtesting Historical Signals ---
            log_info(f"Attempting to backtest historical signals for {coin_id}...")
            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)
            log_info("Backtesting completed.")
            if not backtest_results_df.empty:
                print("Backtest Results (head):")
                print(backtest_results_df.head().to_markdown(index=False))

                # --- Export Backtest Results ---
                log_info("Exporting backtest results...")
                excel_output_path = os.path.join(DATA_DIR, "signal_backtest_report.xlsx")
                html_output_path = os.path.join(DATA_DIR, "signal_backtest_report.html")
                export_to_excel(backtest_results_path, excel_output_path)
                export_to_html(backtest_results_path, html_output_path)
                log_info(f"Backtest results exported to {excel_output_path} and {html_output_path}.")


                # --- ML Model Training ---
                log_info("Attempting to train ML model...")
                if os.path.exists(backtest_results_path):
                    try:
                        model, report = train_ml_model(backtest_results_path)
                        log_info("ML model training completed.")
                        print("Classification Report:")
                        for label, metrics in report.items():
                             if isinstance(metrics, dict):
                                  print(f"  {label}:")
                                  for metric, value in metrics.items():
                                       print(f"    {metric}: {value:.4f}")
                             else:
                                  print(f"  {label}: {metrics:.4f}")

                        # Optional: Save the trained model
                        # import joblib
                        # joblib.dump(model, os.path.join(DATA_DIR, 'ml_model.pkl'))
                        # log_info("ML model saved.")


                    except ValueError as ve:
                        log_error(f"Error during ML training: {ve}. Skipping training.")
                    except Exception as e:
                        log_error(f"Error during ML training: {e}. Skipping training.")
                else:
                    log_info(f"Warning: Backtest results file not found at {backtest_results_path}. Skipping ML model training.")


            else:
                log_info("No backtest results to display. Skipping ML model training.")


        else:
             log_info("Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.")


    except Exception as e:
        log_error(f"Error processing historical data: {e}. Skipping historical signals.")
    log_info("--- Historical Data Pipeline Finished ---")


    # --- Display Results (for Top 10 snapshot) ---
    print("\\n--- Results (Top 10 Snapshot) ---")
    if not df_signals_top10.empty:
        print(df_signals_top10.head().to_markdown(index=False))
    else:
        print("No top 10 snapshot signals generated due to processing errors or no data.")

    # --- Display Results (for Historical, if processed) ---
    print("\\n--- Results (Historical Data) ---")
    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:
         print("Historical signals (tail):")
         print(history_df_signals_display.tail().to_markdown(index=False))
         print("\nHistorical signals (sample):")
         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))
    else:
         print("No historical signals generated or historical data processing skipped.")


    log_info("Bot pipeline finished.")


if __name__ == "__main__":
    main()
"""

# Write the content to signal_bot/main.py
with open('signal_bot/main.py', 'w') as f:
    f.write(main_py_content)

print("signal_bot/main.py regenerated successfully.")

# Regenerate requirements.txt with all known dependencies
import os

DATA_DIR = 'signal_bot' # Requirements.txt is at the root of signal_bot

# Ensure signal_bot directory exists
os.makedirs(DATA_DIR, exist_ok=True)

requirements_content = """pandas
ta
scikit-learn
streamlit
apscheduler
requests
"""

with open(os.path.join(DATA_DIR, 'requirements.txt'), 'w') as f:
    f.write(requirements_content)

print("signal_bot/requirements.txt regenerated successfully.")

# Step 3: Run the Streamlit dashboard
# Ensure the previous cell has finished executing before running this one.
# This cell will stay in a 'working' state while the dashboard server is running.
print("Attempting to run Streamlit dashboard...")
!streamlit run signal_bot/dashboard.py

# Execute scheduler.py to trigger manual test execution of jobs
!python -m signal_bot.scheduler

# Regenerate signal_bot/utils/coingecko_api.py
import os

os.makedirs('signal_bot/utils', exist_ok=True)

coingecko_api_content = """# utils/coingecko_api.py
import requests

BASE_URL = "https://api.coingecko.com/api/v3"

def get_top_coins(limit=10):
    url = f"{BASE_URL}/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": limit,
        "page": 1,
        "sparkline": "true"
    }
    try:
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes
        return response.json()
    except requests.exceptions.RequestException as e:
        print(f"Error fetching top coins: {e}")
        return None

def get_coin_history(coin_id, days="30"):
    url = f"{BASE_URL}/coins/{coin_id}/market_chart"
    params = {"vs_currency": "usd", "days": days} # Removed 'interval': 'hourly'
    try:
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes
        data = response.json()
        return data
    except requests.exceptions.RequestException as e:
        print(f"Error fetching historical data for {coin_id}: {e}")
        if 'response' in locals() and response is not None:
             print(f"Response status code: {response.status_code}")
             print(f"Response text: {response.text}")
        return None
"""

with open('signal_bot/utils/coingecko_api.py', 'w') as f:
    f.write(coingecko_api_content)

print("signal_bot/utils/coingecko_api.py regenerated.")

# Regenerate signal_bot/indicators/ta_utils.py
os.makedirs('signal_bot/indicators', exist_ok=True)

ta_utils_content = """# indicators/ta_utils.py
import pandas as pd
import ta

def compute_indicators(df):
    df = df.copy()
    # Ensure required columns are present before computing indicators
    if 'close' not in df.columns:
        print("Error: 'close' column not found for indicator computation.")
        return df # Return original df or handle error appropriately

    df['rsi'] = ta.momentum.RSIIndicator(df['close']).rsi()
    df['ema_20'] = ta.trend.EMAIndicator(df['close'], window=20).ema_indicator()
    df['macd'] = ta.trend.MACD(df['close']).macd_diff() # Use macd_diff directly
    bb = ta.volatility.BollingerBands(df['close'], window=20)
    df['bb_upper'] = bb.bollinger_hband()
    df['bb_lower'] = bb.bollinger_lband()
    return df
"""
with open('signal_bot/indicators/ta_utils.py', 'w') as f:
    f.write(ta_utils_content)
print("signal_bot/indicators/ta_utils.py regenerated.")


# Regenerate signal_bot/signals/generate_signals.py (Keeping it for potential future use, although signal_finder is used)
os.makedirs('signal_bot/signals', exist_ok=True)

generate_signals_content = """# signals/generate_signals.py
import pandas as pd

def generate_signal(df):
    signals = []
    # This is a basic signal function, find_signals is more developed
    if df.empty or 'rsi' not in df.columns or 'macd' not in df.columns or 'close' not in df.columns or 'ema_20' not in df.columns:
        df['signal'] = 'HOLD'
        return df[['signal']]

    latest_row = df.iloc[-1]

    sigs = []
    if latest_row["rsi"] < 30:
        sigs.append("BUY_RSI")
    # Add other basic signals if needed
    signals.append(", ".join(sigs) if sigs else "HOLD")

    df['signal'] = signals # Assign the generated signals (assuming single row for simplicity)
    return df[['signal']] # Return only the signal column as a DataFrame
"""
with open('signal_bot/signals/generate_signals.py', 'w') as f:
    f.write(generate_signals_content)
print("signal_bot/signals/generate_signals.py regenerated.")

# Regenerate signal_bot/signals/signal_finder.py (Corrected version with NaN handling)
signal_finder_content = """# signals/signal_finder.py
import pandas as pd
import numpy as np

def find_signals(df):
    """Detect signals based on indicator thresholds."""
    # print("\\n--- Inside find_signals ---") # Debug print
    # print("Input DataFrame columns:", df.columns.tolist()) # Debug print
    # print("Input DataFrame head:\\n", df.head().to_markdown(index=False)) # Debug print


    # Ensure DataFrame has required columns
    required_cols = ["rsi", "macd_diff", "current_price", "bb_upper", "bb_lower"]
    if not all(col in df.columns for col in required_cols):
        print(f"Warning: Missing required columns for signal generation: {required_cols}. Returning HOLD signals.")
        df["signal"] = "HOLD"
        return df

    # Initialize signal column
    df["signal"] = "HOLD"

    # Add a column for previous MACD diff for crossover check
    df["macd_diff_prev"] = df["macd_diff"].shift(1)

    # Check if there are any rows with valid indicator values
    # Dropping rows with NaNs in required indicator columns for this check
    if df[required_cols].dropna().empty:
         # print("Warning: All required indicator values are NaN. Returning HOLD signals.")
         if 'macd_diff_prev' in df.columns:
              df = df.drop(columns=["macd_diff_prev"])
         # print("--- Exiting find_signals ---\\n") # Debug print
         return df


    for index, row in df.iterrows():
        sigs = []

        # Skip rows with NaN indicators for signal generation
        if pd.isna(row["rsi"]) or pd.isna(row["macd_diff"]) or pd.isna(row["current_price"]) or pd.isna(row["bb_upper"]) or pd.isna(row["bb_lower"]):
            continue

        # RSI signals
        if row["rsi"] < 30:
            sigs.append("BUY_RSI_OVERSOLD")
        if row["rsi"] > 70:
            sigs.append("SELL_RSI_OVERBOUGHT")

        # MACD signals - check for cross above/below 0
        if not pd.isna(row["macd_diff_prev"]):
            if row["macd_diff"] > 0 and row["macd_diff_prev"] <= 0:
                sigs.append("BUY_MACD_CROSS")
            elif row["macd_diff"] < 0 and row["macd_diff_prev"] >= 0:
                sigs.append("SELL_MACD_CROSS")

        # Bollinger Bands signals
        if row["current_price"] > row["bb_upper"]:
            sigs.append("OVERBOUGHT_VOLATILE")
        if row["current_price"] < row["bb_lower"]:
            sigs.append("POTENTIAL_BREAKOUT")

        # Combine signals for the current row
        df.loc[index, "signal"] = ", ".join(sigs) if sigs else "HOLD"

    # Drop the temporary column
    df = df.drop(columns=["macd_diff_prev"])
    # print("--- Exiting find_signals ---\\n") # Debug print

    return df
"""
with open('signal_bot/signals/signal_finder.py', 'w') as f:
    f.write(signal_finder_content)
print("signal_bot/signals/signal_finder.py regenerated.")


# Regenerate signal_bot/anomaly_detector.py
anomaly_detector_content = """# anomaly_detector.py
import pandas as pd
from sklearn.ensemble import IsolationForest
from datetime import datetime
import os

def load_market_snapshot(snapshot_csv):
    """Load full market snapshot from CSV."""
    try:
        df = pd.read_csv(snapshot_csv, parse_dates=["timestamp"])
        return df
    except FileNotFoundError:
        print(f"Error: Market snapshot file not found at {snapshot_csv}.")
        return pd.DataFrame() # Return empty DataFrame on error
    except Exception as e:
        print(f"Error loading market snapshot from {snapshot_csv}: {e}")
        return pd.DataFrame() # Return empty DataFrame on other errors


def prepare_features(df):
    """Create features for anomaly detection (e.g., price % change)."""
    if df.empty or "price_change_percentage_24h" not in df.columns:
         print("Warning: DataFrame is empty or missing 'price_change_percentage_24h' for anomaly detection.")
         return pd.DataFrame() # Return empty if essential data is missing

    # Ensure the column is numeric, coerce errors to NaN
    df["price_change_percentage_24h"] = pd.to_numeric(df["price_change_percentage_24h"], errors='coerce')
    df = df.dropna(subset=["price_change_percentage_24h"]) # Drop rows where conversion failed

    if df.empty:
         print("Warning: No valid data remaining after cleaning for anomaly detection.")
         return pd.DataFrame() # Return empty if no valid data

    df["pct_change_24h"] = df["price_change_percentage_24h"].fillna(0) # Fill any remaining NaNs after dropna (unlikely here but safe)
    return df[["id", "symbol", "name", "pct_change_24h", "timestamp"]].copy()

def run_isolation_forest(df, contamination=0.01):
    """Apply Isolation Forest to detect anomalies based on % change."""
    if df.empty or "pct_change_24h" not in df.columns:
         print("Warning: No features available for Isolation Forest.")
         return pd.DataFrame() # Return empty if no features

    # Ensure contamination is valid
    if not (0 < contamination < 0.5):
         print(f"Warning: Invalid contamination value {contamination}. Using default (0.1) or adjusting.")
         # Adjust contamination if necessary, or use a default. Let's ensure it's between 0 and 0.5
         contamination = max(0.001, min(0.499, contamination)) # Clamp value if outside reasonable range


    # Ensure there are enough samples for Isolation Forest
    if len(df) < 2: # Isolation Forest needs at least 2 samples
         print("Warning: Not enough data points for Isolation Forest (need at least 2).")
         df["anomaly_score"] = 1 # Tag all as not anomaly or handle as appropriate
         return df


    model = IsolationForest(contamination=contamination, random_state=42)
    try:
        # Reshape the feature data for the model
        X = df[["pct_change_24h"]].values
        df["anomaly_score"] = model.fit_predict(X)
        return df
    except Exception as e:
         print(f"Error running Isolation Forest: {e}")
         df["anomaly_score"] = 1 # Tag all as not anomaly on error
         return df


def save_anomalies(df, output_dir="data"):
    """Save anomalies to timestamped CSV."""
    if df.empty or "anomaly_score" not in df.columns:
         print("Warning: No anomalies to save.")
         return None, pd.DataFrame()

    timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    anomalies = df[df["anomaly_score"] == -1].copy() # Use .copy() to avoid SettingWithCopyWarning
    if anomalies.empty:
         print("No anomalies found to save.")
         return None, pd.DataFrame()


    path = os.path.join(output_dir, f"anomalies_{timestamp}.csv")
    os.makedirs(output_dir, exist_ok=True)
    try:
        anomalies.to_csv(path, index=False)
        print(f"Anomalies saved to {path}")
        return path, anomalies
    except Exception as e:
        print(f"Error saving anomalies to {path}: {e}")
        return None, pd.DataFrame()


def detect_anomalies(snapshot_csv, output_dir="data"):
    """End-to-end anomaly detection workflow."""
    print(f"Starting anomaly detection from {snapshot_csv}")
    df = load_market_snapshot(snapshot_csv)
    if df.empty:
         print("Anomaly detection skipped due to no data.")
         return None, pd.DataFrame()

    features = prepare_features(df)
    if features.empty:
         print("Anomaly detection skipped due to no valid features.")
         return None, pd.DataFrame()

    with_scores = run_isolation_forest(features)
    if with_scores.empty: # Check if Isolation Forest returned empty
         print("Anomaly detection skipped after Isolation Forest due to no results.")
         return None, pd.DataFrame()

    path, anomalies = save_anomalies(with_scores, output_dir)

    # Return the path and the anomalies DataFrame
    return path, anomalies

# Example Usage (can be removed or commented out)
# if __name__ == '__main__':
#     # Assuming signal_bot/data/full_market_snapshot.csv exists
#     DATA_DIR = 'signal_bot/data'
#     snapshot_path = os.path.join(DATA_DIR, 'full_market_snapshot.csv')
#     if os.path.exists(snapshot_path):
#         anomaly_file_path, detected_anomalies = detect_anomalies(snapshot_path, DATA_DIR)
#         if anomaly_file_path:
#             print(f"Detected {len(detected_anomalies)} anomalies.")
#             # print(detected_anomalies.head().to_markdown(index=False))
#     else:
#          print(f"Input file not found for anomaly detection: {snapshot_path}")

"""
with open('signal_bot/anomaly_detector.py', 'w') as f:
    f.write(anomaly_detector_content)
print("signal_bot/anomaly_detector.py regenerated.")

# Regenerate signal_bot/ml_logger.py
ml_logger_content = """# ml_logger.py
import pandas as pd
import os
from datetime import datetime

def log_ml_features(indicator_csv, output_csv):
    """Append new features to ML training dataset."""
    try:
        df = pd.read_csv(indicator_csv)

        required_features = [
            "id", "timestamp", "current_price",
            "rsi", "ema_20", "macd", # Use 'macd' as computed in ta_utils
            "bb_upper", "bb_lower"
        ]

        # Check if all required features are in the DataFrame and are numeric
        missing_features = [col for col in required_features if col not in df.columns]
        if missing_features:
            print(f"Warning: Missing required features for ML logging in {indicator_csv}: {missing_features}. Skipping logging for this file.")
            return pd.DataFrame()

        # Ensure numeric columns are numeric, handle potential errors
        numeric_cols = ["current_price", "rsi", "ema_20", "macd", "bb_upper", "bb_lower"]
        for col in numeric_cols:
             if col in df.columns:
                  df[col] = pd.to_numeric(df[col], errors='coerce')

        # Drop rows where essential numeric features could not be converted
        df = df.dropna(subset=[col for col in numeric_cols if col in df.columns])

        if df.empty:
            print(f"Warning: No valid numeric data remaining for ML logging from {indicator_csv} after cleaning. Skipping logging.")
            return pd.DataFrame()


        features = df[required_features].copy()

        if os.path.exists(output_csv):
            existing = pd.read_csv(output_csv)
            # Ensure timestamp is datetime for correct dropping duplicates
            existing['timestamp'] = pd.to_datetime(existing['timestamp'])
            features['timestamp'] = pd.to_datetime(features['timestamp'])

            combined = pd.concat([existing, features]).drop_duplicates(subset=["id", "timestamp"]).sort_values(by=["id", "timestamp"])
        else:
            combined = features

        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_csv) if os.path.dirname(output_csv) else '.', exist_ok=True)

        combined.to_csv(output_csv, index=False)
        return combined

    except FileNotFoundError:
        print(f"Error: Input file for ML logging not found at {indicator_csv}. Skipping logging.")
        return pd.DataFrame()
    except Exception as e:
        print(f"Error during ML feature logging for {indicator_csv}: {e}. Skipping logging.")
        return pd.DataFrame()

# Example Usage (can be removed or commented out)
# if __name__ == '__main__':
#     # Assuming data/top10_with_indicators.csv exists
#     DATA_DIR = 'signal_bot/data'
#     indicators_path = os.path.join(DATA_DIR, 'top10_with_indicators.csv')
#     ml_output_path = os.path.join(DATA_DIR, 'ml_training.csv')
#     if os.path.exists(indicators_path):
#         log_ml_features(indicators_path, ml_output_path)
#     else:
#          print(f"Input file not found for ML logging: {indicators_path}")

"""
with open('signal_bot/ml_logger.py', 'w') as f:
    f.write(ml_logger_content)
print("signal_bot/ml_logger.py regenerated.")


# Regenerate signal_bot/backtester.py
backtester_content = """# backtester.py
import pandas as pd
import os
import numpy as np # Import numpy for isna

def backtest_signals(signal_csv, price_csv, threshold=0.05, window=6):
    """
    Backtests trading signals against historical price data.

    Args:
        signal_csv (str): Path to the CSV file containing signals.
        price_csv (str): Path to the CSV file containing historical price data.
        threshold (float): The percentage price increase considered a successful BUY signal.
        window (int): The number of future price points (rows) to consider after a signal.

    Returns:
        pd.DataFrame: DataFrame containing backtest results.
    """
    print("\\n--- Inside backtest_signals ---")
    try:
        signals = pd.read_csv(signal_csv, parse_dates=["timestamp"])
        prices = pd.read_csv(price_csv, parse_dates=["timestamp"])
        print(f"Successfully loaded {signal_csv} and {price_csv} for backtesting.")

    except FileNotFoundError as e:
        print(f"Error loading data for backtesting: {e}")
        print("--- Exiting backtest_signals ---\\n")
        return pd.DataFrame()
    except Exception as e:
        print(f"Error reading CSV files for backtesting: {e}")
        print("--- Exiting backtest_signals ---\\n")
        return pd.DataFrame()


    results = []
    # Ensure data is sorted for correct future price lookup
    signals = signals.sort_values("timestamp")
    prices = prices.sort_values("timestamp")

    # Filter for BUY signals using a more explicit method
    if 'signal' not in signals.columns:
         print("Warning: 'signal' column not found in signals CSV. Cannot backtest.")
         print("--- Exiting backtest_signals ---\\n")
         return pd.DataFrame()

    # Ensure the 'signal' column is treated as string
    signals['signal'] = signals['signal'].astype(str).fillna('') # Fill NaN with empty string for contains check

    # Use a boolean mask with apply and lambda for filtering
    buy_signals = signals[signals['signal'].apply(lambda x: 'BUY' in x)].copy()

    print(f"Found {len(buy_signals)} BUY signals for backtesting after filtering.")


    if buy_signals.empty:
        print("No BUY signals found for backtesting.")
        print("--- Exiting backtest_signals ---\\n")
        return pd.DataFrame()

    # Ensure prices DataFrame has necessary columns
    if "id" not in prices.columns or "timestamp" not in prices.columns or "current_price" not in prices.columns:
         print("Error: Required columns ('id', 'timestamp', 'current_price') missing in prices DataFrame for lookup.")
         print("--- Exiting backtest_signals ---\\n")
         return pd.DataFrame()


    # Iterate through BUY signals and find future prices
    for index, signal in buy_signals.iterrows():
        # print(f"Backtesting signal: {signal['signal']} at {signal['timestamp']}") # Debug print (can be verbose)
        coin_id = signal["id"]
        signal_time = signal["timestamp"]

        # Get the price at the signal time from the prices DataFrame
        price_at_signal_row = prices[
            (prices["id"] == coin_id) &
            (prices["timestamp"] == signal_time)
        ]
        if price_at_signal_row.empty:
             # This can happen if signals timestamp doesn't exactly match a price timestamp
             # Find the closest price point at or before the signal time as an alternative
             closest_price_row = prices[
                 (prices["id"] == coin_id) &
                 (prices["timestamp"] <= signal_time)
             ].sort_values("timestamp", ascending=False).head(1)

             if closest_price_row.empty:
                  # print(f"Warning: Price at or before signal time not found for {coin_id} at {signal_time}. Skipping.") # Debug print
                  continue # Skip this signal if no price found
             price_at_signal = closest_price_row.iloc[0]["current_price"]
             # print(f"Using closest price {price_at_signal} at {closest_price_row.iloc[0]['timestamp']} for signal at {signal_time}") # Debug print

        else:
             price_at_signal = price_at_signal_row.iloc[0]["current_price"]
             # print(f"Using exact price {price_at_signal} at {signal_time}") # Debug print


        # Filter for future prices of this coin within the window (next 'window' data points)
        # Find the index of the current signal in the sorted prices DataFrame for this coin
        price_indices_for_coin = prices[prices["id"] == coin_id].sort_values("timestamp").index
        signal_price_index_loc = price_indices_for_coin[price_indices_for_coin >= price_at_signal_row.index[0] if not price_at_signal_row.empty else price_indices_for_coin[price_indices_for_coin <= closest_price_row.index[0]].max()] # Find index of exact or closest price

        if pd.isna(signal_price_index_loc): # Handle case where closest price lookup failed
             # print(f"Warning: Could not find price index for signal at {signal_time}. Skipping.") # Debug print
             continue


        # Select the next 'window' rows starting from the row after the signal price index
        try:
            start_idx_in_full_prices = prices.index.get_loc(signal_price_index_loc) # Get the integer location in the full prices df
            future_prices = prices.iloc[start_idx_in_full_prices + 1 : start_idx_in_full_prices + 1 + window] # Get the next 'window' rows
            # print(f"Selected {len(future_prices)} future prices for window.") # Debug print
        except IndexError:
             # print(f"Warning: Not enough future data points within window for {coin_id} at index {signal_price_index_loc}. Skipping backtest for this signal.") # Debug print
             continue # Not enough future data


        if not future_prices.empty:
            # Find the maximum price within the future window
            if "current_price" not in future_prices.columns:
                 print("Error: 'current_price' column not found in future prices DataFrame subset.")
                 continue
            max_price = future_prices["current_price"].max()
            return_pct = (max_price - price_at_signal) / price_at_signal

            result = {
                "id": coin_id,
                "timestamp": signal_time,
                "signal": signal["signal"],
                "price_at_signal": price_at_signal,
                "max_future_price": max_price,
                "return_pct": return_pct,
                "success": return_pct >= threshold
            }
            results.append(result)
        # else: # No future prices found within the window (handled by continue in try block)
             # print(f"Warning: No future price data found within window for {coin_id} at {signal_time}. Skipping backtest for this signal.") # Debug print


    if not results:
         print("No backtest results generated (either no BUY signals processed or no successful trades based on criteria).")
         print("--- Exiting backtest_signals ---\\n")
         return pd.DataFrame()

    result_df = pd.DataFrame(results)
    output_path = "data/signal_backtest.csv"
    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)

    try:
        result_df.to_csv(output_path, index=False)
        print(f"Backtest results saved to {output_path}")
    except Exception as e:
        print(f"Error saving backtest results to {output_path}: {e}")

    print("--- Exiting backtest_signals ---\\n")
    return result_df
"""
with open('signal_bot/backtester.py', 'w') as f:
    f.write(backtester_content)
print("signal_bot/backtester.py regenerated.")


# Regenerate signal_bot/ml_model_trainer.py
ml_model_trainer_content = """# ml_model_trainer.py
import pandas as pd
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import os

def train_ml_model(training_csv):
    """
    Trains an ML model (RandomForestClassifier) on historical signal outcomes.

    Args:
        training_csv (str): Path to the CSV file containing training data
                            (e.g., backtest results with 'success' column).

    Returns:
        tuple: A tuple containing the trained model and the classification report (dict).
               Returns (None, None) if training fails or data is insufficient.
    """
    print("\\n--- Inside train_ml_model ---")
    try:
        if not os.path.exists(training_csv):
            print(f"Error: Training data file not found at {training_csv}.")
            print("--- Exiting train_ml_model ---\\n")
            return None, None

        df = pd.read_csv(training_csv)
        print(f"Successfully loaded training data from {training_csv}.")

        # Ensure required columns exist and are numeric
        required_features = ["rsi", "ema_20", "macd", "bb_upper", "bb_lower"] # Use 'macd' as computed
        required_label = "success"

        if not all(col in df.columns for col in required_features + [required_label]):
            missing = [col for col in required_features + [required_label] if col not in df.columns]
            print(f"Error: Missing required columns for training: {missing}. Skipping training.")
            print("--- Exiting train_ml_model ---\\n")
            return None, None

        # Ensure feature columns are numeric, coerce errors
        for col in required_features:
             df[col] = pd.to_numeric(df[col], errors='coerce')

        # Drop rows with NaN in required features or label
        df = df.dropna(subset=required_features + [required_label])

        if df.empty:
            print("Error: No valid data remaining after dropping NaNs for training. Skipping training.")
            print("--- Exiting train_ml_model ---\\n")
            return None, None

        print(f"Using {len(df)} valid rows for training.")


        # Define features (X) and label (y)
        X = df[required_features]
        y = df[required_label].astype(int) # Ensure label is integer


        # Check if there are at least two classes in the label (for classification)
        if y.nunique() < 2:
            print(f"Error: Training data contains only one class ({y.iloc[0]}) in the '{required_label}' column. Need at least two classes for classification. Skipping training.")
            print("--- Exiting train_ml_model ---\\n")
            return None, None

        # Check if there are enough samples for train/test split
        if len(df) < 2: # Need at least 2 samples for split
             print("Error: Not enough data points for train/test split (need at least 2). Skipping training.")
             print("--- Exiting train_ml_model ---\\n")
             return None, None

        # Perform train/test split
        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Use stratify if classes are imbalanced

        print(f"Training model on {len(X_train)} samples, testing on {len(X_test)} samples.")

        # Initialize and train the model
        model = RandomForestClassifier(random_state=42)
        model.fit(X_train, y_train)

        # Evaluate the model
        y_pred = model.predict(X_test)
        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0) # Use zero_division=0 to handle cases with no predicted samples for a class

        print("ML model training successful.")
        print("--- Exiting train_ml_model ---\\n")
        return model, report

    except ValueError as ve:
        print(f"ValueError during ML training: {ve}. Skipping training.")
        print("--- Exiting train_ml_model ---\\n")
        return None, None
    except Exception as e:
        print(f"Error during ML training: {e}. Skipping training.")
        print("--- Exiting train_ml_model ---\\n")
        return None, None

# Example Usage (can be removed or commented out)
# if __name__ == '__main__':
#     # Assuming data/signal_backtest.csv exists with a 'success' column
#     DATA_DIR = 'signal_bot/data'
#     training_data_path = os.path.join(DATA_DIR, 'signal_backtest.csv')
#     if os.path.exists(training_data_path):
#         trained_model, training_report = train_ml_model(training_data_path)
#         if trained_model:
#             print("\\nTraining Report Summary:")
#             # Print classification report in a readable format
#             for label, metrics in training_report.items():
#                  if isinstance(metrics, dict):
#                       print(f"  {label}:")
#                       for metric, value in metrics.items():
#                            print(f"    {metric}: {value:.4f}")
#                  else:
#                       print(f"  {label}: {metrics:.4f}")
#     else:
#          print(f"Training data not found: {training_data_path}")

"""
with open('signal_bot/ml_model_trainer.py', 'w') as f:
    f.write(ml_model_trainer_content)
print("signal_bot/ml_model_trainer.py regenerated.")

# Regenerate signal_bot/exporter.py
exporter_content = """# exporter.py
import pandas as pd
import os

def export_to_excel(csv_path, output_path):
    """Exports data from a CSV to an Excel file."""
    print(f"Attempting to export {csv_path} to Excel at {output_path}")
    try:
        df = pd.read_csv(csv_path)
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True) # Ensure output directory exists
        df.to_excel(output_path, index=False)
        print(f"Successfully exported {csv_path} to Excel at {output_path}")
    except FileNotFoundError:
        print(f"Error: Input CSV file not found at {csv_path} for Excel export.")
    except Exception as e:
        print(f"Error exporting to Excel from {csv_path}: {e}")


def export_to_html(csv_path, output_path):
    """Exports data from a CSV to an HTML file."""
    print(f"Attempting to export {csv_path} to HTML at {output_path}")
    try:
        df = pd.read_csv(csv_path)
        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True) # Ensure output directory exists
        df.to_html(output_path, index=False)
        print(f"Successfully exported {csv_path} to HTML at {output_path}")
    except FileNotFoundError:
        print(f"Error: Input CSV file not found at {csv_path} for HTML export.")
    except Exception as e:
        print(f"Error exporting to HTML from {csv_path}: {e}")

# Example Usage (can be removed or commented out)
# if __name__ == '__main__':
#     # Assuming data/signal_backtest.csv exists after running main.py
#     DATA_DIR = 'signal_bot/data'
#     backtest_csv = os.path.join(DATA_DIR, 'signal_backtest.csv')
#     excel_output = os.path.join(DATA_DIR, 'signal_backtest.xlsx')
#     html_output = os.path.join(DATA_DIR, 'signal_backtest.html')
#
#     if os.path.exists(backtest_csv):
#          export_to_excel(backtest_csv, excel_output)
#          export_to_html(backtest_csv, html_output)
#     else:
#          print(f"Input file not found: {backtest_csv}")

"""
with open('signal_bot/exporter.py', 'w') as f:
    f.write(exporter_content)
print("signal_bot/exporter.py regenerated.")


# Regenerate signal_bot/logger.py
logger_content = """# logger.py
import logging
import os

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def setup_logger(log_file="bot.log"):
    """Sets up the root logger."""
    log_path = os.path.join(DATA_DIR, log_file)
    # Use basicConfig for simplicity, force=True allows reconfiguring in Colab
    logging.basicConfig(
        filename=log_path,
        level=logging.INFO,
        format='%(asctime)s - %(levelname)s - %(message)s',
        force=True
    )
    # Add a handler to print logs to the console as well
    console_handler = logging.StreamHandler()
    console_handler.setLevel(logging.INFO)
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')
    console_handler.setFormatter(formatter)

    # Avoid adding duplicate console handlers if basicConfig is called multiple times
    # Check if a StreamHandler is already attached to the root logger
    root_logger = logging.getLogger()
    if not any(isinstance(h, logging.StreamHandler) for h in root_logger.handlers):
         root_logger.addHandler(console_handler)


def log_info(message):
    """Logs an informational message."""
    logging.info(message)

def log_error(message):
    """Logs an error message."""
    logging.error(message)

# Example usage (can be removed or commented out in final version)
# if __name__ == '__main__':
#     setup_logger()
#     log_info("Logger setup complete.")
#     log_error("This is a test error message.")
"""
with open('signal_bot/logger.py', 'w') as f:
    f.write(logger_content)
print("signal_bot/logger.py regenerated.")

# Regenerate signal_bot/dataset_manager.py
dataset_manager_content = """# dataset_manager.py
import pandas as pd

def clean_and_normalize(df):
    """Clean and normalize DataFrame columns and data types."""
    if df.empty:
        print("Warning: Input DataFrame is empty for cleaning and normalization.")
        return df

    # Convert column names to lowercase and replace spaces with underscores
    df.columns = [col.lower().replace(" ", "_") for col in df.columns]

    # Drop rows where essential columns are missing
    required_subset_cols = ["id", "current_price"]
    if not all(col in df.columns for col in required_subset_cols):
         print(f"Warning: Required columns for dropna subset missing: {required_subset_cols}. Skipping dropna subset.")
    else:
         df = df.dropna(subset=required_subset_cols)

    # Convert timestamp column to datetime, coerce errors to NaT
    if "timestamp" in df.columns:
        df["timestamp"] = pd.to_datetime(df["timestamp"], errors='coerce')
        # Optionally drop rows where timestamp conversion failed
        # df = df.dropna(subset=["timestamp"])
    else:
         print("Warning: 'timestamp' column not found for datetime conversion.")


    # Sort by id and timestamp
    if "id" in df.columns and "timestamp" in df.columns:
        df = df.sort_values(by=["id", "timestamp"])
    elif "id" in df.columns:
        df = df.sort_values(by="id")
    elif "timestamp" in df.columns:
        df = df.sort_values(by="timestamp")
    else:
        print("Warning: 'id' and 'timestamp' columns not found for sorting.")


    return df
"""
with open('signal_bot/dataset_manager.py', 'w') as f:
    f.write(dataset_manager_content)
print("signal_bot/dataset_manager.py regenerated.")


# Regenerate signal_bot/data_collector.py
data_collector_content = """# signal_bot/data_collector.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins
from .dataset_manager import clean_and_normalize # Import clean_and_normalize
from .logger import setup_logger, log_info, log_error # Import logger

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def collect_data(limit=250):
    """
    Fetches a snapshot of market data for a specified number of top coins
    and saves it to full_market_snapshot.csv.

    Args:
        limit (int): The number of top coins to fetch data for. Note CoinGecko
                     /coins/markets has a per_page limit (typically 250).
                     Fetching more than the limit might require pagination.
                     Current implementation fetches up to 'limit' on page 1.

    Returns:
        pd.DataFrame or None: DataFrame with market data if successful, None otherwise.
    """
    # setup_logger() # Setup logger for this function if not setup globally

    log_info(f"Fetching market data for top {limit} coins for full snapshot...")
    try:
        data = get_top_coins(limit=limit)
        if data is not None:
            df = pd.DataFrame(data)

            # Add current timestamp to each row
            df["timestamp"] = pd.Timestamp.utcnow()

            # Clean and normalize the collected data
            df = clean_and_normalize(df)
            log_info("Data collected and cleaned.")

            # Ensure output directory exists (already done by DATA_DIR os.makedirs)
            full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")

            # Append data to existing file or create new
            if os.path.exists(full_snapshot_path):
                 df.to_csv(full_snapshot_path, mode='a', header=False, index=False)
                 log_info(f"Market snapshot appended to {full_snapshot_path}.")
            else:
                 df.to_csv(full_snapshot_path, index=False)
                 log_info(f"New market snapshot created at {full_snapshot_path}.")

            return df
        else:
            log_error("Failed to fetch market data for full snapshot.")
            return None
    except Exception as e:
        log_error(f"Error during full market data collection: {e}")
        return None

# Example of how to run the data collector as a script
if __name__ == '__main__':
    # setup_logger() # Setup logger if running this script directly
    collect_data(limit=250) # Fetch data for top 250 coins for the snapshot
"""
with open('signal_bot/data_collector.py', 'w') as f:
    f.write(data_collector_content)
print("signal_bot/data_collector.py regenerated.")


# Regenerate signal_bot/scheduler.py
scheduler_content = """# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    # In a full implementation, data collection for top 10 would be here as well
    # For now, we'll fetch top 10 data within the pipeline job
    log_info("Fetching top 10 coin data within pipeline job...")
    try:
        top10_data = collect_data(limit=10) # Reuse collect_data for top 10, assuming limit works
        if top10_data is not None:
             top10_data.to_csv(top10_input_path, index=False) # Save as top10_market_data.csv
             log_info("Top 10 data fetched and saved within pipeline job.")
        else:
             log_error("Failed to fetch top 10 data within pipeline job. Skipping.")
             return # Exit job if data fetching fails

    except Exception as e:
        log_error(f"Error fetching top 10 data within pipeline job: {e}. Skipping.")
        return # Exit job on error


    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found after collection. Skipping pipeline job.")
        return


    try:
        log_info("Computing indicators for top 10...")
        # Read from the saved CSV to ensure consistency, although top10_data DF is available
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generating signals for top 10...")
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    # Ensure the full market snapshot is available - it's generated by full_data_collection_job
    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found for anomaly detection. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        # collect_data will fetch the full snapshot (limit=250) and append to full_market_snapshot.csv
        collected_df = collect_data(limit=250) # Assuming collect_data handles saving/appending

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
# scheduler.start() # Comment out to prevent immediate blocking execution

# Manual test execution of individual jobs for debugging in Colab:
# setup_logger()
# print("\\n--- Manual Pipeline Job Test ---")
# pipeline_job()
# print("\\n--- Manual Full Data Collection Job Test ---")
# full_data_collection_job()
# print("\\n--- Manual Anomaly Job Test ---")
# anomaly_job()
''')

!git clone github.com/9to5ninja-projects/GeckoBot.git

from google.colab import drive
drive.mount('/content/drive')

# Commented out IPython magic to ensure Python compatibility.
# Ensure you are in the cloned repository directory
# Check current directory - should be 'GeckoBot'
# %pwd

# Push the changes to your GitHub repository again
# You WILL LIKELY be prompted for your GitHub username and password/Personal Access Token.
# Enter your GitHub username when prompted for "Username for 'https://github.com':"
# Enter your Personal Access Token when prompted for "Password for 'https://YOUR_GITHUB_USERNAME@github.com':"
!git push origin main # Or 'master', depending on your default branch name

print("Attempted to push changes again.")
print("Please check the output above for any prompts for credentials or error messages.")
print("After successful authentication and push, check your GitHub repository on the web.")

# Push the changes to your GitHub repository
# You may be prompted for your GitHub username and password/Personal Access Token
!git push origin main # Or 'master', depending on your default branch name

print("Changes pushed to GitHub.")

# Commit the changes
# Replace "Initial commit" or provide a meaningful commit message
!git commit -m "Add/Update project files from Colab"

print("Changes committed.")

# Add all changes to the staging area
!git add .

print("All changes added to staging.")

# Commented out IPython magic to ensure Python compatibility.
# Navigate into the cloned repository directory
# %cd GeckoBot

print("Changed directory to GeckoBot.")

# Copy your project files into the cloned repository directory
# Ensure signal_bot directory exists in the current environment
import os
import shutil

source_dir = 'signal_bot'
dest_dir = 'GeckoBot/signal_bot' # Copy into the cloned repo

# Ensure source directory exists
if not os.path.exists(source_dir):
    print(f"Error: Source directory not found: {source_dir}. Cannot copy files.")
else:
    # Remove destination directory if it exists to avoid merging issues
    if os.path.exists(dest_dir):
        print(f"Removing existing destination directory: {dest_dir}")
        shutil.rmtree(dest_dir)

    # Copy the signal_bot directory
    shutil.copytree(source_dir, dest_dir)
    print(f"Copied {source_dir} to {dest_dir}.")

    # Also copy .env if it exists at the root
    env_file = '.env'
    if os.path.exists(env_file):
        shutil.copy(env_file, 'GeckoBot/.env')
        print(f"Copied {env_file} to GeckoBot/.")
    else:
        print(".env file not found at the root. Skipping copy.")

# Clone your GitHub repository
# Replace YOUR_GITHUB_USERNAME with '9to5ninja-projects' and YOUR_REPO_NAME with 'GeckoBot'
!git clone https://github.com/9to5ninja-projects/GeckoBot.git

print("Repository cloned.")

# Configure Git (replace with your name and email)
!git config --global user.name "YOUR NAME"
!git config --global user.email "YOUR EMAIL"

print("Git configured.")

import os

# Create the directory if it doesn't exist
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the anomaly_detector.py file
with open('signal_bot/anomaly_detector.py', 'w') as f:
    f.write('''# anomaly_detector.py
import pandas as pd
from sklearn.ensemble import IsolationForest
from datetime import datetime
import os
from .logger import setup_logger, log_info, log_error


def load_market_snapshot(snapshot_csv):
    """Load full market snapshot from CSV."""
    setup_logger()
    log_info(f"Loading market snapshot from {snapshot_csv} for anomaly detection.")
    try:
        df = pd.read_csv(snapshot_csv, parse_dates=["timestamp"])
        log_info("Market snapshot loaded successfully.")
        return df
    except FileNotFoundError:
        log_error(f"Error: Snapshot file not found at {snapshot_csv}.")
        return pd.DataFrame()
    except Exception as e:
        log_error(f"Error loading market snapshot from {snapshot_csv}: {e}")
        return pd.DataFrame()


def prepare_features(df):
    """Create features for anomaly detection (e.g., price % change)."""
    setup_logger()
    log_info("Preparing features for anomaly detection.")
    if df.empty:
        log_info("Input DataFrame is empty, cannot prepare features.")
        return pd.DataFrame()
    # Ensure required columns exist before accessing them
    required_cols = ["price_change_percentage_24h", "id", "symbol", "name", "timestamp"]
    if not all(col in df.columns for col in required_cols):
         missing = [col for col in required_cols if col not in df.columns]
         log_error(f"Missing required columns for feature preparation: {missing}. Cannot prepare features.")
         return pd.DataFrame()

    df["pct_change_24h"] = df["price_change_percentage_24h"].fillna(0)
    log_info("Features prepared.")
    return df[["id", "symbol", "name", "pct_change_24h", "timestamp"]].copy()


def run_isolation_forest(df, contamination=0.01):
    """Apply Isolation Forest to detect anomalies based on % change."""
    setup_logger()
    log_info("Running Isolation Forest for anomaly detection.")
    if df.empty or "pct_change_24h" not in df.columns:
        log_info("Input DataFrame is empty or missing 'pct_change_24h', skipping Isolation Forest.")
        return pd.DataFrame()

    try:
        # Ensure there's enough data points for IsolationForest (at least 2)
        if len(df) < 2:
             log_info(f"Not enough data points ({len(df)}) for Isolation Forest (requires at least 2). Skipping.")
             df["anomaly_score"] = 1 # Mark as not an anomaly if insufficient data
             return df

        # Fit and predict anomalies
        model = IsolationForest(contamination=contamination, random_state=42)
        df["anomaly_score"] = model.fit_predict(df[["pct_change_24h"]])
        log_info("Isolation Forest completed.")
        return df
    except Exception as e:
        log_error(f"Error running Isolation Forest: {e}")
        df["anomaly_score"] = 1 # Default to not an anomaly on error
        return df


def save_anomalies(df, output_dir="data"):
    """Save anomalies to timestamped CSV."""
    setup_logger()
    if df.empty or "anomaly_score" not in df.columns:
        log_info("Input DataFrame is empty or missing 'anomaly_score', skipping saving anomalies.")
        return None, pd.DataFrame()

    anomalies = df[df["anomaly_score"] == -1]
    if anomalies.empty:
        log_info("No anomalies detected to save.")
        return None, pd.DataFrame()

    timestamp = datetime.utcnow().strftime("%Y%m%dT%H%M%SZ")
    path = os.path.join(output_dir, f"anomalies_{timestamp}.csv")
    try:
        os.makedirs(output_dir, exist_ok=True)
        anomalies.to_csv(path, index=False)
        log_info(f"Anomalies saved to {path}. Found {len(anomalies)} anomalies.")
        return path, anomalies
    except Exception as e:
        log_error(f"Error saving anomalies to {path}: {e}")
        return None, pd.DataFrame()


def detect_anomalies(snapshot_csv, output_dir="data"):
    """End-to-end anomaly detection workflow."""
    setup_logger()
    log_info("Starting anomaly detection workflow.")
    df = load_market_snapshot(snapshot_csv)
    if df.empty:
        log_info("Anomaly detection workflow skipped due to empty data.")
        return None, pd.DataFrame()

    features = prepare_features(df)
    if features.empty:
        log_info("Anomaly detection workflow skipped due to empty features.")
        return None, pd.DataFrame()

    with_scores = run_isolation_forest(features)
    if with_scores.empty:
         log_info("Anomaly detection workflow skipped after Isolation Forest.")
         return None, pd.DataFrame()

    path, anomalies = save_anomalies(with_scores, output_dir)
    log_info("Anomaly detection workflow finished.")
    return path, anomalies
''')

# Now that anomaly_detector.py exists, try running the scheduler again.
# Ensure scheduler.start() is uncommented in scheduler.py.
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently
    # For now, we rely on the full_data_collection_job which runs hourly.
    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.
    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        # compute_indicators expects input_csv and output_csv paths
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        # find_signals expects a DataFrame
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        # log_ml_features expects indicator_csv and output_csv paths
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        # detect_anomalies expects snapshot_csv and output_dir
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        # collect_data returns a DataFrame
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
''')

# Execute the scheduler script
!python -m signal_bot.scheduler

import os

# Create the directory if it doesn't exist
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the ml_logger.py file
with open('signal_bot/ml_logger.py', 'w') as f:
    f.write('''# ml_logger.py
import pandas as pd
import os
from datetime import datetime
from .logger import setup_logger, log_info, log_error # Import logger


def log_ml_features(indicator_csv, output_csv):
    """Append new features to ML training dataset."""
    setup_logger() # Ensure logger is setup
    log_info(f"Attempting to log ML features from {indicator_csv} to {output_csv}...")

    try:
        if not os.path.exists(indicator_csv):
            log_info(f"Warning: Input file for ML logging not found at {indicator_csv}. Skipping logging.")
            return pd.DataFrame() # Return empty DataFrame on error

        df = pd.read_csv(indicator_csv)
        log_info(f"Successfully read {indicator_csv} for ML logging. Columns: {df.columns.tolist()}")

        required_features = [
            "id", "timestamp", "current_price",
            "rsi", "ema_20", "macd_diff",
            "bb_upper", "bb_lower", "signal" # Added signal as a required feature
        ]

        # Check if all required features are in the DataFrame
        missing_features = [col for col in required_features if col not in df.columns]
        if missing_features:
            log_info(f"Warning: Missing required features for ML logging in {indicator_csv}: {missing_features}. Skipping logging for this file.")
            return pd.DataFrame() # Return empty DataFrame if features are missing

        features = df[required_features].copy()

        # Add a placeholder 'success' column for ML training.
        # In a real scenario, this would be populated by backtesting or manual labeling.
        if "signal" in features.columns and "BUY" in features["signal"].str.upper().str:
             # For simplicity, label any row with a BUY signal as needing potential backtest/labeling
             # A more sophisticated approach would look at future price movement
             features["success"] = pd.NA # Mark as needing labeling
        else:
             features["success"] = 0 # Assume not a successful trade if no BUY signal

        if os.path.exists(output_csv):
            existing = pd.read_csv(output_csv)
            # Ensure timestamp is datetime for proper merging/deduplication
            existing["timestamp"] = pd.to_datetime(existing["timestamp"])
            features["timestamp"] = pd.to_datetime(features["timestamp"])

            # Combine existing and new data, drop duplicates based on id and timestamp
            combined = pd.concat([existing, features]).drop_duplicates(subset=["id", "timestamp"]).reset_index(drop=True)
        else:
            combined = features

        combined.to_csv(output_csv, index=False)
        log_info(f"ML features logged successfully to {output_csv}. Total entries: {len(combined)}")
        return combined

    except Exception as e:
        log_error(f"Error during ML feature logging for {indicator_csv}: {e}. Skipping logging.")
        return pd.DataFrame() # Return empty DataFrame on error

''')

# Now that ml_logger.py exists, try running the scheduler again.
# Ensure scheduler.start() is uncommented in scheduler.py.
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently
    # For now, we rely on the full_data_collection_job which runs hourly.
    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.
    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        # compute_indicators expects input_csv and output_csv paths
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        # find_signals expects a DataFrame
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        # log_ml_features expects indicator_csv and output_csv paths
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        # detect_anomalies expects snapshot_csv and output_dir
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        # collect_data returns a DataFrame
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
''')

# Execute the scheduler script
!python -m signal_bot.scheduler

import os

# Create the directories if they don't exist
os.makedirs('signal_bot/signals', exist_ok=True)

# Write the content to the signal_finder.py file
with open('signal_bot/signals/signal_finder.py', 'w') as f:
    f.write('''# signal_bot/signals/signal_finder.py
import pandas as pd
import numpy as np
from ..logger import setup_logger, log_info, log_error


def find_signals(df):
    """
    Detects trading signals based on technical indicators.

    Args:
        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).
                           Assumes DataFrame is sorted by timestamp.

    Returns:
        pd.DataFrame: DataFrame with 'signal' column added. Returns DataFrame with HOLD signals on error or insufficient data.
    """
    setup_logger() # Ensure logger is setup
    log_info("Attempting to find signals...")

    # Ensure DataFrame has required columns.
    required_cols = ["rsi", "ema_20", "macd_diff", "close", "bb_upper", "bb_lower"]
    if not all(col in df.columns for col in required_cols):
        missing = [col for col in required_cols if col not in df.columns]
        log_error(f"Missing required columns for signal generation: {missing}. Returning HOLD signals.")
        df["signal"] = "HOLD"
        return df[['signal']]

    if df.empty:
        log_info("Input DataFrame is empty. Returning HOLD signals.")
        df["signal"] = "HOLD"
        return df[['signal']]

    # Initialize signal column
    df["signal"] = "HOLD"

    # Find the first index where all required indicators are not NaN
    df_cleaned = df.dropna(subset=required_cols)

    if df_cleaned.empty:
        log_info("All required indicator values are NaN. Returning HOLD signals.")
        # Set all signals to HOLD if no valid indicator rows are found
        df["signal"] = "HOLD"
        return df[['signal']]

    first_valid_index = df_cleaned.index.min()
    first_valid_iloc = df.index.get_loc(first_valid_index)

    # Apply signal logic based on technical indicators, starting from the first valid index
    for i in range(first_valid_iloc, len(df)):
        current_row = df.iloc[i]

        # Ensure previous row exists and has valid indicator values
        prev_row_iloc = i - 1
        if prev_row_iloc < first_valid_iloc:
             # Previous row does not have valid indicators
             df.loc[current_row.name, 'signal'] = 'HOLD'
             continue

        prev_row = df.iloc[prev_row_iloc]
        signals = []

        # RSI signals (using current and previous crossing thresholds)
        if current_row['rsi'] < 30 and prev_row['rsi'] >= 30:
            signals.append("BUY_RSI_CROSS")
        elif current_row['rsi'] > 70 and prev_row['rsi'] <= 70:
            signals.append("SELL_RSI_CROSS")

        # RSI extreme conditions (may add to existing signals)
        if current_row['rsi'] < 20:
             signals.append("EXTREME_OVERSOLD")
        elif current_row['rsi'] > 80:
             signals.append("EXTREME_OVERBOUGHT")


        # MACD signals (using macd_diff crossing 0)
        # Ensure previous macd_diff is not NaN for cross check
        if not np.isnan(prev_row['macd_diff']):
            if current_row['macd_diff'] > 0 and prev_row['macd_diff'] <= 0:
                signals.append("BUY_MACD_CROSS")
            elif current_row['macd_diff'] < 0 and prev_row['macd_diff'] >= 0:
                signals.append("SELL_MACD_CROSS")

        # Bollinger Bands signals (breakouts)
        # Ensure previous BB values are not NaN for breakout check
        if not np.isnan(prev_row['bb_upper']) and not np.isnan(prev_row['bb_lower']):
            if current_row['close'] > current_row['bb_upper'] and prev_row['close'] <= prev_row['bb_upper']:
                signals.append("BREAKOUT_UPPER")
            elif current_row['close'] < current_row['bb_lower'] and prev_row['close'] >= prev_row['bb_lower']:
                signals.append("BREAKOUT_LOWER")

        # Update the signal for the current row
        df.loc[current_row.name, 'signal'] = ", ".join(signals) if signals else "HOLD"


    # For rows before the first valid indicator index, set signal to HOLD
    if first_valid_iloc > 0:
        df.loc[df.index[:first_valid_iloc], 'signal'] = 'HOLD'

    log_info("Signal finding completed.")
    return df[['signal']] # Return only the signal column as a DataFrame

''')

# Now that signal_finder.py exists, try running the scheduler again.
# Ensure scheduler.start() is uncommented in scheduler.py.
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently
    # For now, we rely on the full_data_collection_job which runs hourly.
    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.
    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        # compute_indicators expects input_csv and output_csv paths
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        # find_signals expects a DataFrame
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        # log_ml_features expects indicator_csv and output_csv paths
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        # detect_anomalies expects snapshot_csv and output_dir
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        # collect_data returns a DataFrame
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
''')


# Execute the scheduler script
!python -m signal_bot.scheduler

import os

# Create the directories if they don't exist
os.makedirs('signal_bot/indicators', exist_ok=True)

# Write the content to the ta_utils.py file
with open('signal_bot/indicators/ta_utils.py', 'w') as f:
    f.write('''# indicators/ta_utils.py
import pandas as pd
import ta
from ..logger import setup_logger, log_info, log_error # Import logger from parent directory


def compute_indicators(df_or_path, output_csv=None):
    """
    Computes technical indicators (RSI, EMA, MACD, Bollinger Bands) for price data.

    Args:
        df_or_path (pd.DataFrame or str): Input data as a DataFrame or path to a CSV file.
        output_csv (str, optional): Path to save the DataFrame with indicators to CSV. Defaults to None.

    Returns:
        pd.DataFrame: DataFrame with technical indicators added. Returns empty DataFrame on error.
    """
    setup_logger() # Ensure logger is setup

    try:
        if isinstance(df_or_path, pd.DataFrame):
            df = df_or_path.copy()
            log_info("Computing indicators on provided DataFrame.")
        elif isinstance(df_or_path, str) and os.path.exists(df_or_path):
            df = pd.read_csv(df_or_path, parse_dates=["timestamp"])
            log_info(f"Computing indicators on data loaded from {df_or_path}.")
        else:
            log_error(f"Invalid input: Expected a DataFrame or valid file path, got {type(df_or_path)}.")
            return pd.DataFrame()

        # Ensure close price exists and data is sorted by timestamp
        if "close" not in df.columns and "current_price" in df.columns:
             df["close"] = df["current_price"]
        elif "close" not in df.columns:
             log_error("Input DataFrame must contain a 'close' or 'current_price' column.")
             return pd.DataFrame()

        # Ensure timestamp is datetime and sort
        df["timestamp"] = pd.to_datetime(df["timestamp"])
        df = df.sort_values("timestamp")

        # Compute Indicators - Handle potential errors or insufficient data for windows
        if len(df) > 14: # Minimum required for RSI default window
            try:
                df["rsi"] = ta.momentum.RSIIndicator(df["close"]).rsi()
            except Exception as e:
                 log_error(f"Error computing RSI: {e}")
                 df["rsi"] = pd.NA # Assign pandas NA on error or insufficient data

            if len(df) > 20: # Minimum required for EMA and BB default window
                 try:
                     df["ema_20"] = ta.trend.EMAIndicator(df["close"], window=20).ema_indicator()
                     bb = ta.volatility.BollingerBands(df["close"], window=20, window_dev=2)
                     df["bb_upper"] = bb.bollinger_hband()
                     df["bb_lower"] = bb.bollinger_lband()
                 except Exception as e:
                      log_error(f"Error computing EMA or Bollinger Bands: {e}")
                      df["ema_20"] = pd.NA
                      df["bb_upper"] = pd.NA
                      df["bb_lower"] = pd.NA

                 try:
                    # MACD requires more data than the default window for meaningful values
                    # ta.trend.MACD default windows are (12, 26, 9). MACD_diff needs at least 26 data points.
                    if len(df) > 26:
                         df["macd_diff"] = ta.trend.MACD(df["close"]).macd_diff()
                    else:
                         log_info(f"Not enough data points ({len(df)}) for MACD calculation (requires > 26).")
                         df["macd_diff"] = pd.NA
                 except Exception as e:
                      log_error(f"Error computing MACD: {e}")
                      df["macd_diff"] = pd.NA


            else:
                 log_info(f"Not enough data points ({len(df)}) for EMA, MACD, or Bollinger Bands calculation (requires > 20).")
                 df["ema_20"] = pd.NA
                 df["bb_upper"] = pd.NA
                 df["bb_lower"] = pd.NA
                 df["macd_diff"] = pd.NA # Ensure MACD is also NA

        else:
            log_info(f"Not enough data points ({len(df)}) for any indicator calculation (requires > 14 for RSI).")
            # Assign NA to all indicator columns if not enough data for any
            df["rsi"] = pd.NA
            df["ema_20"] = pd.NA
            df["bb_upper"] = pd.NA
            df["bb_lower"] = pd.NA
            df["macd_diff"] = pd.NA

        df["processed_timestamp"] = pd.Timestamp.utcnow().isoformat()


        if output_csv:
            try:
                os.makedirs(os.path.dirname(output_csv), exist_ok=True)
                df.to_csv(output_csv, index=False)
                log_info(f"Indicators computed and saved to {output_csv}.")
            except Exception as e:
                log_error(f"Error saving indicators to {output_csv}: {e}")


        return df

    except Exception as e:
        log_error(f"An unexpected error occurred during indicator computation: {e}")
        return pd.DataFrame() # Return empty DataFrame on unexpected errors

''')

# Now that ta_utils.py exists, try running the scheduler again.
# Ensure scheduler.start() is uncommented in scheduler.py.
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently
    # For now, we rely on the full_data_collection_job which runs hourly.
    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.
    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        # compute_indicators expects input_csv and output_csv paths
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        # find_signals expects a DataFrame
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        # log_ml_features expects indicator_csv and output_csv paths
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        # detect_anomalies expects snapshot_csv and output_dir
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        # collect_data returns a DataFrame
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
''')

# Execute the scheduler script
!python -m signal_bot.scheduler

import os

# Create the directory if it doesn't exist
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the logger.py file
with open('signal_bot/logger.py', 'w') as f:
    f.write('''# signal_bot/logger.py
import logging
import sys

def setup_logger():
    """Sets up a basic logger to output to standard output."""
    # Ensure logger is not already configured
    if not logging.getLogger().handlers:
        logging.basicConfig(level=logging.INFO,
                            format='%(asctime)s - %(levelname)s - %(message)s',
                            stream=sys.stdout)

def log_info(message):
    """Logs an informational message."""
    logging.info(message)

def log_error(message):
    """Logs an error message."""
    logging.error(message)
''')

# Now that logger.py exists, try running the scheduler again.
# Ensure scheduler.start() is uncommented in scheduler.py.
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        # compute_indicators expects input_csv and output_csv paths
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        # find_signals expects a DataFrame
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        # log_ml_features expects indicator_csv and output_csv paths
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        # detect_anomalies expects snapshot_csv and output_dir
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        # collect_data returns a DataFrame
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
''')

# Execute the scheduler script
!python -m signal_bot.scheduler

import os

# Create the directories if they don't exist
os.makedirs('signal_bot/utils', exist_ok=True)

# Write the content to the coingecko_api.py file
with open('signal_bot/utils/coingecko_api.py', 'w') as f:
    f.write('''# utils/coingecko_api.py
import requests
import pandas as pd
import os
from ..logger import setup_logger, log_info, log_error # Import logger from parent directory

BASE_URL = "https://api.coingecko.com/api/v3"

def get_top_coins(limit=10):
    """Fetches market data for top N coins."""
    setup_logger() # Ensure logger is setup
    url = f"{BASE_URL}/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": limit,
        "page": 1,
        "sparkline": "true"
    }
    log_info(f"Fetching top {limit} coins from {url} with params {params}")
    try:
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes
        log_info("Successfully fetched top coins.")
        return response.json()
    except requests.exceptions.RequestException as e:
        log_error(f"Error fetching top coins: {e}")
        if 'response' in locals() and response is not None:
             log_error(f"Response status code: {response.status_code}")
             log_error(f"Response text: {response.text}")
        return None

def get_coin_history(coin_id, days="30"):
    """Fetches historical market data for a specific coin."""
    setup_logger() # Ensure logger is setup
    url = f"{BASE_URL}/coins/{coin_id}/market_chart"
    # Removing "interval": "hourly" as it's for Enterprise plan
    params = {"vs_currency": "usd", "days": days}
    log_info(f"Fetching historical data for {coin_id} from {url} with params: {params}")
    try:
        response = requests.get(url, params=params)
        response.raise_for_status() # Raise an exception for bad status codes
        data = response.json()
        log_info(f"Successfully fetched historical data for {coin_id}. Response keys: {data.keys()}")
        return data
    except requests.exceptions.RequestException as e:
        log_error(f"Error fetching historical data for {coin_id}: {e}")
        if 'response' in locals() and response is not None:
             log_error(f"Response status code: {response.status_code}")
             log_error(f"Response text: {response.text}")
        return None''')

# Now that coingecko_api.py exists, try running the scheduler again.
# Ensure scheduler.start() is uncommented in scheduler.py.
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        # compute_indicators expects input_csv and output_csv paths
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        # find_signals expects a DataFrame
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        # log_ml_features expects indicator_csv and output_csv paths
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        # detect_anomalies expects snapshot_csv and output_dir
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        # collect_data returns a DataFrame
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
''')


# Execute the scheduler script
!python -m signal_bot.scheduler

import os

# Create the directory if it doesn't exist
os.makedirs('signal_bot', exist_ok=True)
os.makedirs('signal_bot/data', exist_ok=True) # Ensure data directory exists

# Write the content to the file
with open('signal_bot/data_collector.py', 'w') as f:
    f.write('''# signal_bot/data_collector.py
import pandas as pd
import os
from .utils.coingecko_api import get_top_coins # Import get_top_coins
from .logger import setup_logger, log_info, log_error # Import logger

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def collect_data(limit=250):
    """
    Fetches a snapshot of market data for a specified number of top coins.

    Args:
        limit (int): The number of top coins to fetch data for.

    Returns:
        pd.DataFrame or None: DataFrame with market data if successful, None otherwise.
    """
    setup_logger() # Ensure logger is setup within the function
    log_info(f"Fetching market data for top {limit} coins...")
    try:
        data = get_top_coins(limit=limit)
        if data is not None:
            df = pd.DataFrame(data)
            df["timestamp"] = pd.Timestamp.utcnow()
            full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
            df.to_csv(full_snapshot_path, index=False)
            log_info(f"Full market snapshot fetched and saved to {full_snapshot_path}.")
            return df
        else:
            log_error("Failed to fetch market data for full snapshot.")
            return None
    except Exception as e:
        log_error(f"Error during market data collection: {e}")
        return None

if __name__ == '__main__':
    # Example of how to run the data collector as a script
    collect_data(limit=250) # Fetch data for top 250 coins for the snapshot
''')

# Now that data_collector.py exists, try running the scheduler again.
# Modify the scheduler.py file to ensure scheduler.start() is uncommented.
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
''')

# Execute the scheduler script
!python -m signal_bot.scheduler

import os

# Create the directory if it doesn't exist
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the file, uncommenting scheduler.start()
with open('signal_bot/scheduler.py', 'w') as f:
    f.write('''# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
''')

# Execute the scheduler script
!python -m signal_bot.scheduler

# Manually run the anomaly job for testing
import os
from signal_bot.scheduler import setup_logger, anomaly_job

# Ensure logger is setup for manual runs
setup_logger()

# Run the job
anomaly_job()

# Execute data_collector.py as a module to generate the full market snapshot
!python -m signal_bot.data_collector

# Manually run the full data collection job for testing
import os
from signal_bot.scheduler import setup_logger, full_data_collection_job

# Ensure logger is setup for manual runs
setup_logger()

# Run the job
full_data_collection_job()

import pandas as pd
import os
import glob # Import glob to find files

DATA_DIR = 'signal_bot/data'

# --- Verify top10_signals.csv ---
signals_path = os.path.join(DATA_DIR, "top10_signals.csv")
print(f"--- Verifying {signals_path} ---")
if os.path.exists(signals_path):
    try:
        signals_df = pd.read_csv(signals_path)
        print(f"{signals_path} head:")
        display(signals_df.head().to_markdown(index=False))
        print(f"\n{signals_path} info:")
        signals_df.info()
    except Exception as e:
        print(f"Error reading or displaying {signals_path}: {e}")
else:
    print(f"File not found: {signals_path}")

# --- Verify latest anomalies_*.csv ---
print("\n--- Verifying latest anomalies_*.csv ---")
anomalies_dir = DATA_DIR
anomaly_files = glob.glob(os.path.join(anomalies_dir, 'anomalies_*.csv'))
anomaly_files.sort(reverse=True) # Sort to get the latest first

if anomaly_files:
    latest_anomaly_file = anomaly_files[0]
    print(f"Latest anomaly file found: {latest_anomaly_file}")
    try:
        anomalies_df = pd.read_csv(latest_anomaly_file)
        print(f"{latest_anomaly_file} head:")
        display(anomalies_df.head().to_markdown(index=False))
        print(f"\n{latest_anomaly_file} info:")
        anomalies_df.info()
    except Exception as e:
        print(f"Error reading or displaying {latest_anomaly_file}: {e}")
else:
    print(f"No anomaly files found in {anomalies_dir}.")

# --- Verify top10_with_indicators.csv ---
print("\n--- Verifying top10_with_indicators.csv ---")
indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
if os.path.exists(indicators_path):
    try:
        indicators_df = pd.read_csv(indicators_path)
        print(f"{indicators_path} head:")
        display(indicators_df.head().to_markdown(index=False))
        print(f"\n{indicators_path} info:")
        indicators_df.info()
    except Exception as e:
        print(f"Error reading or displaying {indicators_path}: {e}")
else:
    print(f"File not found: {indicators_path}")

# Step 2: Run the main pipeline to generate data files
# This will fetch data, compute indicators, generate signals, backtest, and log
print("Running main pipeline...")
!python -m signal_bot.main
print("Main pipeline execution completed.")

# Step 1: Install dependencies (including streamlit)
# Ensure required libraries are installed
!pip install -q -r signal_bot/requirements.txt
print("Dependencies installed.")

# Optional: Verify streamlit installation
!pip show streamlit

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot', exist_ok=True)

# Write the content to the file
with open('signal_bot/main.py', 'w') as f:
    f.write('''# main.py
import pandas as pd
import os
from utils.coingecko_api import get_top_coins
from indicators.ta_utils import compute_indicators
from signals.generate_signals import generate_signal

DATA_DIR = 'signal_bot/data'
os.makedirs(DATA_DIR, exist_ok=True)

def main():
    print("Fetching top 10 coin data...")
    data = get_top_coins()
    df = pd.DataFrame(data)
    df["timestamp"] = pd.Timestamp.utcnow()
    df.to_csv(f"{DATA_DIR}/top10_market_data.csv", index=False)
    print("Data fetched and saved.")

    print("Computing technical indicators...")
    # For now, we'll use the current price as 'close' for indicator calculation
    # In a real scenario, we'd fetch historical data for proper indicator calculation
    df["close"] = df["current_price"]
    df_ind = compute_indicators(df.copy())
    df_ind.to_csv(f"{DATA_DIR}/top10_with_indicators.csv", index=False)
    print("Indicators computed and saved.")

    print("Generating signals...")
    # Note: The current generate_signal function expects a single row or a dataframe
    # with sufficient historical data. For this initial test with only current data,
    # the signals will likely be 'HOLD' or based on limited info.
    df_signals = generate_signal(df_ind) # This needs adjustment for processing multiple rows
    df_signals.to_csv(f"{DATA_DIR}/signals.csv", index=False)
    print("Signals generated and saved.")

if __name__ == "__main__":
    main()
''')

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot/signals', exist_ok=True)

# Write the content to the file
with open('signal_bot/signals/generate_signals.py', 'w') as f:
    f.write('''# signals/generate_signals.py
def generate_signal(df):
    signals = []

    if df['rsi'].iloc[-1] < 30:
        signals.append("BUY (RSI oversold)")

    if df['macd'].iloc[-1] > 0 and df['macd'].iloc[-2] < 0:
        signals.append("BUY (MACD crossover)")

    if df['close'].iloc[-1] > df['ema_20'].iloc[-1]:
        signals.append("UPTREND")

    return signals''')

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot/indicators', exist_ok=True)

# Write the content to the file
with open('signal_bot/indicators/ta_utils.py', 'w') as f:
    f.write('''# indicators/ta_utils.py
import pandas as pd
import ta  # pip install ta

def compute_indicators(df):
    df = df.copy()
    df['rsi'] = ta.momentum.RSIIndicator(df['close']).rsi()
    df['ema_20'] = ta.trend.EMAIndicator(df['close'], window=20).ema_indicator()
    df['macd'] = ta.trend.MACD(df['close']).macd_diff()
    bb = ta.volatility.BollingerBands(df['close'], window=20)
    df['bb_upper'] = bb.bollinger_hband()
    df['bb_lower'] = bb.bollinger_lband()
    return df''')

# Create the directory if it doesn't exist
import os
os.makedirs('signal_bot/utils', exist_ok=True)

# Write the content to the file
with open('signal_bot/utils/coingecko_api.py', 'w') as f:
    f.write('''# utils/coingecko_api.py
import requests

BASE_URL = "https://api.coingecko.com/api/v3"

def get_top_coins(limit=10):
    url = f"{BASE_URL}/coins/markets"
    params = {
        "vs_currency": "usd",
        "order": "market_cap_desc",
        "per_page": limit,
        "page": 1,
        "sparkline": "true"
    }
    response = requests.get(url, params=params)
    return response.json()

def get_coin_history(coin_id, days="30"):
    url = f"{BASE_URL}/coins/{coin_id}/market_chart"
    params = {"vs_currency": "usd", "days": days, "interval": "hourly"}
    response = requests.get(url, params=params)
    return response.json()''')

import os

project_dirs = [
    'signal_bot/data',
    'signal_bot/indicators',
    'signal_bot/models',
    'signal_bot/signals',
    'signal_bot/utils'
]

for dir_path in project_dirs:
    os.makedirs(dir_path, exist_ok=True)

# Create an empty requirements.txt for now
with open('signal_bot/requirements.txt', 'w') as f:
    pass

"""# Task
Integrate the provided Python code snippets for fetching CoinGecko data, computing technical indicators, and generating basic trading signals into a single executable script to test the core functionality before adding machine learning capabilities.

## Create a main script

### Subtask:
Create a new Python file, likely `main.py`, to orchestrate the process.

**Reasoning**:
Create the `main.py` file and write the initial structure, including necessary imports and the main execution block.

## Import necessary modules

### Subtask:
Import the functions from `coingecko_api.py`, `ta_utils.py`, and `generate_signals.py`.

**Reasoning**:
Import the necessary functions from the created Python files into the main script.

## Fetch historical data

### Subtask:
Fetch historical data for a specific coin using the `get_coin_history` function from `coingecko_api.py`.

**Reasoning**:
Modify the main function to fetch historical data for a selected coin and convert it to a DataFrame suitable for indicator calculation.

**Reasoning**:
Correct the syntax error in the string literal by escaping the apostrophe in "prices".

## Compute technical indicators

### Subtask:
Compute technical indicators on the historical data using the `compute_indicators` function from `ta_utils.py`.

## Generate signals

### Subtask:
Generate signals on the historical data using the `generate_signal` function from `signals.generate_signals.py`.

**Reasoning**:
I need to modify the `signal_bot/main.py` file to generate signals using the `history_df_ind` DataFrame and save the output to a new file indicating historical signals. I will locate the section where signals are generated and update it to use `history_df_ind` and change the output filename.

## Display results

### Subtask:
Display the generated signals along with the relevant data.

**Reasoning**:
Modify the main.py file to display the head of the historical signals dataframe.

## Refine and test

### Subtask:
Refine the code in `signal_bot/main.py` and test its execution to ensure it runs without errors and produces the expected output.

**Reasoning**:
Execute the `signal_bot/main.py` script to test the integration of the different components and observe the output and generated files.

**Reasoning**:
The traceback indicates that `get_coin_history` is not defined in `main.py`. I need to add the import for this function from `utils.coingecko_api`.

**Reasoning**:
Now that `get_coin_history` is imported, I need to execute the script again to check if the previous error is resolved and if the rest of the script runs as expected.

**Reasoning**:
The traceback indicates a `KeyError: 'prices'`, meaning the `history_data` dictionary returned by `get_coin_history` does not contain a key named 'prices'. I need to inspect the structure of `history_data` to understand what keys are available and how to access the price information. I will add a print statement to the `main` function to inspect the keys.

**Reasoning**:
Now that I've added the print statement to inspect the keys of `history_data`, I need to execute the script again to see the output and identify the correct key for the price data.

**Reasoning**:
The output shows that `history_data` only contains the key 'status', indicating that the call to `get_coin_history` failed to return the historical price data. This could be due to an issue with the API call itself. I need to investigate why the API call might be failing. I will remove the unnecessary print statement and add a check for the 'prices' key before attempting to create the DataFrame to handle cases where the API call is unsuccessful.

**Reasoning**:
I have added a check for the 'prices' key in `history_data`. Now I need to execute the script again to see if the error is handled and if the script can proceed or exit gracefully.

**Reasoning**:
The script now correctly identifies that historical data is not available but exits the `main` function prematurely. The issue is likely with the `get_coin_history` function or the CoinGecko API itself at this moment. Since the primary goal of this subtask is to refine the code and test its execution flow with the provided components, and the current API response prevents further processing of historical data, I will proceed by focusing on the top 10 coin data, which was fetched successfully. I will modify the script to ensure the signal generation and display parts for the historical data are skipped when historical data is not available, allowing the script to complete the processing of the top 10 coin data. I will remove the previous check and instead wrap the historical data processing in a try-except block to handle the potential `KeyError` more robustly and continue with the top 10 data processing.

## Refine and test retry 1

### Subtask:
Refine the code in `signal_bot/main.py` and test its execution to ensure it runs without errors and processes at least the top 10 coin data, even if historical data fetching fails.

**Reasoning**:
Open the `signal_bot/main.py` file, locate the historical data processing block, wrap it in a try-except block to handle potential errors, ensure `history_df_signals` is initialized in the except block, and add print statements to confirm signal generation and display the head of the dataframes. Finally, save the modified file.

**Reasoning**:
Execute the modified `signal_bot/main.py` script to test the implemented error handling for historical data fetching and processing, and to verify that top 10 data processing and signal generation still occur.

**Reasoning**:
The previous execution showed that the historical data fetching failed with "Error: Historical data not available.". The current code in `main.py` exits the `main` function if historical data is not available, preventing the processing of top 10 data and signal generation for the top 10. The subtask requires ensuring that top 10 data is processed even if historical data fetching fails. I need to modify the `main` function to remove the `return` statement within the historical data check and ensure that the code for processing the top 10 data and generating its signals is executed regardless of the historical data fetching outcome. I also need to adjust the try-except block to cover the historical data processing part correctly. Finally, I will execute the script again to verify the fix.

**Reasoning**:
The input contains a command `code_block` followed by a block of python code.

**Reasoning**:
The previous execution resulted in a `SyntaxError: expected 'except' or 'finally' block` on the `try:` line within the historical data processing block. This indicates an issue with the structure of the try-except block or surrounding code after the modification. I need to review the `signal_bot/main.py` file to identify and fix the syntax error. I will read the file content, find the `try:` block, and examine the indentation and structure to ensure it is correctly formed with a corresponding `except` block. I will then rewrite the file with the corrected syntax and execute it again to verify the fix.

**Reasoning**:
The previous attempt to fix the syntax error failed because the script could not find the `except` block. This indicates an issue with the logic used to locate the `except` block within the `main` function lines. I need to re-examine the code in `signal_bot/main.py` to understand the current structure and correctly identify the `except` block. I will then rewrite the code to accurately locate the `try` and `except` blocks, extract the historical data processing code, wrap it in a correctly formatted try-except block, and insert it back into the `main` function. I will also ensure that the top 10 data processing and signal generation code remains outside this block and that print statements are added to confirm the execution flow and display the results. Finally, I will execute the script to test the changes.

**Reasoning**:
The script failed to find the historical data fetch or top 10 signal generation line, which is needed to define the historical processing block. This suggests that the lines identifying these sections were not found in the current content of `signal_bot/main.py`. I need to carefully re-examine the current state of the `main.py` file to identify the exact text of the lines that should mark the beginning and end of the historical data processing block and the start of the top 10 signal generation. Once I have the correct line content, I will modify the script to accurately locate these lines and proceed with wrapping the historical data processing in a try-except block, ensuring top 10 processing is outside, and adding the necessary print statements. Finally, I will execute the script to test the changes.

**Reasoning**:
The output shows the current content of `signal_bot/main.py`, which has nested `try` blocks and duplicated `except` blocks, causing the `SyntaxError`. It also shows that the script could not find the expected lines for modification, likely due to the previous failed attempts altering the file structure. I need to rewrite the `main` function to have a clear structure: fetch top 10 data, compute indicators for top 10, then a `try...except` block for fetching historical data, processing it, computing indicators, and generating signals for historical data. Outside the `try...except` block, generate signals for the top 10 data and add print statements to display the results for both. I will then execute the script to verify the fix and ensure that the top 10 data is processed even if historical data fetching fails.

## Refine and test retry 2

### Subtask:
Refine the code in `signal_bot/main.py` and test its execution to ensure it runs without errors and processes at least the top 10 coin data, even if historical data fetching fails.

**Reasoning**:
Rewrite the entire content of `signal_bot/main.py` with the corrected and robust structure as described in the instructions, including imports, the main function, fetching and processing of top 10 data, a try-except block for historical data, and print statements for results. Then, execute the script to test.

**Reasoning**:
The previous execution failed because the `generate_signal` function from `signals/generate_signals.py` returns a list of strings, not a DataFrame, which caused an error when trying to call `.to_csv()` and `.head()` on the result. The `generate_signal` function needs to be updated to return a DataFrame with the signals. I will modify the `signals/generate_signals.py` file to return a DataFrame and then re-execute `signal_bot/main.py` to test the fix and ensure the top 10 data is processed.

**Reasoning**:
The previous command failed due to a `SyntaxError: incomplete input`. This is likely caused by using triple quotes within the triple-quoted string literal for the file content. I need to fix the syntax in the string literal for `signals/generate_signals.py` by escaping any internal triple quotes or using a different quoting mechanism if necessary. I will rewrite the code block with the corrected string and then re-execute the main script.

**Reasoning**:
The previous command failed with a `SyntaxError: invalid syntax` in `signals/generate_signals.py`. This is because the code block included a shell command (`!python signal_bot/main.py`) within the Python code that was being written to the file. Shell commands should not be included in Python scripts like this. I need to remove the shell command from the string literal that writes to `signals/generate_signals.py` and execute the shell command in a separate code block.

**Reasoning**:
I have fixed the `signals/generate_signals.py` file. Now I need to execute `signal_bot/main.py` to test if the script runs without errors and processes the top 10 data, even if historical data fetching fails.

## Summary:

### Data Analysis Key Findings

*   The `main.py` script was successfully created to orchestrate the data fetching, technical indicator computation, and signal generation process.
*   The script successfully fetches the top 10 coin data, saves it to a CSV file, computes technical indicators on this data, and saves the result to another CSV.
*   The script attempts to fetch historical data for a specific coin (Bitcoin), but this process consistently failed during testing due to the CoinGecko API response not containing the expected 'prices' key.
*   Error handling (`try...except`) was implemented around the historical data processing block in `main.py` to prevent the script from crashing if historical data fetching or processing fails.
*   The `generate_signal` function was updated to ensure it returns a pandas DataFrame containing only the generated signals.
*   The script now successfully runs without errors, processing the top 10 coin data and gracefully handling the historical data fetching failure.
*   The heads of both the historical signals DataFrame (which is empty when fetching fails) and the top 10 snapshot signals DataFrame are printed to the console, confirming the execution flow and generated output for the top 10 data.

### Insights or Next Steps

*   Investigate the CoinGecko API `get_coin_history` function call or API key (if applicable) to understand why the 'prices' key is missing, as this is crucial for historical analysis.
*   Further refine the `generate_signal` function to produce meaningful signals from both historical time-series data and the single-row snapshot data, considering the limitations of using only current price for indicators on the top 10 snapshot.

# Task
Integrate the provided Python code snippets for a CoinGecko signal bot, including modules for data fetching, technical indicators, and basic signal generation. Ensure the code is functional and ready for testing before incorporating machine learning capabilities.

## Investigate coingecko api `get coin history`

### Subtask:
Investigate the CoinGecko API `get_coin_history` function call or API key (if applicable) to understand why the 'prices' key is missing, as this is crucial for historical analysis.

**Reasoning**:
I will examine the `get_coin_history` function in `signal_bot/utils/coingecko_api.py` and the relevant CoinGecko API documentation, and add more robust error handling and logging to the function to help diagnose why the 'prices' key might be missing.

**Reasoning**:
The error message from the previous execution clearly indicates that the `interval=hourly` parameter in the `get_coin_history` function is only available for Enterprise plan customers. The CoinGecko API documentation tips suggest that hourly data can be retrieved automatically for 'days' values between 2 and 90 without specifying the interval. I need to modify the `get_coin_history` function to remove the `interval='hourly'` parameter and update the `main.py` script to handle the case where `get_coin_history` returns `None` due to an API error, preventing the "argument of type 'NoneType' is not iterable" error. I will also try fetching historical data for a different coin to further investigate.

**Reasoning**:
The traceback indicates a `SyntaxError: 'continue' not properly in loop`. This is because the `continue` statement was added inside an `if` block within a `try` block, but not within a loop construct. The intention was to skip the rest of the historical data processing block when fetching fails. Instead of using `continue`, I should structure the code to only execute the processing steps if `history_data` is not `None`. I will modify `main.py` to remove the incorrect `continue` and wrap the historical data processing logic in an `if history_data is not None:` block. I will also revert the coin ID back to "bitcoin" for consistency in testing the historical data fetching mechanism itself before introducing variations.

**Reasoning**:
The `IndentationError` indicates that the line `print("Historical data fetching failed. Skipping historical data processing.")` is not properly indented within the `if history_data is not None:` block. My previous attempt to modify `main.py` to wrap the historical data processing in an `if` block introduced incorrect indentation. I need to fix the indentation in `main.py` to ensure that the code within the `if history_data is not None:` block is correctly indented and the code outside of it maintains its original indentation. I will rewrite the relevant section of the `main.py` file to ensure correct indentation and logic for handling the `None` case from `get_coin_history`.

## Summary:

### Data Analysis Key Findings

*   The initial attempt to fetch historical data using `get_coin_history` failed with a `401 Client Error: Unauthorized`.
*   The error message indicated that the parameter `interval=hourly` is restricted to CoinGecko Enterprise plan customers.
*   Removing the `interval='hourly'` parameter from the `get_coin_history` function resolved the API error and allowed the historical data, including the crucial 'prices' key, to be successfully retrieved.
*   Initial attempts to implement error handling for a failed `get_coin_history` call in `main.py` led to `SyntaxError` and `IndentationError`.
*   Correctly restructuring the error handling logic in `main.py` allowed the script to gracefully handle cases where historical data fetching might fail while still processing other data.

### Insights or Next Steps

*   Ensure that API calls adhere to the restrictions of the chosen plan (e.g., free vs. enterprise) to avoid unexpected errors.
*   Refine the error handling and logging mechanisms in the `main.py` script to provide more specific information about which part of the data processing pipeline failed.

# Task
Integrate the provided Python code snippets for a CoinGecko signal bot, including data fetching, technical indicator calculation, anomaly detection, and ML feature logging. Refine the signal generation logic and implement a dedicated signal finder module. Explore using a larger window for indicator calculation and add logging to the main script. The code should be structured into modular files as described in the initial plan.

## Refine the `generate signal` function

### Subtask:
Refine the `generate_signal` function to produce more meaningful signals based on the technical indicators, especially for time-series historical data where indicator values change over time.

**Reasoning**:
I need to modify the `generate_signal` function in `signal_bot/signals/generate_signals.py` to iterate through the DataFrame and apply signal logic based on technical indicators, returning a DataFrame with signals.

**Reasoning**:
The `generate_signal` function has been updated. Now I need to test it by executing the main script, which will call this function with historical data.

**Reasoning**:
The output indicates that the `generate_signal` function is returning "HOLD" signals for both top 10 snapshot data and historical data, with a warning "Insufficient data or missing columns for signal generation." This suggests that the DataFrame passed to `generate_signal` either has missing required columns or is empty, even though previous steps indicate data was fetched and indicators computed. I need to inspect the DataFrames (`df_ind_top10` and `history_df_ind`) being passed to `generate_signal` in `main.py` to confirm they contain the expected columns and data after indicator computation. I will add print statements in `main.py` before calling `generate_signal` to display the columns and a few rows of these DataFrames.

**Reasoning**:
The print statements show that `df_ind_top10` has all the required columns including technical indicators, but `history_df_ind` has `NaN` values for all technical indicators in the head, and the warning indicates "Insufficient data or missing columns". This confirms that the issue is not missing columns, but likely insufficient data points to compute the indicators, specifically for the window size used (20 for EMA, MACD, and Bollinger Bands, and 14 for RSI by default). The historical data fetched is for 30 days, and hourly data is being requested (although the 'interval' parameter was removed, the API might default to hourly for this range). To compute indicators with a window of 20, at least 20 data points are needed. The historical data likely starts with fewer than 20 points with valid indicator values due to the rolling window calculation. The top 10 snapshot data only has one row, which is insufficient for any indicator calculation requiring a window greater than 1.

To address this, I need to:
1. Acknowledge in `generate_signals.py` that initial rows might have NaN values and the signal generation logic should handle this or start from an index where indicators are valid.
2. Explicitly handle the case of insufficient data points for indicator calculation in `generate_signals.py` to avoid the misleading "missing columns" warning.
3. Accept that for the single-row top 10 snapshot data, meaningful signals based on time-series indicators cannot be generated, and it's appropriate to return "HOLD" or a specific message.

I will modify `generate_signals.py` to handle NaN values in indicators by checking for them explicitly before applying signal logic and refine the insufficient data check.

**Reasoning**:
I have updated the `generate_signal` function to handle NaN values and insufficient data. Now I need to execute the main script again to test these changes and see if signals are generated for the historical data where indicators are valid.

**Reasoning**:
The output still shows "Insufficient data or missing columns for signal generation" and "HOLD" signals, despite the inspection showing that `history_df_ind` has the necessary columns. The warning message likely means the DataFrame passed to `generate_signal` is either empty or has entirely NaN values for the required indicator columns after filtering, or the length check is too strict. However, the head of `history_df_ind` clearly shows some data rows, even if indicators are NaN initially. The check for `len(df) < min_data_points` might be causing the function to exit prematurely for the historical data. I will modify `generate_signal` to remove the strict length check at the beginning and rely solely on dropping NaNs to find the first valid index for signal computation.

**Reasoning**:
I have updated the `generate_signal` function to handle NaN values and remove the strict length check. Now I need to execute the main script again to test these changes and see if signals are generated for the historical data starting from the first row with valid indicators.

**Reasoning**:
The output still shows the "Insufficient data or missing columns" warning and all "HOLD" signals, which is unexpected given the modifications. The warning message in `generate_signal` needs to be more specific to understand if it's an empty DataFrame, missing columns, or if all indicator values are NaN after filtering. I will refine the warning message in `generate_signal` to pinpoint the exact condition causing the function to return "HOLD". This will help diagnose why signals are not being generated for the historical data.

## Implement the `signal finder.py` module

### Subtask:
Implement the `signal finder.py` module to house more complex signal generation logic, potentially incorporating multiple indicator conditions and strategies.

**Reasoning**:
Create the `signal_finder.py` file and define the `find_signals` function with basic structure and imports.

# Task
Build a highly efficient, precise signal bot for CoinGecko with ML capabilities, integrating data collection, technical indicator calculation, signal generation, anomaly detection, ML model training, and backtesting into a scheduled workflow.

## Implement full data collection

### Subtask:
Implement full data collection: Fetch and save the `full_market_snapshot.csv` file required for the anomaly detection job.

**Reasoning**:
Modify the main function in signal_bot/main.py to fetch a larger market snapshot using get_top_coins with a higher limit and save it to full_market_snapshot.csv.

**Reasoning**:
Execute the main script to test if the full market snapshot is fetched and saved correctly, and if the rest of the pipeline runs as expected.

**Reasoning**:
The traceback indicates a `SyntaxError: unterminated string literal` in `signal_bot/main.py` at line 153. This suggests an issue with a print statement or string within that line or the surrounding lines. I need to examine the `main.py` file, identify the problematic string literal around line 153, and fix the syntax error, likely by ensuring quotes are properly matched or escaped.

**Reasoning**:
The output shows the `main.py` file content is truncated, ending with an incomplete `print("No anoma...`. This is the cause of the `SyntaxError: unterminated string literal`. I need to rewrite the entire `main.py` content, ensuring the string literal is complete and correctly formatted, including closing quotes for all print statements.

**Reasoning**:
The input text contains a Python code block with the command `!python -m signal_bot.main`.

**Reasoning**:
The traceback still shows a `SyntaxError: unterminated string literal` at line 153 in `main.py`, even after rewriting the file content. This is unexpected and suggests the previous rewrite did not correctly fix the string literal or there's a persistent issue. I will attempt to rewrite the `main.py` content again, paying close attention to all string literals, especially around the `print` statements in the results display section, to ensure they are properly closed.

**Reasoning**:
The `SyntaxError` should be resolved after rewriting the `main.py` file with the complete and correctly formatted code. Now I need to execute the script again to verify that it runs without errors and that the full market snapshot is fetched and saved as required by the subtask.

**Reasoning**:
The `SyntaxError: unterminated string literal` at line 153 in `main.py` persists despite multiple attempts to rewrite the file content. This is highly unusual and suggests a potential issue with how the string literal is being handled or interpreted in this environment. Since I cannot directly debug the file content line by line within this format, and rewriting the entire string literal repeatedly isn't resolving the issue, I will try a different approach to write the file content. I will write the content of `main.py` piece by piece, carefully constructing the string literal for each section and writing it to the file incrementally. This might help isolate where the string literal is being terminated prematurely.

# Task
Integrate the scheduler for automated runs, refine data collector for pagination, explore ML model prediction, refine signal generation logic and backtesting, and consider production deployment for the CoinGecko signal bot.

## Integrate the scheduler for automated runs

### Subtask:
Decide how to run the `scheduler.py` script continuously to automate the scheduled jobs (e.g., as a background process, in a dedicated environment). In a Colab notebook, this typically involves running `!python -m signal_bot.scheduler` in a cell and keeping that cell executing.

**Reasoning**:
Modify the `signal_bot/scheduler.py` file to uncomment the `scheduler.start()` call and execute the script to start the scheduler.

**Reasoning**:
The traceback indicates that the `signal_bot.data_collector` module was not found. I need to create the `signal_bot/data_collector.py` file with the necessary content so that the scheduler can import and use it.

**Reasoning**:
The traceback indicates that the `signal_bot.utils` module was not found, specifically when `data_collector.py` tried to import `get_top_coins` from `signal_bot.utils.coingecko_api`. This means the `utils` directory or the `coingecko_api.py` file within it is missing or not structured correctly. I need to create the `signal_bot/utils/coingecko_api.py` file with the necessary content, including the `get_top_coins` function.

**Reasoning**:
The traceback indicates that the `signal_bot.logger` module was not found, specifically when `coingecko_api.py` tried to import logging functions from it. This means the `signal_bot/logger.py` file is missing. I need to create this file with the necessary logging setup functions.

**Reasoning**:
The traceback indicates that the `signal_bot.indicators` module was not found, specifically when `scheduler.py` tried to import `compute_indicators` from `signal_bot.indicators.ta_utils`. This means the `indicators` directory or the `ta_utils.py` file within it is missing or not structured correctly. I need to create the `signal_bot/indicators/ta_utils.py` file with the necessary content, including the `compute_indicators` function.

**Reasoning**:
The traceback indicates that the `signal_bot.signals` module was not found, specifically when `scheduler.py` tried to import `find_signals` from `signal_bot.signals.signal_finder`. This means the `signals` directory or the `signal_finder.py` file within it is missing or not structured correctly. I need to create the `signal_bot/signals/signal_finder.py` file with the necessary content, including the `find_signals` function.

**Reasoning**:
The traceback indicates that the `signal_bot.ml_logger` module was not found, specifically when `scheduler.py` tried to import `log_ml_features` from it. This means the `signal_bot/ml_logger.py` file is missing. I need to create this file with the necessary content, including the `log_ml_features` function.

**Reasoning**:
The traceback indicates that the `signal_bot.anomaly_detector` module was not found, specifically when `scheduler.py` tried to import `detect_anomalies` from it. This means the `signal_bot/anomaly_detector.py` file is missing. I need to create this file with the necessary content, including the `detect_anomalies` function.

# Task
Guide the user through setting up a development environment in Google Cloud Shell Editor by creating a GitHub repository, uploading the existing code, cloning the repository into Cloud Shell, setting up the environment, and explaining how to work on the code and push changes back to GitHub.

## Create a github repository

### Subtask:
If you don't already have one for this project, create a new public or private repository on GitHub (e.g., `crypto-signal-bot`).

## Upload your code to github

### Subtask:
Upload your code to github: Add all the project files and directories (`signal_bot/`, `.env`, etc.) to this new GitHub repository. You can do this via the GitHub web interface or using Git commands if you're familiar with them (e.g., `git clone`, `git add`, `git commit`, `git push`).

## Open google cloud shell editor

### Subtask:
Open google cloud shell editor

## Clone your github repository in cloud shell

### Subtask:
Clone your github repository in cloud shell: In the Cloud Shell Editor terminal, use the `git clone` command to clone your project repository from GitHub.

**Reasoning**:
Execute the git clone command in the Cloud Shell Editor terminal to clone the GitHub repository.

## Clone your github repository in cloud shell

### Subtask:
Clone your github repository in cloud shell: In the Cloud Shell Editor terminal, use the `git clone` command to clone your project repository from GitHub.

## Set up the environment in cloud shell

### Subtask:
Set up the environment in cloud shell: Navigate into your cloned project directory in the Cloud Shell terminal, ensure Python is installed, install project dependencies, and set up your CoinGecko API key.

## Work on your code in cloud shell editor

### Subtask:
Work on your code in cloud shell editor: You can now edit your project files directly within the Cloud Shell Editor's browser-based IDE. Use the integrated terminal to run your scripts (e.g., `python -m signal_bot.main`, `python -m signal_bot.scheduler`).

**Reasoning**:
The traceback shows a `SyntaxError: unterminated string literal` at line 189 in `signal_bot/main.py`. I need to regenerate the `main.py` file, carefully ensuring all string literals, especially multi-line strings and print statements, are correctly closed and formatted to resolve this syntax error and make the script executable.
"""