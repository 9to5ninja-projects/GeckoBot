# scheduler.py
from apscheduler.schedulers.blocking import BlockingScheduler
from signal_bot.data_collector import collect_data
from signal_bot.indicators.ta_utils import compute_indicators
from signal_bot.signals.signal_finder import find_signals
from signal_bot.ml_logger import log_ml_features
from signal_bot.anomaly_detector import detect_anomalies
import pandas as pd
import os
from signal_bot.logger import setup_logger, log_info, log_error


scheduler = BlockingScheduler()

@scheduler.scheduled_job('interval', minutes=10)
def pipeline_job():
    setup_logger()
    log_info("Running pipeline job...")

    DATA_DIR = 'signal_bot/data'
    top10_input_path = os.path.join(DATA_DIR, "top10_market_data.csv")
    top10_indicators_path = os.path.join(DATA_DIR, "top10_with_indicators.csv")
    ml_log_output_path = os.path.join(DATA_DIR, "ml_training.csv")
    top10_signals_path = os.path.join(DATA_DIR, "top10_signals.csv")

    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently
    # For now, we rely on the full_data_collection_job which runs hourly.
    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.
    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)

    if not os.path.exists(top10_input_path):
        log_info(f"Warning: {top10_input_path} not found. Skipping pipeline job.")
        return

    try:
        log_info("Computing indicators for top 10...")
        # compute_indicators expects input_csv and output_csv paths
        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)
        log_info("Indicators computed for top 10.")

        log_info("Generatingsignals for top 10...")
        # find_signals expects a DataFrame
        df_signals_top10 = find_signals(df_ind_top10.copy())
        df_signals_top10.to_csv(top10_signals_path, index=False)
        log_info("Signals generated for top 10.")

        log_info("Logging ML features for top 10...")
        # log_ml_features expects indicator_csv and output_csv paths
        log_ml_features(top10_indicators_path, ml_log_output_path)
        log_info("ML features logged for top 10.")

    except Exception as e:
        log_error(f"Error in pipeline job: {e}")

    log_info("Pipeline job finished.")


@scheduler.scheduled_job('interval', hours=1)
def anomaly_job():
    setup_logger()
    log_info("Running anomaly job...")

    DATA_DIR = 'signal_bot/data'
    full_snapshot_path = os.path.join(DATA_DIR, "full_market_snapshot.csv")
    anomalies_output_dir = DATA_DIR

    if not os.path.exists(full_snapshot_path):
        log_info(f"Warning: {full_snapshot_path} not found. Skipping anomaly job.")
        return

    try:
        log_info("Running Anomaly Detection...")
        # detect_anomalies expects snapshot_csv and output_dir
        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)
        log_info(f"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.")

    except Exception as e:
        log_error(f"Error in anomaly job: {e}")

    log_info("Anomaly job finished.")


@scheduler.scheduled_job('interval', hours=1)
def full_data_collection_job():
    setup_logger()
    log_info("Running full data collection job...")
    try:
        # collect_data returns a DataFrame
        collected_df = collect_data(limit=250)

        if collected_df is not None:
             log_info("Full market data collection completed.")
        else:
             log_error("Full market data collection failed.")

    except Exception as e:
        log_error(f"Error in full data collection job: {e}")

    log_info("Full data collection job finished.")


print("Starting scheduler...")
scheduler.start() # Uncommented to start the scheduler
