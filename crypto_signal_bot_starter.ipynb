{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/9to5ninja-projects/GeckoBot/blob/main/crypto_signal_bot_starter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sh9ajBn7qROy"
      },
      "source": [
        "# 🚀 Crypto Signal Bot — Google Colab Starter Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pcVkqvn4qRO0",
        "outputId": "e4b70a40-dea8-4149-d6a9-2673508308cc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "7dEOtxkvqRO2"
      },
      "outputs": [],
      "source": [
        "# Set data path\n",
        "DATA_DIR = '/content/drive/MyDrive/crypto_signal_bot/data'\n",
        "\n",
        "# Create directory\n",
        "import os\n",
        "os.makedirs(DATA_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "158d36d0",
        "outputId": "9bb429aa-84b4-420e-e367-4c5c215902bd"
      },
      "source": [
        "# Run the Streamlit dashboard again\n",
        "!streamlit run signal_bot/dashboard.py"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: streamlit: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ab0bf5f1",
        "outputId": "faf3809c-b325-4a22-dfae-f1e6b2dfd3ee"
      },
      "source": [
        "# Run the Streamlit dashboard again with updated code\n",
        "!streamlit run signal_bot/dashboard.py"
      ],
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.45.187.235:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/bin/streamlit\", line 8, in <module>\n",
            "    sys.exit(main())\n",
            "             ^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1442, in __call__\n",
            "    return self.main(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1363, in main\n",
            "    rv = self.invoke(ctx)\n",
            "         ^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1830, in invoke\n",
            "    return _process_result(sub_ctx.command.invoke(sub_ctx))\n",
            "                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 1226, in invoke\n",
            "    return ctx.invoke(self.callback, **ctx.params)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 793, in invoke\n",
            "    with ctx:\n",
            "         ^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/core.py\", line 489, in __exit__\n",
            "    pop_context()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/click/globals.py\", line 49, in pop_context\n",
            "    def pop_context() -> None:\n",
            "    \n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/bootstrap.py\", line 43, in signal_handler\n",
            "    server.stop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/web/server/server.py\", line 509, in stop\n",
            "    self._runtime.stop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/streamlit/runtime/runtime.py\", line 329, in stop\n",
            "    async_objs.eventloop.call_soon_threadsafe(stop_on_eventloop)\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 844, in call_soon_threadsafe\n",
            "    self._check_closed()\n",
            "  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 545, in _check_closed\n",
            "    raise RuntimeError('Event loop is closed')\n",
            "RuntimeError: Event loop is closed\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f4717b5",
        "outputId": "f18b84fb-2504-4797-89c4-fc6b3cf2b604"
      },
      "source": [
        "# Execute the main script again to ensure data files are up to date\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 188\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 188)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2b088edc",
        "outputId": "bc464d73-3141-4354-989c-70bce7483488"
      },
      "source": [
        "# Install Streamlit\n",
        "!pip install streamlit"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.48.1-py3-none-any.whl.metadata (9.5 kB)\n",
            "Requirement already satisfied: altair!=5.4.0,!=5.4.1,<6,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.0)\n",
            "Requirement already satisfied: blinker<2,>=1.5.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (1.9.0)\n",
            "Requirement already satisfied: cachetools<7,>=4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.5.2)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.2.1)\n",
            "Requirement already satisfied: numpy<3,>=1.23 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.0.2)\n",
            "Requirement already satisfied: packaging<26,>=20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (25.0)\n",
            "Requirement already satisfied: pandas<3,>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.2.2)\n",
            "Requirement already satisfied: pillow<12,>=7.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (11.3.0)\n",
            "Requirement already satisfied: protobuf<7,>=3.20 in /usr/local/lib/python3.12/dist-packages (from streamlit) (5.29.5)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (18.1.0)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.12/dist-packages (from streamlit) (2.32.4)\n",
            "Requirement already satisfied: tenacity<10,>=8.1.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (8.5.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.12/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from streamlit) (4.14.1)\n",
            "Requirement already satisfied: watchdog<7,>=2.1.5 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.0.0)\n",
            "Requirement already satisfied: gitpython!=3.1.19,<4,>=3.0.7 in /usr/local/lib/python3.12/dist-packages (from streamlit) (3.1.45)\n",
            "Collecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: tornado!=6.5.0,<7,>=6.0.3 in /usr/local/lib/python3.12/dist-packages (from streamlit) (6.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.1.6)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (4.25.1)\n",
            "Requirement already satisfied: narwhals>=1.14.2 in /usr/local/lib/python3.12/dist-packages (from altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2.1.2)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.12/dist-packages (from gitpython!=3.1.19,<4,>=3.0.7->streamlit) (4.0.12)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas<3,>=1.4.0->streamlit) (2025.2)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.27->streamlit) (2025.8.3)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.12/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit) (5.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (3.0.2)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema>=3.0->altair!=5.4.0,!=5.4.1,<6,>=4.0->streamlit) (0.27.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.4.0->streamlit) (1.17.0)\n",
            "Downloading streamlit-1.48.1-py3-none-any.whl (9.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.9/9.9 MB\u001b[0m \u001b[31m48.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pydeck, streamlit\n",
            "Successfully installed pydeck-0.9.1 streamlit-1.48.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6SpuBWQvqRO2",
        "outputId": "6a5e0277-904b-4f27-c85d-99bb53035b57"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Collecting ta\n",
            "  Downloading ta-0.11.0.tar.gz (25 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Building wheels for collected packages: ta\n",
            "  Building wheel for ta (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ta: filename=ta-0.11.0-py3-none-any.whl size=29412 sha256=0b1bc9d0744873c9193ed6e66e2424a74c99e6ce110ca46783b502deaea7beec\n",
            "  Stored in directory: /root/.cache/pip/wheels/5c/a1/5f/c6b85a7d9452057be4ce68a8e45d77ba34234a6d46581777c6\n",
            "Successfully built ta\n",
            "Installing collected packages: ta\n",
            "Successfully installed ta-0.11.0\n"
          ]
        }
      ],
      "source": [
        "# Install dependencies\n",
        "!pip install pandas ta scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "_dUa4Co4qRO3"
      },
      "outputs": [],
      "source": [
        "# Imports\n",
        "import pandas as pd\n",
        "import requests\n",
        "import ta"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyXHrhzZqRO3"
      },
      "source": [
        "## 🪙 Top 10 Coin Data Fetch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 429
        },
        "id": "ro7UW7ViqRO4",
        "outputId": "2a2d7e35-51c6-4cbb-c38a-506adade1b2d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "            id symbol      name  \\\n",
              "0      bitcoin    btc   Bitcoin   \n",
              "1     ethereum    eth  Ethereum   \n",
              "2       ripple    xrp       XRP   \n",
              "3       tether   usdt    Tether   \n",
              "4  binancecoin    bnb       BNB   \n",
              "\n",
              "                                               image  current_price  \\\n",
              "0  https://coin-images.coingecko.com/coins/images...      116390.00   \n",
              "1  https://coin-images.coingecko.com/coins/images...        4747.47   \n",
              "2  https://coin-images.coingecko.com/coins/images...           3.05   \n",
              "3  https://coin-images.coingecko.com/coins/images...           1.00   \n",
              "4  https://coin-images.coingecko.com/coins/images...         884.47   \n",
              "\n",
              "      market_cap  market_cap_rank  fully_diluted_valuation  total_volume  \\\n",
              "0  2318823641626                1            2318825505073   48165145738   \n",
              "1   573773410628                2             573773410628   47192091649   \n",
              "2   181453348875                3             305338655550    8596482307   \n",
              "3   167066241341                4             167066241341  105419685804   \n",
              "4   123198089668                5             123198089668    1947760904   \n",
              "\n",
              "    high_24h  ...        ath  ath_change_percentage                  ath_date  \\\n",
              "0  116947.00  ...  124128.00               -6.19882  2025-08-14T00:37:02.582Z   \n",
              "1    4756.71  ...    4878.26               -2.59935  2021-11-10T14:24:19.604Z   \n",
              "2       3.05  ...       3.65              -16.26854  2025-07-18T03:40:53.808Z   \n",
              "3       1.00  ...       1.32              -24.41789  2018-07-24T00:00:00.000Z   \n",
              "4     884.49  ...     882.83                0.15262  2025-08-22T16:22:15.364Z   \n",
              "\n",
              "         atl  atl_change_percentage                  atl_date  \\\n",
              "0  67.810000           1.716081e+05  2013-07-06T00:00:00.000Z   \n",
              "1   0.432979           1.097288e+06  2015-10-20T00:00:00.000Z   \n",
              "2   0.002686           1.135609e+05  2014-05-22T00:00:00.000Z   \n",
              "3   0.572521           7.467015e+01  2015-03-02T00:00:00.000Z   \n",
              "4   0.039818           2.220471e+06  2017-10-19T00:00:00.000Z   \n",
              "\n",
              "                                                 roi  \\\n",
              "0                                               None   \n",
              "1  {'times': 53.52004050789701, 'currency': 'btc'...   \n",
              "2                                               None   \n",
              "3                                               None   \n",
              "4                                               None   \n",
              "\n",
              "               last_updated  \\\n",
              "0  2025-08-22T16:43:50.262Z   \n",
              "1  2025-08-22T16:43:51.063Z   \n",
              "2  2025-08-22T16:43:50.288Z   \n",
              "3  2025-08-22T16:43:51.726Z   \n",
              "4  2025-08-22T16:43:43.982Z   \n",
              "\n",
              "                                     sparkline_in_7d  \\\n",
              "0  {'price': [119094.80293447574, 118775.09135434...   \n",
              "1  {'price': [4639.459910312292, 4633.74246457088...   \n",
              "2  {'price': [3.1125098270197817, 3.1189465322930...   \n",
              "3  {'price': [1.0005866174726743, 1.0006444577450...   \n",
              "4  {'price': [844.9150105318392, 845.980182138990...   \n",
              "\n",
              "                         timestamp  \n",
              "0 2025-08-22 16:43:52.935153+00:00  \n",
              "1 2025-08-22 16:43:52.935153+00:00  \n",
              "2 2025-08-22 16:43:52.935153+00:00  \n",
              "3 2025-08-22 16:43:52.935153+00:00  \n",
              "4 2025-08-22 16:43:52.935153+00:00  \n",
              "\n",
              "[5 rows x 28 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f5d4e550-ca53-4684-838d-592415e47769\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>symbol</th>\n",
              "      <th>name</th>\n",
              "      <th>image</th>\n",
              "      <th>current_price</th>\n",
              "      <th>market_cap</th>\n",
              "      <th>market_cap_rank</th>\n",
              "      <th>fully_diluted_valuation</th>\n",
              "      <th>total_volume</th>\n",
              "      <th>high_24h</th>\n",
              "      <th>...</th>\n",
              "      <th>ath</th>\n",
              "      <th>ath_change_percentage</th>\n",
              "      <th>ath_date</th>\n",
              "      <th>atl</th>\n",
              "      <th>atl_change_percentage</th>\n",
              "      <th>atl_date</th>\n",
              "      <th>roi</th>\n",
              "      <th>last_updated</th>\n",
              "      <th>sparkline_in_7d</th>\n",
              "      <th>timestamp</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>bitcoin</td>\n",
              "      <td>btc</td>\n",
              "      <td>Bitcoin</td>\n",
              "      <td>https://coin-images.coingecko.com/coins/images...</td>\n",
              "      <td>116390.00</td>\n",
              "      <td>2318823641626</td>\n",
              "      <td>1</td>\n",
              "      <td>2318825505073</td>\n",
              "      <td>48165145738</td>\n",
              "      <td>116947.00</td>\n",
              "      <td>...</td>\n",
              "      <td>124128.00</td>\n",
              "      <td>-6.19882</td>\n",
              "      <td>2025-08-14T00:37:02.582Z</td>\n",
              "      <td>67.810000</td>\n",
              "      <td>1.716081e+05</td>\n",
              "      <td>2013-07-06T00:00:00.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-22T16:43:50.262Z</td>\n",
              "      <td>{'price': [119094.80293447574, 118775.09135434...</td>\n",
              "      <td>2025-08-22 16:43:52.935153+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>ethereum</td>\n",
              "      <td>eth</td>\n",
              "      <td>Ethereum</td>\n",
              "      <td>https://coin-images.coingecko.com/coins/images...</td>\n",
              "      <td>4747.47</td>\n",
              "      <td>573773410628</td>\n",
              "      <td>2</td>\n",
              "      <td>573773410628</td>\n",
              "      <td>47192091649</td>\n",
              "      <td>4756.71</td>\n",
              "      <td>...</td>\n",
              "      <td>4878.26</td>\n",
              "      <td>-2.59935</td>\n",
              "      <td>2021-11-10T14:24:19.604Z</td>\n",
              "      <td>0.432979</td>\n",
              "      <td>1.097288e+06</td>\n",
              "      <td>2015-10-20T00:00:00.000Z</td>\n",
              "      <td>{'times': 53.52004050789701, 'currency': 'btc'...</td>\n",
              "      <td>2025-08-22T16:43:51.063Z</td>\n",
              "      <td>{'price': [4639.459910312292, 4633.74246457088...</td>\n",
              "      <td>2025-08-22 16:43:52.935153+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ripple</td>\n",
              "      <td>xrp</td>\n",
              "      <td>XRP</td>\n",
              "      <td>https://coin-images.coingecko.com/coins/images...</td>\n",
              "      <td>3.05</td>\n",
              "      <td>181453348875</td>\n",
              "      <td>3</td>\n",
              "      <td>305338655550</td>\n",
              "      <td>8596482307</td>\n",
              "      <td>3.05</td>\n",
              "      <td>...</td>\n",
              "      <td>3.65</td>\n",
              "      <td>-16.26854</td>\n",
              "      <td>2025-07-18T03:40:53.808Z</td>\n",
              "      <td>0.002686</td>\n",
              "      <td>1.135609e+05</td>\n",
              "      <td>2014-05-22T00:00:00.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-22T16:43:50.288Z</td>\n",
              "      <td>{'price': [3.1125098270197817, 3.1189465322930...</td>\n",
              "      <td>2025-08-22 16:43:52.935153+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>tether</td>\n",
              "      <td>usdt</td>\n",
              "      <td>Tether</td>\n",
              "      <td>https://coin-images.coingecko.com/coins/images...</td>\n",
              "      <td>1.00</td>\n",
              "      <td>167066241341</td>\n",
              "      <td>4</td>\n",
              "      <td>167066241341</td>\n",
              "      <td>105419685804</td>\n",
              "      <td>1.00</td>\n",
              "      <td>...</td>\n",
              "      <td>1.32</td>\n",
              "      <td>-24.41789</td>\n",
              "      <td>2018-07-24T00:00:00.000Z</td>\n",
              "      <td>0.572521</td>\n",
              "      <td>7.467015e+01</td>\n",
              "      <td>2015-03-02T00:00:00.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-22T16:43:51.726Z</td>\n",
              "      <td>{'price': [1.0005866174726743, 1.0006444577450...</td>\n",
              "      <td>2025-08-22 16:43:52.935153+00:00</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>binancecoin</td>\n",
              "      <td>bnb</td>\n",
              "      <td>BNB</td>\n",
              "      <td>https://coin-images.coingecko.com/coins/images...</td>\n",
              "      <td>884.47</td>\n",
              "      <td>123198089668</td>\n",
              "      <td>5</td>\n",
              "      <td>123198089668</td>\n",
              "      <td>1947760904</td>\n",
              "      <td>884.49</td>\n",
              "      <td>...</td>\n",
              "      <td>882.83</td>\n",
              "      <td>0.15262</td>\n",
              "      <td>2025-08-22T16:22:15.364Z</td>\n",
              "      <td>0.039818</td>\n",
              "      <td>2.220471e+06</td>\n",
              "      <td>2017-10-19T00:00:00.000Z</td>\n",
              "      <td>None</td>\n",
              "      <td>2025-08-22T16:43:43.982Z</td>\n",
              "      <td>{'price': [844.9150105318392, 845.980182138990...</td>\n",
              "      <td>2025-08-22 16:43:52.935153+00:00</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows × 28 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f5d4e550-ca53-4684-838d-592415e47769')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f5d4e550-ca53-4684-838d-592415e47769 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f5d4e550-ca53-4684-838d-592415e47769');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8f157354-1a2a-475f-8f5e-29aa86c8e1b1\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8f157354-1a2a-475f-8f5e-29aa86c8e1b1')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8f157354-1a2a-475f-8f5e-29aa86c8e1b1 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "df_top10"
            }
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "def fetch_top10_market_data():\n",
        "    url = \"https://api.coingecko.com/api/v3/coins/markets\"\n",
        "    params = {\n",
        "        \"vs_currency\": \"usd\",\n",
        "        \"order\": \"market_cap_desc\",\n",
        "        \"per_page\": 10,\n",
        "        \"page\": 1,\n",
        "        \"sparkline\": \"true\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    data = response.json()\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "    return df\n",
        "\n",
        "df_top10 = fetch_top10_market_data()\n",
        "df_top10.to_csv(f\"{DATA_DIR}/top10_market_data.csv\", index=False)\n",
        "df_top10.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L2tanIIbqRO4"
      },
      "source": [
        "## 📊 Technical Indicators"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "eZ8eq1SfqRO4",
        "outputId": "8dcbf1e5-7949-4bbe-bd60-e0f0c1326f65"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  symbol  rsi  ema_20  macd_diff  bb_upper  bb_lower\n",
              "0    btc  NaN     NaN        NaN       NaN       NaN\n",
              "1    eth  NaN     NaN        NaN       NaN       NaN\n",
              "2    xrp  NaN     NaN        NaN       NaN       NaN\n",
              "3   usdt  NaN     NaN        NaN       NaN       NaN\n",
              "4    bnb  NaN     NaN        NaN       NaN       NaN"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0cadc52a-99b6-429b-be42-f64572c96905\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>rsi</th>\n",
              "      <th>ema_20</th>\n",
              "      <th>macd_diff</th>\n",
              "      <th>bb_upper</th>\n",
              "      <th>bb_lower</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>btc</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eth</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xrp</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>usdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bnb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0cadc52a-99b6-429b-be42-f64572c96905')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0cadc52a-99b6-429b-be42-f64572c96905 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0cadc52a-99b6-429b-be42-f64572c96905');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-cedd1ac3-1660-486c-b401-21fa23da4bc2\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-cedd1ac3-1660-486c-b401-21fa23da4bc2')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-cedd1ac3-1660-486c-b401-21fa23da4bc2 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_ind[[\\\"symbol\\\", \\\"rsi\\\", \\\"ema_20\\\", \\\"macd_diff\\\", \\\"bb_upper\\\", \\\"bb_lower\\\"]]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"symbol\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"eth\",\n          \"bnb\",\n          \"xrp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rsi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"ema_20\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macd_diff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bb_upper\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"bb_lower\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "def compute_indicators(df):\n",
        "    df[\"close\"] = df[\"current_price\"]\n",
        "    df[\"rsi\"] = ta.momentum.RSIIndicator(df[\"close\"]).rsi()\n",
        "    df[\"ema_20\"] = ta.trend.EMAIndicator(df[\"close\"], window=20).ema_indicator()\n",
        "    df[\"macd_diff\"] = ta.trend.MACD(df[\"close\"]).macd_diff()\n",
        "    bb = ta.volatility.BollingerBands(df[\"close\"], window=20)\n",
        "    df[\"bb_upper\"] = bb.bollinger_hband()\n",
        "    df[\"bb_lower\"] = bb.bollinger_lband()\n",
        "    return df\n",
        "\n",
        "df_ind = compute_indicators(df_top10)\n",
        "df_ind.to_csv(f\"{DATA_DIR}/top10_with_indicators.csv\", index=False)\n",
        "df_ind[[\"symbol\", \"rsi\", \"ema_20\", \"macd_diff\", \"bb_upper\", \"bb_lower\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H0uPAg5BqRO5"
      },
      "source": [
        "## 📎 Gemini Prompt Example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AN73jKHQqRO5"
      },
      "source": [
        "Paste this in Gemini:\n",
        "\n",
        "> Here's my function that fetches top 10 coin data and adds RSI, MACD, EMA, and Bollinger Bands. Can you suggest optimizations or extra indicators for high-volatility detection?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PKeRb76jqRO5"
      },
      "source": [
        "## 📈 Signal Finder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "VEdWOst3qRO5",
        "outputId": "3c008f73-afa0-40fb-c82c-79ea6db7976e"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "  symbol  rsi  macd_diff signal\n",
              "0    btc  NaN        NaN   HOLD\n",
              "1    eth  NaN        NaN   HOLD\n",
              "2    xrp  NaN        NaN   HOLD\n",
              "3   usdt  NaN        NaN   HOLD\n",
              "4    bnb  NaN        NaN   HOLD"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-38b36442-77ac-461f-9e1c-761fbc3f5d3f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>symbol</th>\n",
              "      <th>rsi</th>\n",
              "      <th>macd_diff</th>\n",
              "      <th>signal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>btc</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HOLD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>eth</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HOLD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>xrp</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HOLD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>usdt</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HOLD</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>bnb</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>HOLD</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-38b36442-77ac-461f-9e1c-761fbc3f5d3f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-38b36442-77ac-461f-9e1c-761fbc3f5d3f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-38b36442-77ac-461f-9e1c-761fbc3f5d3f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-0484d17a-2ea7-4fe6-adcc-748a569923dc\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-0484d17a-2ea7-4fe6-adcc-748a569923dc')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-0484d17a-2ea7-4fe6-adcc-748a569923dc button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"df_signals[[\\\"symbol\\\", \\\"rsi\\\", \\\"macd_diff\\\", \\\"signal\\\"]]\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"symbol\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 5,\n        \"samples\": [\n          \"eth\",\n          \"bnb\",\n          \"xrp\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"rsi\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"macd_diff\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": null,\n        \"max\": null,\n        \"num_unique_values\": 0,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"signal\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 1,\n        \"samples\": [],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "def generate_signals(df):\n",
        "    signals = []\n",
        "    for _, row in df.iterrows():\n",
        "        sig = []\n",
        "        if row[\"rsi\"] < 30:\n",
        "            sig.append(\"BUY_RSI\")\n",
        "        if row[\"rsi\"] > 70:\n",
        "            sig.append(\"SELL_RSI\")\n",
        "        if row[\"current_price\"] < row[\"bb_lower\"]:\n",
        "            sig.append(\"BREAKOUT_WATCH\")\n",
        "        signals.append(\", \".join(sig) if sig else \"HOLD\")\n",
        "    df[\"signal\"] = signals\n",
        "    return df\n",
        "\n",
        "df_signals = generate_signals(df_ind)\n",
        "df_signals.to_csv(f\"{DATA_DIR}/signals.csv\", index=False)\n",
        "df_signals[[\"symbol\", \"rsi\", \"macd_diff\", \"signal\"]].head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PSHrWNIBqRO6"
      },
      "source": [
        "## 🧠 ML Logger"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "9yOsxFrNqRO6"
      },
      "outputs": [],
      "source": [
        "# Append features to ML training log\n",
        "def log_ml_features(df, path):\n",
        "    features = df[[\n",
        "        \"id\", \"timestamp\", \"current_price\",\n",
        "        \"rsi\", \"ema_20\", \"macd_diff\",\n",
        "        \"bb_upper\", \"bb_lower\", \"signal\"\n",
        "    ]]\n",
        "    if os.path.exists(path):\n",
        "        df_old = pd.read_csv(path)\n",
        "        df_new = pd.concat([df_old, features]).drop_duplicates()\n",
        "    else:\n",
        "        df_new = features\n",
        "    df_new.to_csv(path, index=False)\n",
        "\n",
        "log_ml_features(df_signals, f\"{DATA_DIR}/ml_training.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0f8d714d",
        "outputId": "0a1f4cfc-ea21-45a5-ffc0-7915d5c2d727"
      },
      "source": [
        "# Execute the main script\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Signals generated and saved for top 10 snapshot data.\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Historical data fetched.\n",
            "Warning: 'prices' key not found in historical data for bitcoin.\n",
            "\n",
            "--- Results ---\n",
            "Historical signals (if generated):\n",
            "No historical signals generated.\n",
            "\n",
            "Top 10 snapshot signals:\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07df7dd3"
      },
      "source": [
        "# Modify the signals/generate_signals.py file to return a DataFrame, removing the shell command.\n",
        "with open('signal_bot/signals/generate_signals.py', 'w') as f:\n",
        "    f.write(\"\"\"# signals/generate_signals.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "\n",
        "def generate_signal(df):\n",
        "    \\\"\\\"\\\"\n",
        "    Generates trading signals based on technical indicators.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with technical indicators.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'signal' column added.\n",
        "    \\\"\\\"\\\"\n",
        "    # Ensure the DataFrame is not empty and has required columns\n",
        "    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']\n",
        "    if df.empty or not all(col in df.columns for col in required_cols):\n",
        "        df['signal'] = 'HOLD'\n",
        "        return df[['signal']]\n",
        "\n",
        "    # Initialize signal column\n",
        "    df['signal'] = 'HOLD'\n",
        "\n",
        "    # Apply RSI signals\n",
        "    df.loc[df['rsi'] < 30, 'signal'] = df.loc[df['rsi'] < 30, 'signal'].apply(lambda x: (x + ', BUY_RSI').strip(', ') if x != 'HOLD' else 'BUY_RSI')\n",
        "    df.loc[df['rsi'] > 70, 'signal'] = df.loc[df['rsi'] > 70, 'signal'].apply(lambda x: (x + ', SELL_RSI').strip(', ') if x != 'HOLD' else 'SELL_RSI')\n",
        "\n",
        "    # Apply Bollinger Bands signals\n",
        "    df.loc[df['close'] < df['bb_lower'], 'signal'] = df.loc[df['close'] < df['bb_lower'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_LOWER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_LOWER')\n",
        "    df.loc[df['close'] > df['bb_upper'], 'signal'] = df.loc[df['close'] > df['bb_upper'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_UPPER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_UPPER')\n",
        "\n",
        "    # Simplified MACD signal (crossing above/below 0) - requires previous data\n",
        "    if len(df) > 1:\n",
        "        df['macd_diff_prev'] = df['macd_diff'].shift(1)\n",
        "        df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'] = df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'].apply(lambda x: (x + ', BUY_MACD_CROSS').strip(', ') if x != 'HOLD' else 'BUY_MACD_CROSS')\n",
        "        df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'] = df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'].apply(lambda x: (x + ', SELL_MACD_CROSS').strip(', ') if x != 'HOLD' else 'SELL_MACD_CROSS')\n",
        "        df = df.drop(columns=['macd_diff_prev'])\n",
        "\n",
        "\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame\n",
        "\"\"\"\n",
        ")"
      ],
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zftUJ1gQtfTl",
        "outputId": "3231e506-684a-4af9-9935-c24c22cfca18"
      },
      "source": [
        "# Modify the signals/generate_signals.py file to return a DataFrame with corrected syntax.\n",
        "with open('signal_bot/signals/generate_signals.py', 'w') as f:\n",
        "    f.write(\"\"\"# signals/generate_signals.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "\n",
        "def generate_signal(df):\n",
        "    \\\"\\\"\\\"\n",
        "    Generates trading signals based on technical indicators.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with technical indicators.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'signal' column added.\n",
        "    \\\"\\\"\\\"\n",
        "    # Ensure the DataFrame is not empty and has required columns\n",
        "    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']\n",
        "    if df.empty or not all(col in df.columns for col in required_cols):\n",
        "        df['signal'] = 'HOLD'\n",
        "        return df[['signal']]\n",
        "\n",
        "    # Initialize signal column\n",
        "    df['signal'] = 'HOLD'\n",
        "\n",
        "    # Apply RSI signals\n",
        "    df.loc[df['rsi'] < 30, 'signal'] = df.loc[df['rsi'] < 30, 'signal'].apply(lambda x: (x + ', BUY_RSI').strip(', ') if x != 'HOLD' else 'BUY_RSI')\n",
        "    df.loc[df['rsi'] > 70, 'signal'] = df.loc[df['rsi'] > 70, 'signal'].apply(lambda x: (x + ', SELL_RSI').strip(', ') if x != 'HOLD' else 'SELL_RSI')\n",
        "\n",
        "    # Apply Bollinger Bands signals\n",
        "    df.loc[df['close'] < df['bb_lower'], 'signal'] = df.loc[df['close'] < df['bb_lower'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_LOWER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_LOWER')\n",
        "    df.loc[df['close'] > df['bb_upper'], 'signal'] = df.loc[df['close'] > df['bb_upper'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_UPPER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_UPPER')\n",
        "\n",
        "    # Simplified MACD signal (crossing above/below 0) - requires previous data\n",
        "    if len(df) > 1:\n",
        "        df['macd_diff_prev'] = df['macd_diff'].shift(1)\n",
        "        df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'] = df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'].apply(lambda x: (x + ', BUY_MACD_CROSS').strip(', ') if x != 'HOLD' else 'BUY_MACD_CROSS')\n",
        "        df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'] = df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'].apply(lambda x: (x + ', SELL_MACD_CROSS').strip(', ') if x != 'HOLD' else 'SELL_MACD_CROSS')\n",
        "        df = df.drop(columns=['macd_diff_prev'])\n",
        "\n",
        "\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame\n",
        "\n",
        "# Execute the main script again to test the fix\n",
        "!python signal_bot/main.py\n",
        "\"\"\"\n",
        ")\n",
        "\n",
        "# Execute the main script\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/signal_bot/main.py\", line 6, in <module>\n",
            "    from signals.generate_signals import generate_signal\n",
            "  File \"/content/signal_bot/signals/generate_signals.py\", line 43\n",
            "    !python signal_bot/main.py\n",
            "    ^\n",
            "SyntaxError: invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82ffdbce"
      },
      "source": [
        "# Modify the signals/generate_signals.py file to return a DataFrame, removing the shell command.\n",
        "with open('signal_bot/signals/generate_signals.py', 'w') as f:\n",
        "    f.write(\"\"\"# signals/generate_signals.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "\n",
        "def generate_signal(df):\n",
        "    \\\"\\\"\\\"\n",
        "    Generates trading signals based on technical indicators.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with technical indicators.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'signal' column added.\n",
        "    \\\"\\\"\\\"\n",
        "    # Ensure the DataFrame is not empty and has required columns\n",
        "    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']\n",
        "    if df.empty or not all(col in df.columns for col in required_cols):\n",
        "        df['signal'] = 'HOLD'\n",
        "        return df[['signal']]\n",
        "\n",
        "    # Initialize signal column\n",
        "    df['signal'] = 'HOLD'\n",
        "\n",
        "    # Apply RSI signals\n",
        "    df.loc[df['rsi'] < 30, 'signal'] = df.loc[df['rsi'] < 30, 'signal'].apply(lambda x: (x + ', BUY_RSI').strip(', ') if x != 'HOLD' else 'BUY_RSI')\n",
        "    df.loc[df['rsi'] > 70, 'signal'] = df.loc[df['rsi'] > 70, 'signal'].apply(lambda x: (x + ', SELL_RSI').strip(', ') if x != 'HOLD' else 'SELL_RSI')\n",
        "\n",
        "    # Apply Bollinger Bands signals\n",
        "    df.loc[df['close'] < df['bb_lower'], 'signal'] = df.loc[df['close'] < df['bb_lower'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_LOWER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_LOWER')\n",
        "    df.loc[df['close'] > df['bb_upper'], 'signal'] = df.loc[df['close'] > df['bb_upper'], 'signal'].apply(lambda x: (x + ', BREAKOUT_WATCH_UPPER').strip(', ') if x != 'HOLD' else 'BREAKOUT_WATCH_UPPER')\n",
        "\n",
        "    # Simplified MACD signal (crossing above/below 0) - requires previous data\n",
        "    if len(df) > 1:\n",
        "        df['macd_diff_prev'] = df['macd_diff'].shift(1)\n",
        "        df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'] = df.loc[(df['macd_diff'] > 0) & (df['macd_diff_prev'] <= 0), 'signal'].apply(lambda x: (x + ', BUY_MACD_CROSS').strip(', ') if x != 'HOLD' else 'BUY_MACD_CROSS')\n",
        "        df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'] = df.loc[(df['macd_diff'] < 0) & (df['macd_diff_prev'] >= 0), 'signal'].apply(lambda x: (x + ', SELL_MACD_CROSS').strip(', ') if x != 'HOLD' else 'SELL_MACD_CROSS')\n",
        "        df = df.drop(columns=['macd_diff_prev'])\n",
        "\n",
        "\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame\n",
        "\"\"\")\n",
        "\n",
        "# Execute the main script again to test the fix\n",
        "# Removed: !python signal_bot/main.py"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "934bcaf3",
        "outputId": "739e8f77-42e1-4e4c-f010-cb3454e9ab2c"
      },
      "source": [
        "# Run the Streamlit dashboard\n",
        "!streamlit run signal_bot/dashboard.py"
      ],
      "execution_count": 124,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: streamlit: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "142b7283",
        "outputId": "87b6e9e3-0409-44a1-f0e6-1ab4fe9ff6bd"
      },
      "source": [
        "# List files in the data directory to verify outputs\n",
        "import os\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "\n",
        "if os.path.exists(DATA_DIR):\n",
        "    print(f\"Files in {DATA_DIR}:\")\n",
        "    for filename in os.listdir(DATA_DIR):\n",
        "        print(filename)\n",
        "else:\n",
        "    print(f\"Error: Data directory not found at {DATA_DIR}.\")"
      ],
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files in signal_bot/data:\n",
            "top10_with_indicators.csv\n",
            "bitcoin_historical_with_indicators.csv\n",
            "top10_market_data.csv\n",
            "signals.csv\n",
            "bitcoin_historical_signals.csv\n",
            "top10_signals.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "699dc5c6",
        "outputId": "f9a55294-5ac3-4722-ae8e-0362120f98ed"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 188\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 188)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ea5c8f48",
        "outputId": "1d63557a-60c7-4360-c1c9-661b48bb8757"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 188\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 188)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "301454b8"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the dashboard.py file\n",
        "with open('signal_bot/dashboard.py', 'w') as f:\n",
        "    f.write('''# dashboard.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os # Import os for path handling\n",
        "\n",
        "# Define DATA_DIR\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "\n",
        "st.title(\"Crypto Signal Dashboard\")\n",
        "\n",
        "tab1, tab2, tab3 = st.tabs([\"Signals\", \"Anomalies\", \"Indicators\"])\n",
        "\n",
        "# Ensure files exist before trying to read\n",
        "signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\") # Use top10_signals.csv\n",
        "anomalies_dir = DATA_DIR # Anomalies are saved with timestamp, need to find the latest\n",
        "indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\") # Use top10_with_indicators.csv\n",
        "\n",
        "with tab1:\n",
        "    st.subheader(\"Signal Feed (Top 10)\")\n",
        "    if os.path.exists(signals_path):\n",
        "        try:\n",
        "            signals = pd.read_csv(signals_path)\n",
        "            st.dataframe(signals)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading signals data: {e}\")\n",
        "    else:\n",
        "        st.info(f\"Signal data not found at {signals_path}. Run the main pipeline first.\")\n",
        "\n",
        "\n",
        "with tab2:\n",
        "    st.subheader(\"Anomalies\")\n",
        "    # Find the latest anomaly file\n",
        "    anomaly_files = [f for f in os.listdir(anomalies_dir) if f.startswith('anomalies_') and f.endswith('.csv')]\n",
        "    anomaly_files.sort(reverse=True) # Sort to get the latest first\n",
        "\n",
        "    if anomaly_files:\n",
        "        latest_anomaly_file = os.path.join(anomalies_dir, anomaly_files[0])\n",
        "        st.info(f\"Displaying latest anomalies from: {latest_anomaly_file}\")\n",
        "        try:\n",
        "            anomalies = pd.read_csv(latest_anomaly_file)\n",
        "            st.dataframe(anomalies)\n",
        "        except Exception as e:\n",
        "             st.error(f\"Error loading anomalies data from {latest_anomaly_file}: {e}\")\n",
        "    else:\n",
        "        st.info(f\"No anomaly files found in {anomalies_dir}. Run the main pipeline with anomaly detection enabled.\")\n",
        "\n",
        "\n",
        "with tab3:\n",
        "    st.subheader(\"Top 10 Indicators\")\n",
        "    if os.path.exists(indicators_path):\n",
        "        try:\n",
        "            indicators = pd.read_csv(indicators_path)\n",
        "            # Ensure required columns exist and are numeric before charting\n",
        "            chart_cols = [\"rsi\", \"macd_diff\"]\n",
        "            available_chart_cols = [col for col in chart_cols if col in indicators.columns and pd.api.types.is_numeric_dtype(indicators[col])]\n",
        "\n",
        "            if available_chart_cols:\n",
        "                 st.subheader(\"Indicator Trends (RSI, MACD Diff)\")\n",
        "                 # Check if there's enough data or if all values are NaN for charting\n",
        "                 if not indicators[available_chart_cols].dropna().empty:\n",
        "                      st.line_chart(indicators[available_chart_cols])\n",
        "                 else:\n",
        "                      st.info(f\"Indicator data for charting ({available_chart_cols}) contains only NaN values or is empty. Cannot display chart.\")\n",
        "            elif chart_cols:\n",
        "                 st.warning(f\"Required numeric columns for charting not found or are not numeric: {chart_cols}\")\n",
        "            else:\n",
        "                 st.info(\"No indicator data available for charting.\")\n",
        "\n",
        "            # Display the full indicator data\n",
        "            st.subheader(\"Full Indicator Data\")\n",
        "            st.dataframe(indicators)\n",
        "\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading indicators data: {e}\")\n",
        "    else:\n",
        "        st.info(f\"Indicator data not found at {indicators_path}. Run the main pipeline first.\")\n",
        "\n",
        "# Instructions on how to run the dashboard\n",
        "st.sidebar.subheader(\"How to run the dashboard:\")\n",
        "st.sidebar.markdown(\"1. Ensure you have Streamlit installed (`pip install streamlit`).\")\n",
        "st.sidebar.markdown(\"2. Run the main pipeline (`python -m signal_bot.main`) to generate data files.\")\n",
        "st.sidebar.markdown(\"3. Open a terminal in your project's root directory (`/content/` in Colab).\")\n",
        "st.sidebar.markdown(\"4. Run the command: `streamlit run signal_bot/dashboard.py`\")\n",
        "st.sidebar.markdown(\"5. A local URL will be provided to view the dashboard.\")\n",
        "\n",
        "''')"
      ],
      "execution_count": 129,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fc2bf6f3",
        "outputId": "6f89b26c-e28d-4545-a7ca-bbc28a8d2a4a"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the backtest results file\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "backtest_results_path = os.path.join(DATA_DIR, \"signal_backtest.csv\")\n",
        "\n",
        "# Check if the file exists before attempting to read\n",
        "if os.path.exists(backtest_results_path):\n",
        "    try:\n",
        "        # Read the backtest results file\n",
        "        backtest_results_df = pd.read_csv(backtest_results_path)\n",
        "\n",
        "        # Display the head and some basic analysis of the results\n",
        "        print(f\"Content of {backtest_results_path}:\")\n",
        "        display(backtest_results_df.head().to_markdown(index=False))\n",
        "\n",
        "        print(\"\\nBasic Backtest Analysis:\")\n",
        "        total_signals = len(backtest_results_df)\n",
        "        successful_trades = backtest_results_df['success'].sum()\n",
        "        success_rate = (successful_trades / total_signals) * 100 if total_signals > 0 else 0\n",
        "\n",
        "        print(f\"Total BUY signals backtested: {total_signals}\")\n",
        "        print(f\"Successful BUY signals (price increased by >= threshold): {successful_trades}\")\n",
        "        print(f\"Success Rate: {success_rate:.2f}%\")\n",
        "        print(f\"Average Return (%): {backtest_results_df['return_pct'].mean() * 100:.2f}%\")\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {backtest_results_path} not found. Please run main.py first to generate this file.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or processing {backtest_results_path}: {e}\")\n",
        "else:\n",
        "    print(f\"Error: {backtest_results_path} not found. Please run main.py first to generate this file.\")"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: signal_bot/data/signal_backtest.csv not found. Please run main.py first to generate this file.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "43204302",
        "outputId": "ce122646-2f79-4a0b-ead3-2a3f57becda1"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 179\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 179)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ebe1e3b",
        "outputId": "c2f32edc-e4ce-4ae9-e713-60f5c062ae0a"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 144\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 144)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b46c9097",
        "outputId": "46b15cd5-b621-4a37-c2b5-5b40701e6aec"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 113,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 153\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a5d7bccd"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "os.makedirs('signal_bot/data', exist_ok=True) # Ensure data directory exists\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/data_collector.py', 'w') as f:\n",
        "    f.write('''# signal_bot/data_collector.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins # Import get_top_coins\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def collect_data(limit=250):\n",
        "    \"\"\"\n",
        "    Fetches a snapshot of market data for a specified number of top coins.\n",
        "\n",
        "    Args:\n",
        "        limit (int): The number of top coins to fetch data for.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame or None: DataFrame with market data if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    print(f\"Fetching market data for top {limit} coins...\")\n",
        "    try:\n",
        "        data = get_top_coins(limit=limit)\n",
        "        if data is not None:\n",
        "            df = pd.DataFrame(data)\n",
        "            df[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "            df.to_csv(full_snapshot_path, index=False)\n",
        "            print(f\"Full market snapshot fetched and saved to {full_snapshot_path}.\")\n",
        "            return df\n",
        "        else:\n",
        "            print(\"Failed to fetch market data for full snapshot.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during market data collection: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example of how to run the data collector as a script\n",
        "    collect_data(limit=250) # Fetch data for top 250 coins for the snapshot\n",
        "''')"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e1bbf2d",
        "outputId": "d5e9f05a-e616-4b46-ff5c-a6e8d1de7105"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 111,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 153\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1c5c0d94"
      },
      "source": [
        "# Write the content of signal_bot/main.py piece by piece to avoid string literal issues.\n",
        "main_py_content_part1 = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from .indicators.ta_utils import compute_indicators\n",
        "from .signals.signal_finder import find_signals\n",
        "from .anomaly_detector import detect_anomalies\n",
        "from .ml_logger import log_ml_features\n",
        "from .backtester import backtest_signals\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Starting bot pipeline...\")\n",
        "\n",
        "    # --- Data Collection (Top 10 Snapshot) ---\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data_top10 = get_top_coins(limit=10)\n",
        "        if data_top10 is not None:\n",
        "            df_top10 = pd.DataFrame(data_top10)\n",
        "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            top10_market_data_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "            df_top10.to_csv(top10_market_data_path, index=False)\n",
        "            print(f\"Top 10 data fetched and saved to {top10_market_data_path}.\")\n",
        "\n",
        "            # --- Indicator Computation for Top 10 ---\n",
        "            print(\"Computing technical indicators for top 10 data...\")\n",
        "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "            top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "            df_ind_top10.to_csv(top10_indicators_path, index=False)\n",
        "            print(f\"Indicators computed and saved for top 10 data to {top10_indicators_path}.\")\n",
        "\n",
        "            # --- Signal Generation for Top 10 ---\n",
        "            print(\"Generating signals for top 10 snapshot data...\")\n",
        "            df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "            top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "            df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "            print(f\"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.\")\n",
        "\n",
        "            # --- ML Logging for Top 10 ---\n",
        "            print(\"Logging ML features for top 10 data...\")\n",
        "            ml_log_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "            log_ml_features(top10_indicators_path, ml_log_path)\n",
        "            print(f\"ML features logged to {ml_log_path}.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.\")\n",
        "            df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.\")\n",
        "        df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---\n",
        "    print(\"\\\\nFetching full market snapshot for Anomaly Detection...\")\n",
        "    try:\n",
        "        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot\n",
        "        if data_full is not None:\n",
        "            df_full_snapshot = pd.DataFrame(data_full)\n",
        "            df_full_snapshot[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "            df_full_snapshot.to_csv(full_snapshot_path, index=False)\n",
        "            print(f\"Full market snapshot fetched and saved to {full_snapshot_path}.\")\n",
        "        else:\n",
        "            print(\"Failed to fetch full market snapshot.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or saving full market snapshot: {e}.\")\n",
        "\n",
        "\n",
        "    # --- Anomaly Detection ---\n",
        "    print(\"\\\\nAttempting to run Anomaly Detection...\")\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if os.path.exists(full_snapshot_path):\n",
        "        try:\n",
        "            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "            print(f\"Anomaly detection completed. Anomalies saved to {path}.\")\n",
        "            print(f\"Found {len(anomalies)} anomalies.\")\n",
        "            if not anomalies.empty:\n",
        "                 print(\"Sample anomalies:\")\n",
        "                 print(anomalies.head().to_markdown(index=False))\n",
        "            else:\n",
        "                 print(\"No anomalies detected.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Anomaly Detection: {e}.\")\n",
        "    else:\n",
        "        print(f\"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.\")\n",
        "\n",
        "    # --- Historical Data Processing ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals_display = pd.DataFrame()\n",
        "    historical_price_data_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_price.csv\") # Define path for historical price data\n",
        "    historical_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\") # Define path for historical signals\n",
        "\n",
        "    print(f\"\\\\nAttempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "        if history_data is not None and \"prices\" in history_data:\n",
        "            print(\"Historical data fetched.\")\n",
        "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"current_price\"]) # Use current_price for consistency\n",
        "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data\n",
        "            print(\"Historical data processed and saved.\")\n",
        "\n",
        "            print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "            history_df[\"close\"] = history_df[\"current_price\"] # Use current_price as close for indicators\n",
        "            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly\n",
        "            history_historical_indicators_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_with_indicators.csv\")\n",
        "            history_df_ind.to_csv(history_historical_indicators_path, index=False)\n",
        "            print(f\"Indicators computed and saved for historical data to {history_historical_indicators_path}.\")\n",
        "\n",
        "            print(f\"Generating signals for {coin_id} historical data...\")\n",
        "            history_df_signals = find_signals(history_df_ind.copy())\n",
        "            history_df_signals.to_csv(historical_signals_path, index=False)\n",
        "            print(f\"Historical signals generated and saved to {historical_signals_path}.\")\n",
        "            history_df_signals_display = history_df_signals\n",
        "\n",
        "            # --- Backtesting Historical Signals ---\n",
        "            print(f\"\\\\nAttempting to backtest historical signals for {coin_id}...\")\n",
        "            # backtest_signals expects paths to signal CSV and price CSV\n",
        "            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)\n",
        "            print(\"Backtesting completed.\")\n",
        "            if not backtest_results_df.empty:\n",
        "                print(\"Backtest Results (head):\")\n",
        "                print(backtest_results_df.head().to_markdown(index=False))\n",
        "            else:\n",
        "                print(\"No backtest results to display.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "             print(\"Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.\")\n",
        "\n",
        "\n",
        "    # --- Display Results (for Top 10 snapshot) ---\n",
        "    print(\"\\\\n--- Results (Top 10 Snapshot) ---\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False))\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated due to processing errors or no data.\")\n",
        "\n",
        "    # --- Display Results (for Historical, if processed) ---\n",
        "    print(\"\\\\n--- Results (Historical Data) ---\")\n",
        "    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:\n",
        "         print(\"Historical signals (tail):\")\n",
        "         print(history_df_signals_display.tail().to_markdown(index=False))\n",
        "         print(\"\\nHistorical signals (sample):\")\n",
        "         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))\n",
        "    else:\n",
        "         print(\"No historical signals generated or historical data processing skipped.\")\n",
        "\n",
        "    # Ensure the final print statement is complete\n",
        "    print(\"\\\\nBot pipeline finished.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)"
      ],
      "execution_count": 110,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39bbfd43",
        "outputId": "144daab5-3ba0-4134-8f04-4b6faeda71ff"
      },
      "source": [
        "# Execute the main script to test the fix and verify full snapshot fetching.\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 153\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "490dbf47"
      },
      "source": [
        "# Rewrite the content of signal_bot/main.py again, double-checking all string literals.\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from .indicators.ta_utils import compute_indicators\n",
        "from .signals.signal_finder import find_signals\n",
        "from .anomaly_detector import detect_anomalies\n",
        "from .ml_logger import log_ml_features\n",
        "from .backtester import backtest_signals\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Starting bot pipeline...\")\n",
        "\n",
        "    # --- Data Collection (Top 10 Snapshot) ---\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data_top10 = get_top_coins(limit=10)\n",
        "        if data_top10 is not None:\n",
        "            df_top10 = pd.DataFrame(data_top10)\n",
        "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            top10_market_data_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "            df_top10.to_csv(top10_market_data_path, index=False)\n",
        "            print(f\"Top 10 data fetched and saved to {top10_market_data_path}.\")\n",
        "\n",
        "            # --- Indicator Computation for Top 10 ---\n",
        "            print(\"Computing technical indicators for top 10 data...\")\n",
        "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "            top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "            df_ind_top10.to_csv(top10_indicators_path, index=False)\n",
        "            print(f\"Indicators computed and saved for top 10 data to {top10_indicators_path}.\")\n",
        "\n",
        "            # --- Signal Generation for Top 10 ---\n",
        "            print(\"Generating signals for top 10 snapshot data...\")\n",
        "            df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "            top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "            df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "            print(f\"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.\")\n",
        "\n",
        "            # --- ML Logging for Top 10 ---\n",
        "            print(\"Logging ML features for top 10 data...\")\n",
        "            ml_log_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "            log_ml_features(top10_indicators_path, ml_log_path)\n",
        "            print(f\"ML features logged to {ml_log_path}.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.\")\n",
        "            df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.\")\n",
        "        df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---\n",
        "    print(\"\\\\nFetching full market snapshot for Anomaly Detection...\")\n",
        "    try:\n",
        "        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot\n",
        "        if data_full is not None:\n",
        "            df_full_snapshot = pd.DataFrame(data_full)\n",
        "            df_full_snapshot[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "            df_full_snapshot.to_csv(full_snapshot_path, index=False)\n",
        "            print(f\"Full market snapshot fetched and saved to {full_snapshot_path}.\")\n",
        "        else:\n",
        "            print(\"Failed to fetch full market snapshot.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or saving full market snapshot: {e}.\")\n",
        "\n",
        "\n",
        "    # --- Anomaly Detection ---\n",
        "    print(\"\\\\nAttempting to run Anomaly Detection...\")\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if os.path.exists(full_snapshot_path):\n",
        "        try:\n",
        "            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "            print(f\"Anomaly detection completed. Anomalies saved to {path}.\")\n",
        "            print(f\"Found {len(anomalies)} anomalies.\")\n",
        "            if not anomalies.empty:\n",
        "                 print(\"Sample anomalies:\")\n",
        "                 print(anomalies.head().to_markdown(index=False))\n",
        "            else:\n",
        "                 print(\"No anomalies detected.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Anomaly Detection: {e}.\")\n",
        "    else:\n",
        "        print(f\"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.\")\n",
        "\n",
        "    # --- Historical Data Processing ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals_display = pd.DataFrame()\n",
        "    historical_price_data_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_price.csv\") # Define path for historical price data\n",
        "    historical_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\") # Define path for historical signals\n",
        "\n",
        "    print(f\"\\\\nAttempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "        if history_data is not None and \"prices\" in history_data:\n",
        "            print(\"Historical data fetched.\")\n",
        "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"current_price\"]) # Use current_price for consistency\n",
        "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data\n",
        "            print(\"Historical data processed and saved.\")\n",
        "\n",
        "            print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "            history_df[\"close\"] = history_df[\"current_price\"] # Use current_price as close for indicators\n",
        "            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly\n",
        "            history_historical_indicators_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_with_indicators.csv\")\n",
        "            history_df_ind.to_csv(history_historical_indicators_path, index=False)\n",
        "            print(f\"Indicators computed and saved for historical data to {history_historical_indicators_path}.\")\n",
        "\n",
        "            print(f\"Generating signals for {coin_id} historical data...\")\n",
        "            history_df_signals = find_signals(history_df_ind.copy())\n",
        "            history_df_signals.to_csv(historical_signals_path, index=False)\n",
        "            print(f\"Historical signals generated and saved to {historical_signals_path}.\")\n",
        "            history_df_signals_display = history_df_signals\n",
        "\n",
        "            # --- Backtesting Historical Signals ---\n",
        "            print(f\"\\\\nAttempting to backtest historical signals for {coin_id}...\")\n",
        "            # backtest_signals expects paths to signal CSV and price CSV\n",
        "            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)\n",
        "            print(\"Backtesting completed.\")\n",
        "            if not backtest_results_df.empty:\n",
        "                print(\"Backtest Results (head):\")\n",
        "                print(backtest_results_df.head().to_markdown(index=False))\n",
        "            else:\n",
        "                print(\"No backtest results to display.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "             print(\"Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.\")\n",
        "\n",
        "\n",
        "    # --- Display Results (for Top 10 snapshot) ---\n",
        "    print(\"\\\\n--- Results (Top 10 Snapshot) ---\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False))\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated due to processing errors or no data.\")\n",
        "\n",
        "    # --- Display Results (for Historical, if processed) ---\n",
        "    print(\"\\\\n--- Results (Historical Data) ---\")\n",
        "    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:\n",
        "         print(\"Historical signals (tail):\")\n",
        "         print(history_df_signals_display.tail().to_markdown(index=False))\n",
        "         print(\"\\nHistorical signals (sample):\")\n",
        "         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))\n",
        "    else:\n",
        "         print(\"No historical signals generated or historical data processing skipped.\")\n",
        "\n",
        "\n",
        "    print(\"\\\\nBot pipeline finished.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)"
      ],
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81071973",
        "outputId": "3636da44-1088-49c8-c662-30cb8d777ddf"
      },
      "source": [
        "# Execute the main script to test the fix and verify full snapshot fetching.\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 153\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "038d2079"
      },
      "source": [
        "# Rewrite the content of signal_bot/main.py with the complete and correct string literal.\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from .indicators.ta_utils import compute_indicators\n",
        "from .signals.signal_finder import find_signals\n",
        "from .anomaly_detector import detect_anomalies\n",
        "from .ml_logger import log_ml_features\n",
        "from .backtester import backtest_signals\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Starting bot pipeline...\")\n",
        "\n",
        "    # --- Data Collection (Top 10 Snapshot) ---\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data_top10 = get_top_coins(limit=10)\n",
        "        if data_top10 is not None:\n",
        "            df_top10 = pd.DataFrame(data_top10)\n",
        "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            top10_market_data_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "            df_top10.to_csv(top10_market_data_path, index=False)\n",
        "            print(f\"Top 10 data fetched and saved to {top10_market_data_path}.\")\n",
        "\n",
        "            # --- Indicator Computation for Top 10 ---\n",
        "            print(\"Computing technical indicators for top 10 data...\")\n",
        "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "            top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "            df_ind_top10.to_csv(top10_indicators_path, index=False)\n",
        "            print(f\"Indicators computed and saved for top 10 data to {top10_indicators_path}.\")\n",
        "\n",
        "            # --- Signal Generation for Top 10 ---\n",
        "            print(\"Generating signals for top 10 snapshot data...\")\n",
        "            df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "            top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "            df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "            print(f\"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.\")\n",
        "\n",
        "            # --- ML Logging for Top 10 ---\n",
        "            print(\"Logging ML features for top 10 data...\")\n",
        "            ml_log_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "            log_ml_features(top10_indicators_path, ml_log_path)\n",
        "            print(f\"ML features logged to {ml_log_path}.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.\")\n",
        "            df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.\")\n",
        "        df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---\n",
        "    print(\"\\\\nFetching full market snapshot for Anomaly Detection...\")\n",
        "    try:\n",
        "        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot\n",
        "        if data_full is not None:\n",
        "            df_full_snapshot = pd.DataFrame(data_full)\n",
        "            df_full_snapshot[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "            df_full_snapshot.to_csv(full_snapshot_path, index=False)\n",
        "            print(f\"Full market snapshot fetched and saved to {full_snapshot_path}.\")\n",
        "        else:\n",
        "            print(\"Failed to fetch full market snapshot.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or saving full market snapshot: {e}.\")\n",
        "\n",
        "\n",
        "    # --- Anomaly Detection ---\n",
        "    print(\"\\\\nAttempting to run Anomaly Detection...\")\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if os.path.exists(full_snapshot_path):\n",
        "        try:\n",
        "            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "            print(f\"Anomaly detection completed. Anomalies saved to {path}.\")\n",
        "            print(f\"Found {len(anomalies)} anomalies.\")\n",
        "            if not anomalies.empty:\n",
        "                 print(\"Sample anomalies:\")\n",
        "                 print(anomalies.head().to_markdown(index=False))\n",
        "            else:\n",
        "                 print(\"No anomalies detected.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Anomaly Detection: {e}.\")\n",
        "    else:\n",
        "        print(f\"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.\")\n",
        "\n",
        "    # --- Historical Data Processing ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals_display = pd.DataFrame()\n",
        "    historical_price_data_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_price.csv\") # Define path for historical price data\n",
        "    historical_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\") # Define path for historical signals\n",
        "\n",
        "    print(f\"\\\\nAttempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "        if history_data is not None and \"prices\" in history_data:\n",
        "            print(\"Historical data fetched.\")\n",
        "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"current_price\"]) # Use current_price for consistency\n",
        "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data\n",
        "            print(\"Historical data processed and saved.\")\n",
        "\n",
        "            print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "            history_df[\"close\"] = history_df[\"current_price\"] # Use current_price as close for indicators\n",
        "            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly\n",
        "            history_historical_indicators_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_with_indicators.csv\")\n",
        "            history_df_ind.to_csv(history_historical_indicators_path, index=False)\n",
        "            print(f\"Indicators computed and saved for historical data to {history_historical_indicators_path}.\")\n",
        "\n",
        "            print(f\"Generating signals for {coin_id} historical data...\")\n",
        "            history_df_signals = find_signals(history_df_ind.copy())\n",
        "            history_df_signals.to_csv(historical_signals_path, index=False)\n",
        "            print(f\"Historical signals generated and saved to {historical_signals_path}.\")\n",
        "            history_df_signals_display = history_df_signals\n",
        "\n",
        "            # --- Backtesting Historical Signals ---\n",
        "            print(f\"\\\\nAttempting to backtest historical signals for {coin_id}...\")\n",
        "            # backtest_signals expects paths to signal CSV and price CSV\n",
        "            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)\n",
        "            print(\"Backtesting completed.\")\n",
        "            if not backtest_results_df.empty:\n",
        "                print(\"Backtest Results (head):\")\n",
        "                print(backtest_results_df.head().to_markdown(index=False))\n",
        "            else:\n",
        "                print(\"No backtest results to display.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "             print(\"Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.\")\n",
        "\n",
        "\n",
        "    # --- Display Results (for Top 10 snapshot) ---\n",
        "    print(\"\\\\n--- Results (Top 10 Snapshot) ---\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False))\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated due to processing errors or no data.\")\n",
        "\n",
        "    # --- Display Results (for Historical, if processed) ---\n",
        "    print(\"\\\\n--- Results (Historical Data) ---\")\n",
        "    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:\n",
        "         print(\"Historical signals (tail):\")\n",
        "         print(history_df_signals_display.tail().to_markdown(index=False))\n",
        "         print(\"\\nHistorical signals (sample):\")\n",
        "         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))\n",
        "    else:\n",
        "         print(\"No historical signals generated or historical data processing skipped.\")\n",
        "\n",
        "\n",
        "    print(\"\\\\nBot pipeline finished.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "52811a58",
        "outputId": "bc754d7f-2f56-4b67-9741-2ad43210f0cc"
      },
      "source": [
        "# Read the content of main.py to inspect the syntax error.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    main_content = f.read()\n",
        "\n",
        "print(main_content)"
      ],
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# main.py\n",
            "import pandas as pd\n",
            "import os\n",
            "from .utils.coingecko_api import get_top_coins, get_coin_history\n",
            "from .indicators.ta_utils import compute_indicators\n",
            "from .signals.signal_finder import find_signals\n",
            "from .anomaly_detector import detect_anomalies\n",
            "from .ml_logger import log_ml_features\n",
            "from .backtester import backtest_signals\n",
            "\n",
            "DATA_DIR = 'signal_bot/data'\n",
            "os.makedirs(DATA_DIR, exist_ok=True)\n",
            "\n",
            "def main():\n",
            "    print(\"Starting bot pipeline...\")\n",
            "\n",
            "    # --- Data Collection (Top 10 Snapshot) ---\n",
            "    print(\"Fetching top 10 coin data...\")\n",
            "    try:\n",
            "        data_top10 = get_top_coins(limit=10)\n",
            "        if data_top10 is not None:\n",
            "            df_top10 = pd.DataFrame(data_top10)\n",
            "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
            "            top10_market_data_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
            "            df_top10.to_csv(top10_market_data_path, index=False)\n",
            "            print(f\"Top 10 data fetched and saved to {top10_market_data_path}.\")\n",
            "\n",
            "            # --- Indicator Computation for Top 10 ---\n",
            "            print(\"Computing technical indicators for top 10 data...\")\n",
            "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
            "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
            "            top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
            "            df_ind_top10.to_csv(top10_indicators_path, index=False)\n",
            "            print(f\"Indicators computed and saved for top 10 data to {top10_indicators_path}.\")\n",
            "\n",
            "            # --- Signal Generation for Top 10 ---\n",
            "            print(\"Generating signals for top 10 snapshot data...\")\n",
            "            df_signals_top10 = find_signals(df_ind_top10.copy())\n",
            "            top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
            "            df_signals_top10.to_csv(top10_signals_path, index=False)\n",
            "            print(f\"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.\")\n",
            "\n",
            "            # --- ML Logging for Top 10 ---\n",
            "            print(\"Logging ML features for top 10 data...\")\n",
            "            ml_log_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
            "            log_ml_features(top10_indicators_path, ml_log_path)\n",
            "            print(f\"ML features logged to {ml_log_path}.\")\n",
            "\n",
            "        else:\n",
            "            print(\"Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.\")\n",
            "            df_signals_top10 = pd.DataFrame()\n",
            "\n",
            "    except Exception as e:\n",
            "        print(f\"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.\")\n",
            "        df_signals_top10 = pd.DataFrame()\n",
            "\n",
            "    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---\n",
            "    print(\"\\nFetching full market snapshot for Anomaly Detection...\")\n",
            "    try:\n",
            "        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot\n",
            "        if data_full is not None:\n",
            "            df_full_snapshot = pd.DataFrame(data_full)\n",
            "            df_full_snapshot[\"timestamp\"] = pd.Timestamp.utcnow()\n",
            "            full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
            "            df_full_snapshot.to_csv(full_snapshot_path, index=False)\n",
            "            print(f\"Full market snapshot fetched and saved to {full_snapshot_path}.\")\n",
            "        else:\n",
            "            print(\"Failed to fetch full market snapshot.\")\n",
            "    except Exception as e:\n",
            "        print(f\"Error fetching or saving full market snapshot: {e}.\")\n",
            "\n",
            "\n",
            "    # --- Anomaly Detection ---\n",
            "    print(\"\\nAttempting to run Anomaly Detection...\")\n",
            "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
            "    anomalies_output_dir = DATA_DIR\n",
            "\n",
            "    if os.path.exists(full_snapshot_path):\n",
            "        try:\n",
            "            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
            "            print(f\"Anomaly detection completed. Anomalies saved to {path}.\")\n",
            "            print(f\"Found {len(anomalies)} anomalies.\")\n",
            "            if not anomalies.empty:\n",
            "                 print(\"Sample anomalies:\")\n",
            "                 print(anomalies.head().to_markdown(index=False))\n",
            "            else:\n",
            "                 print(\"No anomalies detected.\")\n",
            "        except Exception as e:\n",
            "            print(f\"Error during Anomaly Detection: {e}.\")\n",
            "    else:\n",
            "        print(f\"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.\")\n",
            "\n",
            "    # --- Historical Data Processing ---\n",
            "    coin_id = \"bitcoin\"\n",
            "    history_df_signals_display = pd.DataFrame()\n",
            "    historical_price_data_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_price.csv\") # Define path for historical price data\n",
            "    historical_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\") # Define path for historical signals\n",
            "\n",
            "    print(f\"\\nAttempting to fetch and process historical data for {coin_id}...\")\n",
            "    try:\n",
            "        history_data = get_coin_history(coin_id, days=\"30\")\n",
            "        if history_data is not None and \"prices\" in history_data:\n",
            "            print(\"Historical data fetched.\")\n",
            "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"current_price\"]) # Use current_price for consistency\n",
            "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
            "            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data\n",
            "            print(\"Historical data processed and saved.\")\n",
            "\n",
            "            print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
            "            history_df[\"close\"] = history_df[\"current_price\"] # Use current_price as close for indicators\n",
            "            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly\n",
            "            history_historical_indicators_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_with_indicators.csv\")\n",
            "            history_df_ind.to_csv(history_historical_indicators_path, index=False)\n",
            "            print(f\"Indicators computed and saved for historical data to {history_historical_indicators_path}.\")\n",
            "\n",
            "            print(f\"Generating signals for {coin_id} historical data...\")\n",
            "            history_df_signals = find_signals(history_df_ind.copy())\n",
            "            history_df_signals.to_csv(historical_signals_path, index=False)\n",
            "            print(f\"Historical signals generated and saved to {historical_signals_path}.\")\n",
            "            history_df_signals_display = history_df_signals\n",
            "\n",
            "            # --- Backtesting Historical Signals ---\n",
            "            print(f\"\\nAttempting to backtest historical signals for {coin_id}...\")\n",
            "            # backtest_signals expects paths to signal CSV and price CSV\n",
            "            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)\n",
            "            print(\"Backtesting completed.\")\n",
            "            if not backtest_results_df.empty:\n",
            "                print(\"Backtest Results (head):\")\n",
            "                print(backtest_results_df.head().to_markdown(index=False))\n",
            "            else:\n",
            "                print(\"No backtest results to display.\")\n",
            "\n",
            "\n",
            "        else:\n",
            "             print(\"Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.\")\n",
            "\n",
            "    except Exception as e:\n",
            "        print(f\"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.\")\n",
            "\n",
            "\n",
            "    # --- Display Results (for Top 10 snapshot) ---\n",
            "    print(\"\\n--- Results (Top 10 Snapshot) ---\")\n",
            "    if not df_signals_top10.empty:\n",
            "        print(df_signals_top10.head().to_markdown(index=False))\n",
            "    else:\n",
            "        print(\"No top 10 snapshot signals generated due to processing errors or no data.\")\n",
            "\n",
            "    # --- Display Results (for Historical, if processed) ---\n",
            "    print(\"\\n--- Results (Historical Data) ---\")\n",
            "    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:\n",
            "         print(\"Historical signals (tail):\")\n",
            "         print(history_df_signals_display.tail().to_markdown(index=False))\n",
            "         print(\"\n",
            "Historical signals (sample):\")\n",
            "         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))\n",
            "    else:\n",
            "         print(\"No historical signals generated or historical data processing skipped.\")\n",
            "\n",
            "\n",
            "    print(\"\\nBot pipeline finished.\")\n",
            "\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1f415cfa",
        "outputId": "549d4395-5e7c-4a7a-d1da-4bc64a1a422a"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 153\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 153)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7c0e941"
      },
      "source": [
        "# Rewrite the content of signal_bot/main.py to include fetching and saving the full market snapshot.\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from .indicators.ta_utils import compute_indicators\n",
        "from .signals.signal_finder import find_signals\n",
        "from .anomaly_detector import detect_anomalies\n",
        "from .ml_logger import log_ml_features\n",
        "from .backtester import backtest_signals\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Starting bot pipeline...\")\n",
        "\n",
        "    # --- Data Collection (Top 10 Snapshot) ---\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data_top10 = get_top_coins(limit=10)\n",
        "        if data_top10 is not None:\n",
        "            df_top10 = pd.DataFrame(data_top10)\n",
        "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            top10_market_data_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "            df_top10.to_csv(top10_market_data_path, index=False)\n",
        "            print(f\"Top 10 data fetched and saved to {top10_market_data_path}.\")\n",
        "\n",
        "            # --- Indicator Computation for Top 10 ---\n",
        "            print(\"Computing technical indicators for top 10 data...\")\n",
        "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "            top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "            df_ind_top10.to_csv(top10_indicators_path, index=False)\n",
        "            print(f\"Indicators computed and saved for top 10 data to {top10_indicators_path}.\")\n",
        "\n",
        "            # --- Signal Generation for Top 10 ---\n",
        "            print(\"Generating signals for top 10 snapshot data...\")\n",
        "            df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "            top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "            df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "            print(f\"Signals generated and saved for top 10 snapshot data to {top10_signals_path}.\")\n",
        "\n",
        "            # --- ML Logging for Top 10 ---\n",
        "            print(\"Logging ML features for top 10 data...\")\n",
        "            ml_log_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "            log_ml_features(top10_indicators_path, ml_log_path)\n",
        "            print(f\"ML features logged to {ml_log_path}.\")\n",
        "\n",
        "        else:\n",
        "            print(\"Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.\")\n",
        "            df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.\")\n",
        "        df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    # --- Data Collection (Full Market Snapshot for Anomaly Detection) ---\n",
        "    print(\"\\\\nFetching full market snapshot for Anomaly Detection...\")\n",
        "    try:\n",
        "        data_full = get_top_coins(limit=250) # Fetch more coins for a broader snapshot\n",
        "        if data_full is not None:\n",
        "            df_full_snapshot = pd.DataFrame(data_full)\n",
        "            df_full_snapshot[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "            df_full_snapshot.to_csv(full_snapshot_path, index=False)\n",
        "            print(f\"Full market snapshot fetched and saved to {full_snapshot_path}.\")\n",
        "        else:\n",
        "            print(\"Failed to fetch full market snapshot.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching or saving full market snapshot: {e}.\")\n",
        "\n",
        "\n",
        "    # --- Anomaly Detection ---\n",
        "    print(\"\\\\nAttempting to run Anomaly Detection...\")\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if os.path.exists(full_snapshot_path):\n",
        "        try:\n",
        "            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "            print(f\"Anomaly detection completed. Anomalies saved to {path}.\")\n",
        "            print(f\"Found {len(anomalies)} anomalies.\")\n",
        "            if not anomalies.empty:\n",
        "                 print(\"Sample anomalies:\")\n",
        "                 print(anomalies.head().to_markdown(index=False))\n",
        "            else:\n",
        "                 print(\"No anomalies detected.\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error during Anomaly Detection: {e}.\")\n",
        "    else:\n",
        "        print(f\"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.\")\n",
        "\n",
        "    # --- Historical Data Processing ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals_display = pd.DataFrame()\n",
        "    historical_price_data_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_price.csv\") # Define path for historical price data\n",
        "    historical_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\") # Define path for historical signals\n",
        "\n",
        "    print(f\"\\\\nAttempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "        if history_data is not None and \"prices\" in history_data:\n",
        "            print(\"Historical data fetched.\")\n",
        "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"current_price\"]) # Use current_price for consistency\n",
        "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "            history_df.to_csv(historical_price_data_path, index=False) # Save historical price data\n",
        "            print(\"Historical data processed and saved.\")\n",
        "\n",
        "            print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "            history_df[\"close\"] = history_df[\"current_price\"] # Use current_price as close for indicators\n",
        "            history_df_ind = compute_indicators(history_df.copy()) # Pass DataFrame directly\n",
        "            history_historical_indicators_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_with_indicators.csv\")\n",
        "            history_df_ind.to_csv(history_historical_indicators_path, index=False)\n",
        "            print(f\"Indicators computed and saved for historical data to {history_historical_indicators_path}.\")\n",
        "\n",
        "            print(f\"Generating signals for {coin_id} historical data...\")\n",
        "            history_df_signals = find_signals(history_df_ind.copy())\n",
        "            history_df_signals.to_csv(historical_signals_path, index=False)\n",
        "            print(f\"Historical signals generated and saved to {historical_signals_path}.\")\n",
        "            history_df_signals_display = history_df_signals\n",
        "\n",
        "            # --- Backtesting Historical Signals ---\n",
        "            print(f\"\\\\nAttempting to backtest historical signals for {coin_id}...\")\n",
        "            # backtest_signals expects paths to signal CSV and price CSV\n",
        "            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)\n",
        "            print(\"Backtesting completed.\")\n",
        "            if not backtest_results_df.empty:\n",
        "                print(\"Backtest Results (head):\")\n",
        "                print(backtest_results_df.head().to_markdown(index=False))\n",
        "            else:\n",
        "                print(\"No backtest results to display.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "             print(\"Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing historical data or backtesting: {e}. Skipping historical signals and backtesting.\")\n",
        "\n",
        "\n",
        "    # --- Display Results (for Top 10 snapshot) ---\n",
        "    print(\"\\\\n--- Results (Top 10 Snapshot) ---\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False))\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated due to processing errors or no data.\")\n",
        "\n",
        "    # --- Display Results (for Historical, if processed) ---\n",
        "    print(\"\\\\n--- Results (Historical Data) ---\")\n",
        "    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:\n",
        "         print(\"Historical signals (tail):\")\n",
        "         print(history_df_signals_display.tail().to_markdown(index=False))\n",
        "         print(\"\\nHistorical signals (sample):\")\n",
        "         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))\n",
        "    else:\n",
        "         print(\"No historical signals generated or historical data processing skipped.\")\n",
        "\n",
        "\n",
        "    print(\"\\\\nBot pipeline finished.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cf9a9afa",
        "outputId": "7e7c121c-6a60-4b98-879a-2bed9648e22b"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 140\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "754ca927",
        "outputId": "ce97c999-df78-4aa3-dc91-1b4d572c4f73"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 140\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f479356a",
        "outputId": "ef3a660e-4963-4f29-d6dc-31f44fb21e83"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 140\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 158
        },
        "id": "d9d3741d",
        "outputId": "d2be3ac1-ff0e-456b-f8ed-d8d646506669"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define the path to the historical signals file\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "historical_signals_path = os.path.join(DATA_DIR, \"bitcoin_historical_signals.csv\")\n",
        "\n",
        "# Check if the file exists before attempting to read\n",
        "if os.path.exists(historical_signals_path):\n",
        "    try:\n",
        "        # Read the historical signals file\n",
        "        historical_signals_df = pd.read_csv(historical_signals_path)\n",
        "\n",
        "        # Display the head and value counts of the 'signal' column\n",
        "        print(f\"Content of {historical_signals_path}:\")\n",
        "        display(historical_signals_df.head().to_markdown(index=False))\n",
        "\n",
        "        print(\"\\nValue counts of 'signal' column:\")\n",
        "        display(historical_signals_df['signal'].value_counts().to_markdown())\n",
        "\n",
        "        # Optionally, display a sample of rows with specific signals if needed\n",
        "        # print(\"\\nSample of BUY signals (if any):\")\n",
        "        # buy_signals_sample = historical_signals_df[historical_signals_df['signal'].str.contains('BUY', na=False)].sample(min(5, len(historical_signals_df[historical_signals_df['signal'].str.contains('BUY', na=False)])))\n",
        "        # display(buy_signals_sample.to_markdown(index=False))\n",
        "\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: {historical_signals_path} not found.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or processing {historical_signals_path}: {e}\")\n",
        "else:\n",
        "    print(f\"Error: {historical_signals_path} not found. Please run main.py first to generate this file.\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Content of signal_bot/data/bitcoin_historical_signals.csv:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'| timestamp               |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\\n|:------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|:---------|\\n| 2025-07-23 18:09:13.433 |  118121 |   nan |      nan |    nan |        nan |        nan | HOLD     |\\n| 2025-07-23 19:09:14.123 |  118402 |   nan |      nan |    nan |        nan |        nan | HOLD     |\\n| 2025-07-23 20:08:03.711 |  118452 |   nan |      nan |    nan |        nan |        nan | HOLD     |\\n| 2025-07-23 21:04:35.426 |  117982 |   nan |      nan |    nan |        nan |        nan | HOLD     |\\n| 2025-07-23 22:02:10.782 |  118133 |   nan |      nan |    nan |        nan |        nan | HOLD     |'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Value counts of 'signal' column:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "'| signal   |   count |\\n|:---------|--------:|\\n| HOLD     |     722 |'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d6b92b6",
        "outputId": "b3638b3a-2179-4ddb-dc8e-0719e251649d"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 95,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 140\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 140)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80234855"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot/models', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/ml_model_trainer.py', 'w') as f:\n",
        "    f.write('''# ml_model_trainer.py\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "def train_ml_model(training_csv):\n",
        "    df = pd.read_csv(training_csv)\n",
        "    df = df.dropna()\n",
        "\n",
        "    # Define success label (this would come from backtest or manual labeling)\n",
        "    if \"success\" not in df.columns:\n",
        "        raise ValueError(\"Expected 'success' column for supervised training.\")\n",
        "\n",
        "    X = df[[\"rsi\", \"ema_20\", \"macd_diff\", \"bb_upper\", \"bb_lower\"]]\n",
        "    y = df[\"success\"].astype(int)\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n",
        "    model = RandomForestClassifier(random_state=42)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    y_pred = model.predict(X_test)\n",
        "    report = classification_report(y_test, y_pred, output_dict=True)\n",
        "    return model, report\n",
        "''')"
      ],
      "execution_count": 93,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c11e96e7",
        "outputId": "ff846f5b-e907-4b2c-c72e-663ada9faedc"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 134\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 134)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d6d68c6",
        "outputId": "d2886ed1-ffb7-40ea-d8a0-103603a2ecdb"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 90,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting bot pipeline...\n",
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved to signal_bot/data/top10_market_data.csv.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data to signal_bot/data/top10_with_indicators.csv.\n",
            "Generating signals for top 10 snapshot data...\n",
            "\n",
            "--- Inside find_signals ---\n",
            "Input DataFrame columns: ['id', 'symbol', 'name', 'image', 'current_price', 'market_cap', 'market_cap_rank', 'fully_diluted_valuation', 'total_volume', 'high_24h', 'low_24h', 'price_change_24h', 'price_change_percentage_24h', 'market_cap_change_24h', 'market_cap_change_percentage_24h', 'circulating_supply', 'total_supply', 'max_supply', 'ath', 'ath_change_percentage', 'ath_date', 'atl', 'atl_change_percentage', 'atl_date', 'roi', 'last_updated', 'sparkline_in_7d', 'timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Input DataFrame head:\n",
            " | id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                               | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:----------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116752        | 2323305351142 |                 1 |             2323308268404 |    49242007686 |  116947    | 111764        |      4322.9        |                       3.845   |             8.48012e+10 |                            3.7883  |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -5.97532 | 2025-08-14T00:37:02.582Z | 67.81       |        172017           | 2013-07-06T00:00:00.000Z |                                                                                   | 2025-08-22T17:47:00.683Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:47:01.542206+00:00 | 116752        |   nan |      nan |    nan |        nan |        nan |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4803.4      |  581580334015 |                 2 |              581580334015 |    51929767716 |    4834.68 |   4211.66     |       563.22       |                      13.2829  |             7.00226e+10 |                           13.6881  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -1.20643 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.11298e+06 | 2015-10-20T00:00:00.000Z | {'times': 54.021206729733564, 'currency': 'btc', 'percentage': 5402.120672973356} | 2025-08-22T17:46:59.300Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:47:01.542206+00:00 |   4803.4      |   nan |      nan |    nan |        nan |        nan |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.07     |  182915735332 |                 3 |              307799470507 |     8967836415 |       3.09 |      2.79     |         0.173188   |                       5.97635 |             1.08665e+10 |                            6.31592 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -15.5543  | 2025-07-18T03:40:53.808Z |  0.00268621 |        114530           | 2014-05-22T00:00:00.000Z |                                                                                   | 2025-08-22T17:46:57.287Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:47:01.542206+00:00 |      3.07     |   nan |      nan |    nan |        nan |        nan |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999988 |  167062514737 |                 4 |              167062514737 |   111032230496 |       1    |      0.999707 |         0.00018202 |                       0.01821 |             4.0639e+07  |                            0.02433 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4215  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6617      | 2015-03-02T00:00:00.000Z |                                                                                   | 2025-08-22T17:46:58.674Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:47:01.542206+00:00 |      0.999988 |   nan |      nan |    nan |        nan |        nan |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      893.48     |  124706012929 |                 5 |              124706010987 |     2104078484 |     895.32 |    835.34     |        51.48       |                       6.11355 |             7.54738e+09 |                            6.44202 |          1.39287e+08 |    1.39287e+08 |      2e+08   |    894.65 |                 0.04376 | 2025-08-22T17:37:53.910Z |  0.0398177  |             2.24775e+06 | 2017-10-19T00:00:00.000Z |                                                                                   | 2025-08-22T17:46:50.708Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:47:01.542206+00:00 |    893.48     |   nan |      nan |    nan |        nan |        nan |\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data to signal_bot/data/top10_signals.csv.\n",
            "Logging ML features for top 10 data...\n",
            "Warning: Missing required features for ML logging in signal_bot/data/top10_with_indicators.csv: ['macd_diff']. Skipping logging for this file.\n",
            "ML features logged to signal_bot/data/ml_training.csv.\n",
            "\n",
            "Attempting to run Anomaly Detection...\n",
            "Warning: signal_bot/data/full_market_snapshot.csv not found. Skipping Anomaly Detection.\n",
            "\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data to signal_bot/data/bitcoin_historical_with_indicators.csv.\n",
            "Generating signals for bitcoin historical data...\n",
            "\n",
            "--- Inside find_signals ---\n",
            "Input DataFrame columns: ['timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Input DataFrame head:\n",
            " | timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan |\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Historical signals generated and saved to signal_bot/data/bitcoin_historical_signals.csv.\n",
            "\n",
            "--- Results (Top 10 Snapshot) ---\n",
            "| id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                               | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:----------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116752        | 2323305351142 |                 1 |             2323308268404 |    49242007686 |  116947    | 111764        |      4322.9        |                       3.845   |             8.48012e+10 |                            3.7883  |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -5.97532 | 2025-08-14T00:37:02.582Z | 67.81       |        172017           | 2013-07-06T00:00:00.000Z |                                                                                   | 2025-08-22T17:47:00.683Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:47:01.542206+00:00 | 116752        |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4803.4      |  581580334015 |                 2 |              581580334015 |    51929767716 |    4834.68 |   4211.66     |       563.22       |                      13.2829  |             7.00226e+10 |                           13.6881  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -1.20643 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.11298e+06 | 2015-10-20T00:00:00.000Z | {'times': 54.021206729733564, 'currency': 'btc', 'percentage': 5402.120672973356} | 2025-08-22T17:46:59.300Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:47:01.542206+00:00 |   4803.4      |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.07     |  182915735332 |                 3 |              307799470507 |     8967836415 |       3.09 |      2.79     |         0.173188   |                       5.97635 |             1.08665e+10 |                            6.31592 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -15.5543  | 2025-07-18T03:40:53.808Z |  0.00268621 |        114530           | 2014-05-22T00:00:00.000Z |                                                                                   | 2025-08-22T17:46:57.287Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:47:01.542206+00:00 |      3.07     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999988 |  167062514737 |                 4 |              167062514737 |   111032230496 |       1    |      0.999707 |         0.00018202 |                       0.01821 |             4.0639e+07  |                            0.02433 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4215  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6617      | 2015-03-02T00:00:00.000Z |                                                                                   | 2025-08-22T17:46:58.674Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:47:01.542206+00:00 |      0.999988 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      893.48     |  124706012929 |                 5 |              124706010987 |     2104078484 |     895.32 |    835.34     |        51.48       |                       6.11355 |             7.54738e+09 |                            6.44202 |          1.39287e+08 |    1.39287e+08 |      2e+08   |    894.65 |                 0.04376 | 2025-08-22T17:37:53.910Z |  0.0398177  |             2.24775e+06 | 2017-10-19T00:00:00.000Z |                                                                                   | 2025-08-22T17:46:50.708Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:47:01.542206+00:00 |    893.48     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "--- Results (Historical Data) ---\n",
            "| timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "Bot pipeline finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fdf2f4bd",
        "outputId": "e36f10d9-2a5c-4bd6-aa8a-df5ca5e3cd92"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting bot pipeline...\n",
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved to signal_bot/data/top10_market_data.csv.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data to signal_bot/data/top10_with_indicators.csv.\n",
            "Generating signals for top 10 snapshot data...\n",
            "\n",
            "--- Inside find_signals ---\n",
            "Input DataFrame columns: ['id', 'symbol', 'name', 'image', 'current_price', 'market_cap', 'market_cap_rank', 'fully_diluted_valuation', 'total_volume', 'high_24h', 'low_24h', 'price_change_24h', 'price_change_percentage_24h', 'market_cap_change_24h', 'market_cap_change_percentage_24h', 'circulating_supply', 'total_supply', 'max_supply', 'ath', 'ath_change_percentage', 'ath_date', 'atl', 'atl_change_percentage', 'atl_date', 'roi', 'last_updated', 'sparkline_in_7d', 'timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Input DataFrame head:\n",
            " | id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                              | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:---------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116693        | 2323305351142 |                 1 |             2323308268404 |    49819804888 |  116947    | 111764        |      4345.53       |                       3.86793 |             8.48012e+10 |                            3.7883  |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -5.97532 | 2025-08-14T00:37:02.582Z | 67.81       |        172017           | 2013-07-06T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:22.509Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:46:18.896134+00:00 | 116693        |   nan |      nan |    nan |        nan |        nan |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4810.13     |  581580334015 |                 2 |              581580334015 |    51767416158 |    4834.68 |   4211.66     |       569.95       |                      13.4418  |             7.00226e+10 |                           13.6881  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -1.20643 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.11298e+06 | 2015-10-20T00:00:00.000Z | {'times': 54.12698887742784, 'currency': 'btc', 'percentage': 5412.698887742784} | 2025-08-22T17:45:26.052Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:46:18.896134+00:00 |   4810.13     |   nan |      nan |    nan |        nan |        nan |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.07     |  182915735332 |                 3 |              307799470507 |     8961033877 |       3.09 |      2.79     |         0.176501   |                       6.09662 |             1.08665e+10 |                            6.31592 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -15.5543  | 2025-07-18T03:40:53.808Z |  0.00268621 |        114530           | 2014-05-22T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:22.924Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:46:18.896134+00:00 |      3.07     |   nan |      nan |    nan |        nan |        nan |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999999 |  167062514737 |                 4 |              167062514737 |   110844567193 |       1    |      0.999707 |         0.00019261 |                       0.01926 |             4.0639e+07  |                            0.02433 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4215  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6617      | 2015-03-02T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:24.550Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:46:18.896134+00:00 |      0.999999 |   nan |      nan |    nan |        nan |        nan |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      893.4      |  124706012929 |                 5 |              124706010987 |     2101830895 |     895.32 |    835.34     |        52.12       |                       6.1957  |             7.54738e+09 |                            6.44202 |          1.39287e+08 |    1.39287e+08 |      2e+08   |    894.65 |                 0.04376 | 2025-08-22T17:37:53.910Z |  0.0398177  |             2.24775e+06 | 2017-10-19T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:27.019Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:46:18.896134+00:00 |    893.4      |   nan |      nan |    nan |        nan |        nan |\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data to signal_bot/data/top10_signals.csv.\n",
            "Logging ML features for top 10 data...\n",
            "Warning: Missing required features for ML logging in signal_bot/data/top10_with_indicators.csv: ['macd_diff']. Skipping logging for this file.\n",
            "ML features logged to signal_bot/data/ml_training.csv.\n",
            "\n",
            "Attempting to run Anomaly Detection...\n",
            "Warning: signal_bot/data/full_market_snapshot.csv not found. Skipping Anomaly Detection.\n",
            "\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data to signal_bot/data/bitcoin_historical_with_indicators.csv.\n",
            "Generating signals for bitcoin historical data...\n",
            "\n",
            "--- Inside find_signals ---\n",
            "Input DataFrame columns: ['timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Input DataFrame head:\n",
            " | timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan |\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Historical signals generated and saved to signal_bot/data/bitcoin_historical_signals.csv.\n",
            "\n",
            "--- Results (Top 10 Snapshot) ---\n",
            "| id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                              | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:---------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116693        | 2323305351142 |                 1 |             2323308268404 |    49819804888 |  116947    | 111764        |      4345.53       |                       3.86793 |             8.48012e+10 |                            3.7883  |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -5.97532 | 2025-08-14T00:37:02.582Z | 67.81       |        172017           | 2013-07-06T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:22.509Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:46:18.896134+00:00 | 116693        |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4810.13     |  581580334015 |                 2 |              581580334015 |    51767416158 |    4834.68 |   4211.66     |       569.95       |                      13.4418  |             7.00226e+10 |                           13.6881  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -1.20643 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.11298e+06 | 2015-10-20T00:00:00.000Z | {'times': 54.12698887742784, 'currency': 'btc', 'percentage': 5412.698887742784} | 2025-08-22T17:45:26.052Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:46:18.896134+00:00 |   4810.13     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.07     |  182915735332 |                 3 |              307799470507 |     8961033877 |       3.09 |      2.79     |         0.176501   |                       6.09662 |             1.08665e+10 |                            6.31592 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -15.5543  | 2025-07-18T03:40:53.808Z |  0.00268621 |        114530           | 2014-05-22T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:22.924Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:46:18.896134+00:00 |      3.07     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999999 |  167062514737 |                 4 |              167062514737 |   110844567193 |       1    |      0.999707 |         0.00019261 |                       0.01926 |             4.0639e+07  |                            0.02433 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4215  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6617      | 2015-03-02T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:24.550Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:46:18.896134+00:00 |      0.999999 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      893.4      |  124706012929 |                 5 |              124706010987 |     2101830895 |     895.32 |    835.34     |        52.12       |                       6.1957  |             7.54738e+09 |                            6.44202 |          1.39287e+08 |    1.39287e+08 |      2e+08   |    894.65 |                 0.04376 | 2025-08-22T17:37:53.910Z |  0.0398177  |             2.24775e+06 | 2017-10-19T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:27.019Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:46:18.896134+00:00 |    893.4      |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "--- Results (Historical Data) ---\n",
            "| timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "Bot pipeline finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b847cbf",
        "outputId": "66204038-671b-495b-a5d7-6e3d86d7b78f"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 86,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting bot pipeline...\n",
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved to signal_bot/data/top10_market_data.csv.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data to signal_bot/data/top10_with_indicators.csv.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data to signal_bot/data/top10_signals.csv.\n",
            "Logging ML features for top 10 data...\n",
            "Warning: Missing required features for ML logging in signal_bot/data/top10_with_indicators.csv: ['macd_diff']. Skipping logging for this file.\n",
            "ML features logged to signal_bot/data/ml_training.csv.\n",
            "\n",
            "Attempting to run Anomaly Detection...\n",
            "Warning: signal_bot/data/full_market_snapshot.csv not found. Skipping Anomaly Detection.\n",
            "\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data to signal_bot/data/bitcoin_historical_with_indicators.csv.\n",
            "Generating signals for bitcoin historical data...\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Historical signals generated and saved to signal_bot/data/bitcoin_historical_signals.csv.\n",
            "\n",
            "--- Results (Top 10 Snapshot) ---\n",
            "| id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                              | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:---------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116693        | 2323305351142 |                 1 |             2323308268404 |    49819804888 |  116947    | 111764        |      4345.53       |                       3.86793 |             8.48012e+10 |                            3.7883  |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -5.97532 | 2025-08-14T00:37:02.582Z | 67.81       |        172017           | 2013-07-06T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:22.509Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:45:32.009530+00:00 | 116693        |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4810.13     |  581580334015 |                 2 |              581580334015 |    51767416158 |    4834.68 |   4211.66     |       569.95       |                      13.4418  |             7.00226e+10 |                           13.6881  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -1.20643 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.11298e+06 | 2015-10-20T00:00:00.000Z | {'times': 54.12698887742784, 'currency': 'btc', 'percentage': 5412.698887742784} | 2025-08-22T17:45:26.052Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:45:32.009530+00:00 |   4810.13     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.07     |  182915735332 |                 3 |              307799470507 |     8961033877 |       3.09 |      2.79     |         0.176501   |                       6.09662 |             1.08665e+10 |                            6.31592 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -15.5543  | 2025-07-18T03:40:53.808Z |  0.00268621 |        114530           | 2014-05-22T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:22.924Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:45:32.009530+00:00 |      3.07     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999999 |  167062514737 |                 4 |              167062514737 |   110844567193 |       1    |      0.999707 |         0.00019261 |                       0.01926 |             4.0639e+07  |                            0.02433 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4215  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6617      | 2015-03-02T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:24.550Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:45:32.009530+00:00 |      0.999999 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      893.4      |  124706012929 |                 5 |              124706010987 |     2101830895 |     895.32 |    835.34     |        52.12       |                       6.1957  |             7.54738e+09 |                            6.44202 |          1.39287e+08 |    1.39287e+08 |      2e+08   |    894.65 |                 0.04376 | 2025-08-22T17:37:53.910Z |  0.0398177  |             2.24775e+06 | 2017-10-19T00:00:00.000Z |                                                                                  | 2025-08-22T17:45:27.019Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:45:32.009530+00:00 |    893.4      |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "--- Results (Historical Data) ---\n",
            "| timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "Bot pipeline finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3d17e32c"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/backtester.py', 'w') as f:\n",
        "    f.write('''# backtester.py\n",
        "import pandas as pd\n",
        "import os # Import os for path handling\n",
        "\n",
        "def backtest_signals(signal_csv, price_csv, threshold=0.05, window=6):\n",
        "    \"\"\"\n",
        "    Backtests trading signals against historical price data.\n",
        "\n",
        "    Args:\n",
        "        signal_csv (str): Path to the CSV file containing signals.\n",
        "        price_csv (str): Path to the CSV file containing historical price data.\n",
        "        threshold (float): The percentage price increase considered a successful BUY signal.\n",
        "        window (int): The number of future price points to consider after a signal.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing backtest results.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        signals = pd.read_csv(signal_csv, parse_dates=[\"timestamp\"])\n",
        "        prices = pd.read_csv(price_csv, parse_dates=[\"timestamp\"])\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data for backtesting: {e}\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV files for backtesting: {e}\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on other errors\n",
        "\n",
        "    print(\"\\\\n--- Inside backtest_signals ---\") # Debug print\n",
        "    print(f\"Read signals CSV: {signal_csv}\") # Debug print\n",
        "    print(\"Signals DataFrame dtypes:\\\\n\", signals.dtypes) # Debug print\n",
        "    print(\"Signals DataFrame head:\\\\n\", signals.head().to_markdown(index=False)) # Debug print\n",
        "\n",
        "\n",
        "    results = []\n",
        "    # Ensure data is sorted for correct future price lookup\n",
        "    signals = signals.sort_values(\"timestamp\")\n",
        "    prices = prices.sort_values(\"timestamp\")\n",
        "\n",
        "    # Filter for BUY signals using a more explicit method\n",
        "    # Ensure the 'signal' column is treated as string for explicit checking\n",
        "    signals['signal'] = signals['signal'].astype(str)\n",
        "    # Use a boolean mask with apply and lambda or list comprehension for filtering\n",
        "    buy_signals = signals[signals['signal'].apply(lambda x: 'BUY' in x)].copy()\n",
        "\n",
        "\n",
        "    print(f\"Found {len(buy_signals)} BUY signals for backtesting after filtering.\") # Debug print\n",
        "\n",
        "\n",
        "    if buy_signals.empty:\n",
        "        print(\"No BUY signals found for backtesting.\")\n",
        "        print(\"--- Exiting backtest_signals ---\\\\n\") # Debug print\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    for index, signal in buy_signals.iterrows(): # Iterate through BUY signals\n",
        "        print(f\"Backtesting signal: {signal['signal']} at {signal['timestamp']}\") # Debug print\n",
        "        coin_id = signal[\"id\"]\n",
        "        signal_time = signal[\"timestamp\"]\n",
        "        # Ensure 'current_price' column exists in the signals DataFrame if needed,\n",
        "        # or get the price at the signal time from the prices DataFrame\n",
        "        # Assuming signals DataFrame has 'current_price' for simplicity based on user code structure\n",
        "        if \"current_price\" not in signal:\n",
        "             # Fallback to prices DataFrame if current_price is not in signals\n",
        "             # Need to ensure the prices DataFrame also has 'id' for lookup\n",
        "             if \"id\" not in prices.columns:\n",
        "                  print(\"Error: 'id' column not found in prices DataFrame for lookup.\")\n",
        "                  continue # Skip this signal\n",
        "             price_at_signal_row = prices[\n",
        "                 (prices[\"id\"] == coin_id) &\n",
        "                 (prices[\"timestamp\"] == signal_time)\n",
        "             ]\n",
        "             if price_at_signal_row.empty:\n",
        "                  print(f\"Warning: Price at signal time not found for {coin_id} at {signal_time}. Skipping.\")\n",
        "                  continue\n",
        "             price_at_signal = price_at_signal_row.iloc[0][\"current_price\"]\n",
        "        else:\n",
        "            price_at_signal = signal[\"current_price\"]\n",
        "\n",
        "\n",
        "        # Filter for future prices of this coin within the window\n",
        "        future_prices = prices[\n",
        "            (prices[\"id\"] == coin_id) &\n",
        "            (prices[\"timestamp\"] > signal_time)\n",
        "        ].sort_values(\"timestamp\").head(window)\n",
        "\n",
        "        print(f\"Found {len(future_prices)} future price points within window.\") # Debug print\n",
        "\n",
        "\n",
        "        if not future_prices.empty:\n",
        "            # Find the maximum price within the future window\n",
        "            # Ensure 'current_price' column exists in future_prices\n",
        "            if \"current_price\" not in future_prices.columns:\n",
        "                 print(\"Error: 'current_price' column not found in future prices DataFrame.\")\n",
        "                 continue # Skip this signal\n",
        "            max_price = future_prices[\"current_price\"].max()\n",
        "            return_pct = (max_price - price_at_signal) / price_at_signal\n",
        "\n",
        "            result = {\n",
        "                \"id\": coin_id,\n",
        "                \"timestamp\": signal_time,\n",
        "                \"signal\": signal[\"signal\"], # Log the specific BUY signal\n",
        "                \"price_at_signal\": price_at_signal, # Add price at signal for context\n",
        "                \"max_future_price\": max_price, # Add max future price\n",
        "                \"return_pct\": return_pct,\n",
        "                \"success\": return_pct >= threshold\n",
        "            }\n",
        "            results.append(result)\n",
        "        else:\n",
        "             # Handle cases where no future price data is available within the window\n",
        "             print(f\"Warning: No future price data found within window for {coin_id} at {signal_time}. Skipping backtest for this signal.\")\n",
        "             # Optional: Log this signal with a specific status\n",
        "\n",
        "    if not results:\n",
        "         print(\"No backtest results generated (either no BUY signals processed or no successful trades).\") # Refine message\n",
        "         print(\"--- Exiting backtest_signals ---\\\\n\") # Debug print\n",
        "         return pd.DataFrame()\n",
        "\n",
        "    result_df = pd.DataFrame(results)\n",
        "    output_path = \"data/signal_backtest.csv\" # Define output path\n",
        "    os.makedirs(os.path.dirname(output_path), exist_ok=True) # Ensure directory exists\n",
        "    result_df.to_csv(output_path, index=False)\n",
        "    print(f\"Backtest results saved to {output_path}\")\n",
        "    print(\"--- Exiting backtest_signals ---\\\\n\") # Debug print\n",
        "    return result_df\n",
        "''')"
      ],
      "execution_count": 101,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8e027a6c",
        "outputId": "94de3455-aa5d-4bc0-c583-b61a58106c92"
      },
      "source": [
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scheduler...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "71cd5e3c",
        "outputId": "c3e25d34-b942-492a-c302-46310a48d557"
      },
      "source": [
        "# Add apscheduler to requirements.txt\n",
        "with open('signal_bot/requirements.txt', 'a') as f:\n",
        "    f.write('apscheduler\\n')\n",
        "\n",
        "# Install dependencies from requirements.txt\n",
        "!pip install -r signal_bot/requirements.txt"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting apscheduler (from -r signal_bot/requirements.txt (line 1))\n",
            "  Downloading APScheduler-3.11.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Requirement already satisfied: tzlocal>=3.0 in /usr/local/lib/python3.12/dist-packages (from apscheduler->-r signal_bot/requirements.txt (line 1)) (5.3.1)\n",
            "Downloading APScheduler-3.11.0-py3-none-any.whl (64 kB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/64.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.0/64.0 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: apscheduler\n",
            "Successfully installed apscheduler-3.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "763bf751",
        "outputId": "bd48df60-2dc0-4e02-882d-993afa5b8f52"
      },
      "source": [
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 2, in <module>\n",
            "    from apscheduler.schedulers.blocking import BlockingScheduler\n",
            "ModuleNotFoundError: No module named 'apscheduler'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "358d2f46",
        "outputId": "c308a1dd-c6ef-4bf8-b96c-ec92c545cd6c"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting bot pipeline...\n",
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved to signal_bot/data/top10_market_data.csv.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data to signal_bot/data/top10_with_indicators.csv.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data to signal_bot/data/top10_signals.csv.\n",
            "Logging ML features for top 10 data...\n",
            "Warning: Missing required features for ML logging in signal_bot/data/top10_with_indicators.csv: ['macd_diff']. Skipping logging for this file.\n",
            "ML features logged to signal_bot/data/ml_training.csv.\n",
            "\n",
            "Attempting to run Anomaly Detection...\n",
            "Warning: signal_bot/data/full_market_snapshot.csv not found. Skipping Anomaly Detection.\n",
            "\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data to signal_bot/data/bitcoin_historical_with_indicators.csv.\n",
            "Generating signals for bitcoin historical data...\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Historical signals generated and saved to signal_bot/data/bitcoin_historical_signals.csv.\n",
            "\n",
            "--- Results (Top 10 Snapshot) ---\n",
            "| id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                               | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:----------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116779        | 2323457770309 |                 1 |             2323457770309 |    48941269036 |  116947    | 111764        |      4330.35       |                       3.85097 |             8.81005e+10 |                            3.94123 |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -5.89332 | 2025-08-14T00:37:02.582Z | 67.81       |        172167           | 2013-07-06T00:00:00.000Z |                                                                                   | 2025-08-22T17:38:51.812Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:39:07.037610+00:00 | 116779        |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4821.36     |  581750852990 |                 2 |              581750852990 |    44952396005 |    4834.68 |   4211.66     |       589.67       |                      13.9347  |             7.10306e+10 |                           13.9079  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -1.25643 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.11242e+06 | 2015-10-20T00:00:00.000Z | {'times': 54.207900404720725, 'currency': 'btc', 'percentage': 5420.790040472072} | 2025-08-22T17:38:52.060Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:39:07.037610+00:00 |   4821.36     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.08     |  183344789995 |                 3 |              308521457589 |     8896753937 |       3.09 |      2.79     |         0.195827   |                       6.77869 |             1.16966e+10 |                            6.81432 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -15.5605  | 2025-07-18T03:40:53.808Z |  0.00268621 |        114522           | 2014-05-22T00:00:00.000Z |                                                                                   | 2025-08-22T17:38:54.631Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:39:07.037610+00:00 |      3.08     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999973 |  167063364044 |                 4 |              167063364044 |   110071004792 |       1    |      0.999707 |         0.00018273 |                       0.01828 |             3.86312e+07 |                            0.02313 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4201  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6649      | 2015-03-02T00:00:00.000Z |                                                                                   | 2025-08-22T17:38:54.989Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:39:07.037610+00:00 |      0.999973 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      894.71     |  124258712651 |                 5 |              124258712651 |     2076767508 |     894.65 |    835.34     |        53.99       |                       6.42193 |             7.22711e+09 |                            6.17535 |          1.39287e+08 |    1.39287e+08 |      2e+08   |    886.99 |                 0.25052 | 2025-08-22T17:17:52.348Z |  0.0398177  |             2.23312e+06 | 2017-10-19T00:00:00.000Z |                                                                                   | 2025-08-22T17:38:54.639Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:39:07.037610+00:00 |    894.71     |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "--- Results (Historical Data) ---\n",
            "| timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "Bot pipeline finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0dc3e73c",
        "outputId": "757c1334-a26f-4193-a9b5-207afef071f0"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting bot pipeline...\n",
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved to signal_bot/data/top10_market_data.csv.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data to signal_bot/data/top10_with_indicators.csv.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Missing required columns for signal generation: ['rsi', 'macd_diff', 'current_price', 'bb_upper', 'bb_lower']. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data to signal_bot/data/top10_signals.csv.\n",
            "Logging ML features for top 10 data...\n",
            "Warning: Missing required features for ML logging in signal_bot/data/top10_with_indicators.csv: ['macd_diff']. Skipping logging for this file.\n",
            "ML features logged to signal_bot/data/ml_training.csv.\n",
            "\n",
            "Attempting to run Anomaly Detection...\n",
            "Warning: signal_bot/data/full_market_snapshot.csv not found. Skipping Anomaly Detection.\n",
            "\n",
            "--- Results (Top 10 Snapshot) ---\n",
            "| id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                               | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |        close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower | signal   |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:----------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|-------------:|------:|---------:|-------:|-----------:|-----------:|:---------|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |    116729       | 2323457770309 |                 1 |             2323457770309 |    48937218273 |  116947    | 111764        |      4372.53       |                       3.89166 |             8.81005e+10 |                            3.94123 |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -5.89332 | 2025-08-14T00:37:02.582Z | 67.81       |        172167           | 2013-07-06T00:00:00.000Z |                                                                                   | 2025-08-22T17:37:01.289Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:37:17.951914+00:00 | 116729       |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |      4827.87    |  581750852990 |                 2 |              581750852990 |    50859251765 |    4834.68 |   4211.66     |       596.18       |                      14.0884  |             7.10306e+10 |                           13.9079  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -1.25643 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.11242e+06 | 2015-10-20T00:00:00.000Z | {'times': 54.307001055785825, 'currency': 'btc', 'percentage': 5430.700105578582} | 2025-08-22T17:36:56.445Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:37:17.951914+00:00 |   4827.87    |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |         3.09    |  183344789995 |                 3 |              308521457589 |     8890912261 |       3.09 |      2.79     |         0.196265   |                       6.79385 |             1.16966e+10 |                            6.81432 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -15.5605  | 2025-07-18T03:40:53.808Z |  0.00268621 |        114522           | 2014-05-22T00:00:00.000Z |                                                                                   | 2025-08-22T17:36:57.667Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:37:17.951914+00:00 |      3.09    |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |         0.99998 |  167063364044 |                 4 |              167063364044 |   109772158470 |       1    |      0.999707 |         0.00019048 |                       0.01905 |             3.86312e+07 |                            0.02313 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4201  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6649      | 2015-03-02T00:00:00.000Z |                                                                                   | 2025-08-22T17:36:59.141Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:37:17.951914+00:00 |      0.99998 |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |       894.45    |  124258712651 |                 5 |              124258712651 |     2072595620 |     894.22 |    835.34     |        55.13       |                       6.56792 |             7.22711e+09 |                            6.17535 |          1.39287e+08 |    1.39287e+08 |      2e+08   |    886.99 |                 0.25052 | 2025-08-22T17:17:52.348Z |  0.0398177  |             2.23312e+06 | 2017-10-19T00:00:00.000Z |                                                                                   | 2025-08-22T17:36:57.808Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:37:17.951914+00:00 |    894.45    |   nan |      nan |    nan |        nan |        nan | HOLD     |\n",
            "\n",
            "Bot pipeline finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "31c078be",
        "outputId": "9c863c00-6e94-4526-e0e1-696ad2a00f2b"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting bot pipeline...\n",
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved to signal_bot/data/top10_market_data.csv.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data to signal_bot/data/top10_with_indicators.csv.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data to signal_bot/data/top10_signals.csv.\n",
            "Logging ML features for top 10 data...\n",
            "Warning: Missing required features for ML logging in signal_bot/data/top10_with_indicators.csv: ['macd_diff']. Skipping logging for this file.\n",
            "ML features logged to signal_bot/data/ml_training.csv.\n",
            "\n",
            "Attempting to run Anomaly Detection...\n",
            "Warning: signal_bot/data/full_market_snapshot.csv not found. Skipping Anomaly Detection.\n",
            "\n",
            "--- Results (Top 10 Snapshot) ---\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "\n",
            "Bot pipeline finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d83b5060",
        "outputId": "24544376-e64b-46a0-b027-7024a5538e78"
      },
      "source": [
        "# Execute the main script\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting bot pipeline...\n",
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved to signal_bot/data/top10_market_data.csv.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data to signal_bot/data/top10_with_indicators.csv.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data to signal_bot/data/top10_signals.csv.\n",
            "Logging ML features for top 10 data...\n",
            "Error processing top 10 data pipeline: \"['macd_diff'] not in index\". Skipping subsequent steps for top 10.\n",
            "\n",
            "Attempting to run Anomaly Detection...\n",
            "Warning: signal_bot/data/full_market_snapshot.csv not found. Skipping Anomaly Detection.\n",
            "\n",
            "--- Results (Top 10 Snapshot) ---\n",
            "No top 10 snapshot signals generated due to processing errors or no data.\n",
            "\n",
            "Bot pipeline finished.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b8caa34",
        "outputId": "04becc9f-43f1-4990-abcb-d3086025fe91"
      },
      "source": [
        "# Execute the main script\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/signal_bot/main.py\", line 7, in <module>\n",
            "    from signal_bot.anomaly_detector import detect_anomalies # Import anomaly_detector\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'signal_bot'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "779727ec"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the file - Temporarily modify to run only full_data_collection_job once\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "# print(\"Starting scheduler...\")\n",
        "# scheduler.start() # Comment out to prevent immediate blocking execution\n",
        "\n",
        "# Manual test execution of full_data_collection_job once\n",
        "setup_logger() # Ensure logger is setup\n",
        "full_data_collection_job()\n",
        "''')"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1cd63fa1"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/dataset_manager.py', 'w') as f:\n",
        "    f.write('''# dataset_manager.py\n",
        "import pandas as pd\n",
        "\n",
        "def clean_and_normalize(df):\n",
        "    \"\"\"Clean and normalize DataFrame columns and data types.\"\"\"\n",
        "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
        "    df = df.dropna(subset=[\"id\", \"current_price\"])\n",
        "    df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "    df = df.sort_values(by=[\"id\", \"timestamp\"])\n",
        "    return df''')"
      ],
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b6e40fb1"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot/signals', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/signals/signal_finder.py', 'w') as f:\n",
        "    f.write('''# signal_finder.py\n",
        "import pandas as pd\n",
        "import numpy as np # Import numpy for NaN checks\n",
        "\n",
        "def find_signals(df):\n",
        "    \"\"\"Detect signals based on indicator thresholds.\"\"\"\n",
        "    print(\"\\\\n--- Inside find_signals ---\") # Debug print\n",
        "    print(\"Input DataFrame columns:\", df.columns.tolist()) # Debug print\n",
        "    print(\"Input DataFrame head:\\\\n\", df.head().to_markdown(index=False)) # Debug print\n",
        "\n",
        "\n",
        "    signals_list = [] # Use a different name to avoid conflict with signals variable inside loop\n",
        "\n",
        "    # Ensure DataFrame has required columns. Add checks for NaN values.\n",
        "    required_cols = [\"rsi\", \"macd_diff\", \"current_price\", \"bb_upper\", \"bb_lower\"]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(f\"Warning: Missing required columns for signal generation: {required_cols}. Returning HOLD signals.\")\n",
        "        df[\"signal\"] = \"HOLD\"\n",
        "        return df\n",
        "\n",
        "    # Initialize signal column\n",
        "    df[\"signal\"] = \"HOLD\"\n",
        "\n",
        "    # Add a column for previous MACD diff to simplify check\n",
        "    df[\"macd_diff_prev\"] = df[\"macd_diff\"].shift(1)\n",
        "\n",
        "    # Check if there are any rows with valid indicator values\n",
        "    if df[required_cols].dropna().empty:\n",
        "         print(\"Warning: All required indicator values are NaN. Returning HOLD signals.\")\n",
        "         # Drop the temporary column before returning\n",
        "         if 'macd_diff_prev' in df.columns:\n",
        "              df = df.drop(columns=[\"macd_diff_prev\"])\n",
        "         print(\"--- Exiting find_signals ---\\\\n\") # Debug print\n",
        "         return df # Return the DataFrame with all HOLD signals\n",
        "\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        sigs = []\n",
        "\n",
        "        # Skip rows with NaN indicators for signal generation\n",
        "        if np.isnan(row[\"rsi\"]) or np.isnan(row[\"macd_diff\"]) or np.isnan(row[\"current_price\"]) or np.isnan(row[\"bb_upper\"]) or np.isnan(row[\"bb_lower\"]):\n",
        "            # print(f\"Skipping row {index} due to NaN indicators\") # Debug print\n",
        "            # signals_list.append(\"HOLD\") # No longer need signals_list, updating df directly\n",
        "            continue # Skip to the next row if indicators are NaN\n",
        "\n",
        "        # RSI signals\n",
        "        if row[\"rsi\"] < 30:\n",
        "            sigs.append(\"BUY_RSI_OVERSOLD\")\n",
        "        if row[\"rsi\"] > 70:\n",
        "            sigs.append(\"SELL_RSI_OVERBOUGHT\")\n",
        "\n",
        "        # MACD signals - check for cross above 0\n",
        "        # Ensure previous macd_diff is not NaN for cross check\n",
        "        if not np.isnan(row[\"macd_diff_prev\"]):\n",
        "            if row[\"macd_diff\"] > 0 and row[\"macd_diff_prev\"] <= 0:\n",
        "                sigs.append(\"BUY_MACD_CROSS\")\n",
        "            elif row[\"macd_diff\"] < 0 and row[\"macd_diff_prev\"] >= 0:\n",
        "                sigs.append(\"SELL_MACD_CROSS\")\n",
        "        # Simplified MACD trending check (can be adjusted)\n",
        "        # if row[\"macd_diff\"] > 0 and row[\"macd_diff\"] > row.get(\"macd_diff_prev\", -float('inf')): # Use -inf to handle first row\n",
        "        #     sigs.append(\"BUY_MACD_TRENDING\")\n",
        "\n",
        "\n",
        "        # Bollinger Bands signals\n",
        "        if row[\"current_price\"] > row[\"bb_upper\"]:\n",
        "            sigs.append(\"OVERBOUGHT_VOLATILE\")\n",
        "        if row[\"current_price\"] < row[\"bb_lower\"]:\n",
        "            sigs.append(\"POTENTIAL_BREAKOUT\")\n",
        "\n",
        "        # Combine signals for the current row\n",
        "        df.loc[index, \"signal\"] = \", \".join(sigs) if sigs else \"HOLD\"\n",
        "\n",
        "    # Drop the temporary column\n",
        "    df = df.drop(columns=[\"macd_diff_prev\"])\n",
        "    print(\"--- Exiting find_signals ---\\\\n\") # Debug print\n",
        "\n",
        "    return df # Return the DataFrame with the added signal column\n",
        "''')"
      ],
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3c411cb4"
      },
      "source": [
        "# signals/generate_signals.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "import numpy as np\n",
        "\n",
        "def generate_signal(df):\n",
        "    \"\"\"\n",
        "    Generates trading signals based on technical indicators for time-series data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).\n",
        "                           Assumes DataFrame is sorted by timestamp.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'signal' column added.\n",
        "    \"\"\"\n",
        "    # Ensure the DataFrame has required columns\n",
        "    required_cols = ['rsi', 'ema_20', 'macd_diff', 'close', 'bb_upper', 'bb_lower']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(f\"Warning: Missing required columns for signal generation. Required: {required_cols}, Found: {df.columns.tolist()}. Returning HOLD signals.\")\n",
        "        df['signal'] = 'HOLD'\n",
        "        return df[['signal']]\n",
        "\n",
        "    if df.empty:\n",
        "         print(\"Warning: Empty DataFrame provided for signal generation. Returning HOLD signals.\")\n",
        "         df['signal'] = 'HOLD'\n",
        "         return df[['signal']]\n",
        "\n",
        "    # Initialize signal column\n",
        "    df['signal'] = 'HOLD'\n",
        "\n",
        "    # Find the first index where all required indicators are not NaN\n",
        "    df_cleaned = df.dropna(subset=required_cols)\n",
        "\n",
        "    if df_cleaned.empty:\n",
        "         print(\"Warning: No rows with valid indicator values found after dropping NaNs. Returning HOLD signals.\")\n",
        "         # Set all signals to HOLD if no valid indicator rows are found\n",
        "         df['signal'] = 'HOLD'\n",
        "         return df[['signal']]\n",
        "\n",
        "    first_valid_index = df_cleaned.index.min()\n",
        "    first_valid_iloc = df.index.get_loc(first_valid_index)\n",
        "\n",
        "\n",
        "    # Apply signal logic based on technical indicators, starting from the first valid index\n",
        "    # Iterate over the original DataFrame to set signals, but use df_cleaned for valid rows\n",
        "    for i in range(first_valid_iloc, len(df)):\n",
        "        current_row = df.iloc[i]\n",
        "\n",
        "        # Ensure previous row exists and has valid indicator values\n",
        "        prev_row_iloc = i - 1\n",
        "        if prev_row_iloc < first_valid_iloc:\n",
        "             # Previous row does not have valid indicators\n",
        "             df.loc[current_row.name, 'signal'] = 'HOLD'\n",
        "             continue\n",
        "\n",
        "        prev_row = df.iloc[prev_row_iloc]\n",
        "        signals = []\n",
        "\n",
        "        # RSI signals (using current and previous crossing thresholds)\n",
        "        if current_row['rsi'] < 30 and prev_row['rsi'] >= 30:\n",
        "            signals.append(\"BUY_RSI_CROSS\")\n",
        "        elif current_row['rsi'] > 70 and prev_row['rsi'] <= 70:\n",
        "            signals.append(\"SELL_RSI_CROSS\")\n",
        "\n",
        "        # RSI extreme conditions (may add to existing signals)\n",
        "        if current_row['rsi'] < 20:\n",
        "             signals.append(\"EXTREME_OVERSOLD\")\n",
        "        elif current_row['rsi'] > 80:\n",
        "             signals.append(\"EXTREME_OVERBOUGHT\")\n",
        "\n",
        "        # MACD signals (using macd_diff crossing 0)\n",
        "        if current_row['macd_diff'] > 0 and prev_row['macd_diff'] <= 0:\n",
        "            signals.append(\"BUY_MACD_CROSS\")\n",
        "        elif current_row['macd_diff'] < 0 and prev_row['macd_diff'] >= 0:\n",
        "            signals.append(\"SELL_MACD_CROSS\")\n",
        "\n",
        "        # Bollinger Bands signals (breakouts)\n",
        "        if current_row['close'] > current_row['bb_upper'] and prev_row['close'] <= prev_row['bb_upper']:\n",
        "            signals.append(\"BREAKOUT_UPPER\")\n",
        "        elif current_row['close'] < current_row['bb_lower'] and prev_row['close'] >= prev_row['bb_lower']:\n",
        "            signals.append(\"BREAKOUT_LOWER\")\n",
        "\n",
        "        # Update the signal for the current row\n",
        "        df.loc[current_row.name, 'signal'] = \", \".join(signals) if signals else \"HOLD\"\n",
        "\n",
        "\n",
        "    # For rows before the first valid indicator index, signal is already HOLD\n",
        "\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "67899ce4",
        "outputId": "9a2cbc42-5e56-40b4-cc76-e0dfb4c1a2e1"
      },
      "source": [
        "# Execute the main script to test the updated signal generation logic after removing the strict length check.\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data.\n",
            "\n",
            "--- Inspecting df_ind_top10 before signal generation ---\n",
            "Columns: ['id', 'symbol', 'name', 'image', 'current_price', 'market_cap', 'market_cap_rank', 'fully_diluted_valuation', 'total_volume', 'high_24h', 'low_24h', 'price_change_24h', 'price_change_percentage_24h', 'market_cap_change_24h', 'market_cap_change_percentage_24h', 'circulating_supply', 'total_supply', 'max_supply', 'ath', 'ath_change_percentage', 'ath_date', 'atl', 'atl_change_percentage', 'atl_date', 'roi', 'last_updated', 'sparkline_in_7d', 'timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Head:\n",
            " | id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                              | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:---------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116641        | 2321968740673 |                 1 |             2321968740673 |    48267959169 |  116947    | 111764        |      4393.85       |                       3.91444 |             8.71636e+10 |                            3.90028 |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -6.38422 | 2025-08-14T00:37:02.582Z | 67.81       |        171269           | 2013-07-06T00:00:00.000Z |                                                                                  | 2025-08-22T17:14:55.235Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:15:24.823508+00:00 | 116641        |   nan |      nan |    nan |        nan |        nan |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4791.23     |  576805000840 |                 2 |              576805000840 |    51748865441 |    4772.99 |   4211.66     |       575.22       |                      13.6437  |             6.79011e+10 |                           13.3426  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -2.63342 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.0969e+06  | 2015-10-20T00:00:00.000Z | {'times': 53.91459545610184, 'currency': 'btc', 'percentage': 5391.459545610184} | 2025-08-22T17:14:55.638Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:15:24.823508+00:00 |   4791.23     |   nan |      nan |    nan |        nan |        nan |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.07     |  181939986461 |                 3 |              306157539672 |     8744552178 |       3.06 |      2.79     |         0.188714   |                       6.55941 |             1.06723e+10 |                            6.23138 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -16.4474  | 2025-07-18T03:40:53.808Z |  0.00268621 |        113318           | 2014-05-22T00:00:00.000Z |                                                                                  | 2025-08-22T17:14:57.909Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:15:24.823508+00:00 |      3.07     |   nan |      nan |    nan |        nan |        nan |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999977 |  167062258976 |                 4 |              167062258976 |   107100795787 |       1    |      0.999707 |         0.00012471 |                       0.01247 |             2.76899e+07 |                            0.01658 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4193  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6669      | 2015-03-02T00:00:00.000Z |                                                                                  | 2025-08-22T17:14:59.397Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:15:24.823508+00:00 |      0.999977 |   nan |      nan |    nan |        nan |        nan |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      886.17     |  123410684083 |                 5 |              123410684083 |     1972388341 |     885.82 |    835.34     |        47.73       |                       5.6921  |             6.648e+09   |                            5.6936  |          1.39287e+08 |    1.39287e+08 |      2e+08   |    884.63 |                -0.03006 | 2025-08-22T16:45:53.883Z |  0.0398177  |             2.22093e+06 | 2017-10-19T00:00:00.000Z |                                                                                  | 2025-08-22T17:14:51.907Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:15:24.823508+00:00 |    886.17     |   nan |      nan |    nan |        nan |        nan |\n",
            "------------------------------------------------------\n",
            "\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data.\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data.\n",
            "\n",
            "--- Inspecting history_df_ind before signal generation ---\n",
            "Columns: ['timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Head:\n",
            " | timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan |\n",
            "--------------------------------------------------------\n",
            "\n",
            "Generating signals for bitcoin historical data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Historical signals generated and saved.\n",
            "\n",
            "--- Results ---\n",
            "Historical signals (if generated):\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "\n",
            "Top 10 snapshot signals:\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34df3806"
      },
      "source": [
        "# signals/generate_signals.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "import numpy as np\n",
        "\n",
        "def generate_signal(df):\n",
        "    \"\"\"\n",
        "    Generates trading signals based on technical indicators for time-series data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).\n",
        "                           Assumes DataFrame is sorted by timestamp.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'signal' column added.\n",
        "    \"\"\"\n",
        "    # Ensure the DataFrame has required columns\n",
        "    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        # This should not happen if compute_indicators ran successfully, but as a safeguard\n",
        "        print(\"Error: Missing required columns for signal generation.\")\n",
        "        df['signal'] = 'HOLD'\n",
        "        return df[['signal']]\n",
        "\n",
        "    # Initialize signal column\n",
        "    df['signal'] = 'HOLD'\n",
        "\n",
        "    # Find the first index where all required indicators are not NaN\n",
        "    first_valid_index = df.dropna(subset=required_cols).index.min()\n",
        "\n",
        "    if pd.isna(first_valid_index):\n",
        "         print(\"Warning: All required indicator values are NaN. Returning HOLD signals.\")\n",
        "         # Set all signals to HOLD if no valid indicator rows are found\n",
        "         df['signal'] = 'HOLD'\n",
        "         return df[['signal']]\n",
        "\n",
        "    first_valid_iloc = df.index.get_loc(first_valid_index)\n",
        "\n",
        "    # Apply signal logic based on technical indicators, starting from the first valid index\n",
        "    for i in range(first_valid_iloc, len(df)):\n",
        "        current_row = df.iloc[i]\n",
        "        # Get previous row if available and if previous indicator values are also valid\n",
        "        prev_row = df.iloc[i-1] if i > 0 and not df.iloc[i-1][required_cols].isnull().any() else None\n",
        "\n",
        "        if prev_row is None:\n",
        "             # Cannot compute signals requiring valid previous values\n",
        "             df.loc[current_row.name, 'signal'] = 'HOLD'\n",
        "             continue\n",
        "\n",
        "        signals = []\n",
        "\n",
        "        # RSI signals (using current and previous crossing thresholds)\n",
        "        if current_row['rsi'] < 30 and prev_row['rsi'] >= 30:\n",
        "            signals.append(\"BUY_RSI_CROSS\")\n",
        "        elif current_row['rsi'] > 70 and prev_row['rsi'] <= 70:\n",
        "            signals.append(\"SELL_RSI_CROSS\")\n",
        "\n",
        "        # RSI extreme conditions (may add to existing signals)\n",
        "        if current_row['rsi'] < 20:\n",
        "             signals.append(\"EXTREME_OVERSOLD\")\n",
        "        elif current_row['rsi'] > 80:\n",
        "             signals.append(\"EXTREME_OVERBOUGHT\")\n",
        "\n",
        "        # MACD signals (using macd_diff crossing 0)\n",
        "        if current_row['macd_diff'] > 0 and prev_row['macd_diff'] <= 0:\n",
        "            signals.append(\"BUY_MACD_CROSS\")\n",
        "        elif current_row['macd_diff'] < 0 and prev_row['macd_diff'] >= 0:\n",
        "            signals.append(\"SELL_MACD_CROSS\")\n",
        "\n",
        "        # Bollinger Bands signals (breakouts)\n",
        "        if current_row['close'] > current_row['bb_upper'] and prev_row['close'] <= prev_row['bb_upper']:\n",
        "            signals.append(\"BREAKOUT_UPPER\")\n",
        "        elif current_row['close'] < current_row['bb_lower'] and prev_row['close'] >= prev_row['bb_lower']:\n",
        "            signals.append(\"BREAKOUT_LOWER\")\n",
        "\n",
        "        # Update the signal for the current row\n",
        "        df.loc[current_row.name, 'signal'] = \", \".join(signals) if signals else \"HOLD\"\n",
        "\n",
        "\n",
        "    # For rows before the first valid indicator index, set signal to HOLD\n",
        "    if first_valid_iloc > 0:\n",
        "        df.loc[df.index[:first_valid_iloc], 'signal'] = 'HOLD'\n",
        "\n",
        "\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame"
      ],
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6b109455",
        "outputId": "e01af13d-c8e6-4bda-e66d-4bae4a03d958"
      },
      "source": [
        "# Execute the main script to test the updated signal generation logic with NaN handling.\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data.\n",
            "\n",
            "--- Inspecting df_ind_top10 before signal generation ---\n",
            "Columns: ['id', 'symbol', 'name', 'image', 'current_price', 'market_cap', 'market_cap_rank', 'fully_diluted_valuation', 'total_volume', 'high_24h', 'low_24h', 'price_change_24h', 'price_change_percentage_24h', 'market_cap_change_24h', 'market_cap_change_percentage_24h', 'circulating_supply', 'total_supply', 'max_supply', 'ath', 'ath_change_percentage', 'ath_date', 'atl', 'atl_change_percentage', 'atl_date', 'roi', 'last_updated', 'sparkline_in_7d', 'timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Head:\n",
            " | id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                              | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:---------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116623        | 2321968740673 |                 1 |             2321968740673 |    48146440417 |  116947    | 111764        |      4376.16       |                       3.89868 |             8.71636e+10 |                            3.90028 |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -6.38422 | 2025-08-14T00:37:02.582Z | 67.81       |        171269           | 2013-07-06T00:00:00.000Z |                                                                                  | 2025-08-22T17:12:52.805Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:14:48.859344+00:00 | 116623        |   nan |      nan |    nan |        nan |        nan |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4778.53     |  576805000840 |                 2 |              576805000840 |    50405362014 |    4772.99 |   4211.66     |       562.53       |                      13.3426  |             6.79011e+10 |                           13.3426  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -2.63342 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.0969e+06  | 2015-10-20T00:00:00.000Z | {'times': 53.77867097378891, 'currency': 'btc', 'percentage': 5377.867097378891} | 2025-08-22T17:12:53.161Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:14:48.859344+00:00 |   4778.53     |   nan |      nan |    nan |        nan |        nan |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.06     |  181939986461 |                 3 |              306157539672 |     8705992840 |       3.06 |      2.79     |         0.185018   |                       6.43096 |             1.06723e+10 |                            6.23138 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -16.4474  | 2025-07-18T03:40:53.808Z |  0.00268621 |        113318           | 2014-05-22T00:00:00.000Z |                                                                                  | 2025-08-22T17:12:49.764Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:14:48.859344+00:00 |      3.06     |   nan |      nan |    nan |        nan |        nan |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999983 |  167062258976 |                 4 |              167062258976 |   106613537094 |       1    |      0.999707 |         0.00013081 |                       0.01308 |             2.76899e+07 |                            0.01658 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4193  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6669      | 2015-03-02T00:00:00.000Z |                                                                                  | 2025-08-22T17:12:49.795Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:14:48.859344+00:00 |      0.999983 |   nan |      nan |    nan |        nan |        nan |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      886.01     |  123410684083 |                 5 |              123410684083 |     1967811181 |     885.82 |    835.34     |        47.57       |                       5.67303 |             6.648e+09   |                            5.6936  |          1.39287e+08 |    1.39287e+08 |      2e+08   |    884.63 |                -0.03006 | 2025-08-22T16:45:53.883Z |  0.0398177  |             2.22093e+06 | 2017-10-19T00:00:00.000Z |                                                                                  | 2025-08-22T17:12:49.379Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:14:48.859344+00:00 |    886.01     |   nan |      nan |    nan |        nan |        nan |\n",
            "------------------------------------------------------\n",
            "\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data.\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data.\n",
            "\n",
            "--- Inspecting history_df_ind before signal generation ---\n",
            "Columns: ['timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Head:\n",
            " | timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan |\n",
            "--------------------------------------------------------\n",
            "\n",
            "Generating signals for bitcoin historical data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Historical signals generated and saved.\n",
            "\n",
            "--- Results ---\n",
            "Historical signals (if generated):\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "\n",
            "Top 10 snapshot signals:\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7467eb2e"
      },
      "source": [
        "# signals/generate_signals.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "import numpy as np\n",
        "\n",
        "def generate_signal(df):\n",
        "    \"\"\"\n",
        "    Generates trading signals based on technical indicators for time-series data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).\n",
        "                           Assumes DataFrame is sorted by timestamp.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'signal' column added.\n",
        "    \"\"\"\n",
        "    # Ensure the DataFrame has required columns\n",
        "    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        # This should not happen if compute_indicators ran successfully, but as a safeguard\n",
        "        print(\"Error: Missing required columns for signal generation.\")\n",
        "        df['signal'] = 'HOLD'\n",
        "        return df[['signal']]\n",
        "\n",
        "    # Initialize signal column\n",
        "    df['signal'] = 'HOLD'\n",
        "\n",
        "    # Check if there are enough rows for indicator calculation to be potentially valid\n",
        "    # RSI needs at least 14 periods, others need 20 based on default window sizes\n",
        "    # Check for at least the largest window size for meaningful signals\n",
        "    min_data_points = 20 # Based on BB and EMA window size\n",
        "\n",
        "    if len(df) < min_data_points:\n",
        "         print(f\"Warning: Not enough data points ({len(df)}) for meaningful signal generation (requires at least {min_data_points}). Returning HOLD signals.\")\n",
        "         return df[['signal']]\n",
        "\n",
        "    # Apply signal logic based on technical indicators, starting from where indicators are likely valid\n",
        "    # Indicators like RSI, MACD, BB will have NaNs for the first period(s) based on their window.\n",
        "    # We can start applying logic from the first row where all required indicators are not NaN.\n",
        "    first_valid_index = df.dropna(subset=required_cols).index.min()\n",
        "\n",
        "    if pd.isna(first_valid_index):\n",
        "         print(\"Warning: All indicator values are NaN. Returning HOLD signals.\")\n",
        "         return df[['signal']]\n",
        "\n",
        "    first_valid_iloc = df.index.get_loc(first_valid_index)\n",
        "\n",
        "\n",
        "    for i in range(first_valid_iloc, len(df)):\n",
        "        current_row = df.iloc[i]\n",
        "        prev_row = df.iloc[i-1] if i > 0 else None # Get previous row if available\n",
        "\n",
        "        if prev_row is None:\n",
        "             # Cannot compute signals requiring previous values for the very first row\n",
        "             continue\n",
        "\n",
        "        signals = []\n",
        "\n",
        "        # RSI signals (using current and previous crossing thresholds)\n",
        "        if current_row['rsi'] < 30 and prev_row['rsi'] >= 30:\n",
        "            signals.append(\"BUY_RSI_CROSS\")\n",
        "        elif current_row['rsi'] > 70 and prev_row['rsi'] <= 70:\n",
        "            signals.append(\"SELL_RSI_CROSS\")\n",
        "\n",
        "        # RSI extreme conditions (may add to existing signals)\n",
        "        if current_row['rsi'] < 20:\n",
        "             signals.append(\"EXTREME_OVERSOLD\")\n",
        "        elif current_row['rsi'] > 80:\n",
        "             signals.append(\"EXTREME_OVERBOUGHT\")\n",
        "\n",
        "\n",
        "        # MACD signals (using macd_diff crossing 0)\n",
        "        if current_row['macd_diff'] > 0 and prev_row['macd_diff'] <= 0:\n",
        "            signals.append(\"BUY_MACD_CROSS\")\n",
        "        elif current_row['macd_diff'] < 0 and prev_row['macd_diff'] >= 0:\n",
        "            signals.append(\"SELL_MACD_CROSS\")\n",
        "\n",
        "        # Bollinger Bands signals (breakouts)\n",
        "        if current_row['close'] > current_row['bb_upper'] and prev_row['close'] <= prev_row['bb_upper']:\n",
        "            signals.append(\"BREAKOUT_UPPER\")\n",
        "        elif current_row['close'] < current_row['bb_lower'] and prev_row['close'] >= prev_row['bb_lower']:\n",
        "            signals.append(\"BREAKOUT_LOWER\")\n",
        "\n",
        "        # Update the signal for the current row\n",
        "        df.loc[current_row.name, 'signal'] = \", \".join(signals) if signals else \"HOLD\"\n",
        "\n",
        "\n",
        "    # For rows before the first valid indicator index, set signal to HOLD\n",
        "    if first_valid_iloc > 0:\n",
        "        df.loc[df.index[:first_valid_iloc], 'signal'] = 'HOLD'\n",
        "\n",
        "\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame\n"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "339091ed",
        "outputId": "e85ecada-591c-4fe4-cabe-0a1a1b60a55c"
      },
      "source": [
        "# Modify main.py to print DataFrame info before generating signals\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from indicators.ta_utils import compute_indicators\n",
        "from signals.generate_signals import generate_signal\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data = get_top_coins()\n",
        "        df_top10 = pd.DataFrame(data)\n",
        "        df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "        df_top10.to_csv(f\"{DATA_DIR}/top10_market_data.csv\", index=False)\n",
        "        print(\"Top 10 data fetched and saved.\")\n",
        "\n",
        "        print(\"Computing technical indicators for top 10 data...\")\n",
        "        # For now, we'll use the current price as 'close' for indicator calculation\n",
        "        df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "        df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "        df_ind_top10.to_csv(f\"{DATA_DIR}/top10_with_indicators.csv\", index=False)\n",
        "        print(\"Indicators computed and saved for top 10 data.\")\n",
        "\n",
        "        # Add print statements to inspect df_ind_top10\n",
        "        print(\"\\\\n--- Inspecting df_ind_top10 before signal generation ---\")\n",
        "        print(\"Columns:\", df_ind_top10.columns.tolist())\n",
        "        print(\"Head:\\\\n\", df_ind_top10.head().to_markdown(index=False))\n",
        "        print(\"------------------------------------------------------\\\\n\")\n",
        "\n",
        "\n",
        "        print(\"Generating signals for top 10 snapshot data...\")\n",
        "        # Note: The current generate_signal function expects a single row or a dataframe\n",
        "        # with sufficient historical data. For this initial test with only current data,\n",
        "        # the signals will likely be 'HOLD' or based on limited info.\n",
        "        df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data\n",
        "        df_signals_top10.to_csv(f\"{DATA_DIR}/top10_signals.csv\", index=False)\n",
        "        print(\"Signals generated and saved for top 10 snapshot data.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing top 10 data: {e}. Skipping top 10 data processing.\")\n",
        "        df_signals_top10 = pd.DataFrame() # Ensure df_signals_top10 is defined even on error\n",
        "\n",
        "    # --- Historical Data Processing with Error Handling ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals = pd.DataFrame() # Initialize as empty DataFrame\n",
        "\n",
        "    print(f\"Attempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "\n",
        "        if history_data is not None: # Check if data fetching was successful\n",
        "            print(\"Historical data fetched.\")\n",
        "\n",
        "            # Process historical price data into a DataFrame\n",
        "            if \"prices\" not in history_data:\n",
        "                print(f\"Warning: 'prices' key not found in historical data for {coin_id}. Response keys: {history_data.keys()}\")\n",
        "            else:\n",
        "                history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\n",
        "                history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "                print(\"Historical data processed into DataFrame.\")\n",
        "\n",
        "                # Compute indicators on the historical data DataFrame\n",
        "                print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "                history_df_ind = compute_indicators(history_df.copy())\n",
        "                history_df_ind.to_csv(f\"{DATA_DIR}/{coin_id}_historical_with_indicators.csv\", index=False)\n",
        "                print(\"Indicators computed and saved for historical data.\")\n",
        "\n",
        "                # Add print statements to inspect history_df_ind\n",
        "                print(\"\\\\n--- Inspecting history_df_ind before signal generation ---\")\n",
        "                print(\"Columns:\", history_df_ind.columns.tolist())\n",
        "                print(\"Head:\\\\n\", history_df_ind.head().to_markdown(index=False))\n",
        "                print(\"--------------------------------------------------------\\\\n\")\n",
        "\n",
        "\n",
        "                # Generate signals on the historical data\n",
        "                print(f\"Generating signals for {coin_id} historical data...\")\n",
        "                history_df_signals = generate_signal(history_df_ind)\n",
        "                history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)\n",
        "                print(\"Historical signals generated and saved.\")\n",
        "        else:\n",
        "             print(\"Historical data fetching failed. Skipping historical data processing.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing historical data for {coin_id}: {e}. Skipping historical signals.\")\n",
        "        # history_df_signals is already initialized as empty\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(\"\\\\n--- Results ---\")\n",
        "    print(\"Historical signals (if generated):\")\n",
        "    if not history_df_signals.empty:\n",
        "        print(history_df_signals.head().to_markdown(index=False)) # Use to_markdown for better printing\n",
        "    else:\n",
        "        print(\"No historical signals generated.\")\n",
        "\n",
        "    print(\"\\\\nTop 10 snapshot signals:\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False)) # Use to_markdown for better printing\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)\n",
        "\n",
        "# Execute the script to inspect the DataFrames\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data.\n",
            "\n",
            "--- Inspecting df_ind_top10 before signal generation ---\n",
            "Columns: ['id', 'symbol', 'name', 'image', 'current_price', 'market_cap', 'market_cap_rank', 'fully_diluted_valuation', 'total_volume', 'high_24h', 'low_24h', 'price_change_24h', 'price_change_percentage_24h', 'market_cap_change_24h', 'market_cap_change_percentage_24h', 'circulating_supply', 'total_supply', 'max_supply', 'ath', 'ath_change_percentage', 'ath_date', 'atl', 'atl_change_percentage', 'atl_date', 'roi', 'last_updated', 'sparkline_in_7d', 'timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Head:\n",
            " | id          | symbol   | name     | image                                                                                       |   current_price |    market_cap |   market_cap_rank |   fully_diluted_valuation |   total_volume |   high_24h |       low_24h |   price_change_24h |   price_change_percentage_24h |   market_cap_change_24h |   market_cap_change_percentage_24h |   circulating_supply |   total_supply |   max_supply |       ath |   ath_change_percentage | ath_date                 |         atl |   atl_change_percentage | atl_date                 | roi                                                                              | last_updated             | sparkline_in_7d                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                  | timestamp                        |         close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:------------|:---------|:---------|:--------------------------------------------------------------------------------------------|----------------:|--------------:|------------------:|--------------------------:|---------------:|-----------:|--------------:|-------------------:|------------------------------:|------------------------:|-----------------------------------:|---------------------:|---------------:|-------------:|----------:|------------------------:|:-------------------------|------------:|------------------------:|:-------------------------|:---------------------------------------------------------------------------------|:-------------------------|:---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|:---------------------------------|--------------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| bitcoin     | btc      | Bitcoin  | https://coin-images.coingecko.com/coins/images/1/large/bitcoin.png?1696501400               |   116623        | 2321968740673 |                 1 |             2321968740673 |    48146440417 |  116947    | 111764        |      4376.16       |                       3.89868 |             8.71636e+10 |                            3.90028 |          1.991e+07   |    1.991e+07   |      2.1e+07 | 124128    |                -6.38422 | 2025-08-14T00:37:02.582Z | 67.81       |        171269           | 2013-07-06T00:00:00.000Z |                                                                                  | 2025-08-22T17:12:52.805Z | {'price': [119094.80293447574, 118775.09135434593, 117912.93866816521, 117387.54999240847, 117367.4347951849, 117407.49931529992, 117670.62799352725, 117219.33341108754, 117000.41118669232, 117371.50148995162, 117372.65388438936, 117805.21195916239, 117396.51852350791, 117800.72549274012, 117954.97333880374, 117569.54211594824, 117533.98734372148, 117903.7017188172, 117696.32142573298, 117451.02331171713, 117663.79245540539, 117657.13593094652, 117411.10125498683, 117466.55165291626, 117448.5772878559, 117816.83364875686, 117711.81257679683, 117817.33458730514, 117745.29749254188, 117821.00630637373, 117763.14050380496, 117748.69876342453, 117734.22808778555, 117709.02188428598, 117503.9270349045, 117484.75585417039, 117397.54362420287, 117369.57653063927, 117521.58845065876, 117663.3727826123, 117784.9202897309, 118146.94391682425, 118082.79247968765, 118027.58938661608, 118094.96996974589, 118375.95123513404, 118348.89228201286, 118359.5056951518, 118384.92193419846, 118185.47472943665, 118451.62072881311, 118213.36966862573, 118303.92136200068, 118019.83988133041, 117975.7857182899, 117800.3890012445, 117659.9232573146, 117744.57938981004, 117792.21371595125, 118004.76868885547, 117519.35606632655, 117293.77662722742, 116457.32687457414, 115762.72294417131, 115355.15362701363, 115582.39765044459, 115677.2437142178, 115295.02327023851, 115338.9847735173, 115197.40503646056, 115074.7800568399, 115124.87334587987, 115044.56004926866, 115527.0309311352, 114995.26995245655, 115648.19465225624, 116256.12045700087, 116083.85779306894, 116717.50032215302, 116612.61591671837, 116359.64918942218, 116467.14070857807, 116713.43762065737, 116445.12054707026, 116315.1187900144, 116578.69611080748, 115840.82295104749, 115739.41713066881, 114853.79567274198, 115310.22932577619, 115021.97815753656, 114985.89568101108, 115011.33437220617, 115244.10458929573, 115371.1864707095, 115429.16618470901, 115559.44716650811, 115599.61514159056, 115239.4878544111, 114004.88666207601, 113820.33698581933, 113404.06193584246, 113494.55052097653, 112832.75554814276, 113271.63940944368, 113497.50086327353, 113414.51430137384, 113170.10811534988, 112896.32534645073, 113221.30878378011, 113038.63562802887, 113436.86955390326, 113499.88101366491, 113576.63342572509, 113712.34453532855, 113616.7780659848, 113548.32584815667, 113979.37893099306, 113738.87260459366, 113814.60747522612, 113772.5953048606, 113704.65095237858, 112721.52528522459, 113468.71586206947, 113364.1275536759, 113947.58727837629, 114200.26919279918, 113588.02274704837, 114227.65280595022, 114351.92452461013, 114422.93252401563, 114590.45445269886, 114250.43172587751, 114337.7268829338, 114723.4072135875, 113987.89517975388, 113987.13392915655, 113808.26421295329, 113954.38733064673, 113737.02812154038, 113890.13604780457, 113638.22918676953, 113396.1776883503, 113373.80279295826, 113132.28582905016, 112952.51362584245, 113504.91074576437, 113266.26639577647, 112773.38343187496, 112398.3636645793, 112379.88592699269, 112571.655424373, 112023.06158201321, 112564.21553111593, 112561.18367557136, 112468.84756023913, 112414.39987336512, 112570.05901657068, 112817.82613496445, 113266.61693979731, 112933.12517331423, 112787.89866622085, 113216.10154703319, 112999.09282659138, 113108.75506378774, 112993.43834510577, 113074.45957010616, 112521.5479279412]} | 2025-08-22 17:14:04.740866+00:00 | 116623        |   nan |      nan |    nan |        nan |        nan |\n",
            "| ethereum    | eth      | Ethereum | https://coin-images.coingecko.com/coins/images/279/large/ethereum.png?1696501628            |     4778.53     |  576805000840 |                 2 |              576805000840 |    50405362014 |    4772.99 |   4211.66     |       562.53       |                      13.3426  |             6.79011e+10 |                           13.3426  |          1.20708e+08 |    1.20708e+08 |    nan       |   4878.26 |                -2.63342 | 2021-11-10T14:24:19.604Z |  0.432979   |             1.0969e+06  | 2015-10-20T00:00:00.000Z | {'times': 53.77867097378891, 'currency': 'btc', 'percentage': 5377.867097378891} | 2025-08-22T17:12:53.161Z | {'price': [4639.459910312292, 4633.7424645708825, 4554.8381709187515, 4495.27457472361, 4445.927670898998, 4415.829282347378, 4444.639360765793, 4398.416327200253, 4386.035919898839, 4425.195987288076, 4429.387512759475, 4484.038379608297, 4430.532465017841, 4474.621447041862, 4489.322103036408, 4448.557665954041, 4437.237650056369, 4454.865585353516, 4427.505655482169, 4424.581230427647, 4457.974429487894, 4450.17090556177, 4408.254855782439, 4408.192105737334, 4388.622203435841, 4411.922207103838, 4398.197957149771, 4415.321720059268, 4402.30918266725, 4411.294936968012, 4406.7748565725105, 4414.239172906332, 4421.106288010189, 4432.431740524485, 4418.157983574278, 4427.9779936676905, 4423.427841805725, 4417.165406421642, 4411.989814369911, 4435.914439706703, 4454.422209228155, 4482.575648159058, 4476.502884147134, 4461.8791539730055, 4470.949341242234, 4530.340506274097, 4540.224310952337, 4560.55754343139, 4554.757519948156, 4540.418094755189, 4537.111670910976, 4554.0672683810435, 4563.720883287974, 4530.230439492318, 4529.979455082307, 4493.546452783142, 4459.252520879271, 4476.649368894867, 4480.210282883734, 4530.179357292165, 4485.033789094146, 4459.5882776334265, 4401.33774918774, 4342.449708890175, 4320.143571558077, 4341.63989960528, 4309.278499635083, 4253.453671625841, 4268.799744597591, 4254.842586503113, 4294.937691080915, 4266.53266038923, 4277.686035026836, 4343.308886687448, 4302.506769313587, 4334.550089566177, 4346.036044031518, 4325.489689418948, 4368.541033339547, 4372.344745518553, 4355.791274883982, 4338.808438274305, 4356.157029331578, 4344.315720750668, 4317.2757573282315, 4345.627728915809, 4295.900339297719, 4288.3505536264565, 4220.472426904265, 4227.445219814358, 4234.761347893579, 4239.598140095132, 4234.140358684951, 4258.534261160376, 4283.655570161312, 4287.017474947467, 4306.89780229794, 4306.138414662149, 4292.479206363202, 4200.741955570188, 4192.074136734997, 4147.946675037544, 4185.513896488857, 4136.557636095331, 4150.5376199983675, 4158.69111387679, 4150.537934666027, 4133.66525923688, 4080.647740086776, 4107.541781461217, 4116.6160388609005, 4136.696358714024, 4139.60926834396, 4170.183548147729, 4182.476932316323, 4181.31557631135, 4190.950558299099, 4236.646696712759, 4222.761287837913, 4227.3404712096435, 4200.587604763044, 4206.723705002232, 4139.006984573057, 4217.767397510883, 4254.971830891948, 4302.4548697326045, 4342.094424018613, 4306.099448652301, 4343.129551827904, 4355.770424892453, 4356.544706287441, 4361.716783581947, 4335.095011843836, 4320.1980430012945, 4333.961726754763, 4292.4304098328475, 4297.465197334503, 4296.585571353684, 4313.3986585238445, 4298.708337862511, 4309.4486339886635, 4290.869649543399, 4276.708693064774, 4291.683586094649, 4279.936860223659, 4245.421345174224, 4266.5729224797005, 4271.484138188116, 4242.451908736863, 4219.881963694916, 4241.536824509093, 4261.992757097628, 4225.847540684238, 4248.233004142016, 4244.163926928581, 4249.305258031314, 4224.438592580698, 4238.056205858333, 4257.145344757436, 4290.755690689207, 4277.367653632968, 4279.657307435543, 4289.600459738463, 4294.924937363455, 4340.875783582629, 4332.522512190271, 4328.1503321390355, 4296.813304664316]}                                                                                                                                       | 2025-08-22 17:14:04.740866+00:00 |   4778.53     |   nan |      nan |    nan |        nan |        nan |\n",
            "| ripple      | xrp      | XRP      | https://coin-images.coingecko.com/coins/images/44/large/xrp-symbol-white-128.png?1696501442 |        3.06     |  181939986461 |                 3 |              306157539672 |     8705992840 |       3.06 |      2.79     |         0.185018   |                       6.43096 |             1.06723e+10 |                            6.23138 |          5.94185e+10 |    9.99858e+10 |      1e+11   |      3.65 |               -16.4474  | 2025-07-18T03:40:53.808Z |  0.00268621 |        113318           | 2014-05-22T00:00:00.000Z |                                                                                  | 2025-08-22T17:12:49.764Z | {'price': [3.1125098270197817, 3.118946532293096, 3.086482341579628, 3.06482081764889, 3.0331740556044866, 3.0391469124520953, 3.0604939571486094, 3.0672931310979377, 3.066689061053934, 3.07793054677591, 3.0810095594498117, 3.0981535122821406, 3.077994496413232, 3.0930378921917985, 3.0952139504149345, 3.092816158051392, 3.112728724272325, 3.110256522156864, 3.0979339365426606, 3.1075924402205097, 3.124595383055742, 3.138896097851856, 3.137738466833966, 3.129965167268773, 3.108037572603128, 3.1145877172121983, 3.1103356072187047, 3.114131474135331, 3.10534922952423, 3.1080543556233367, 3.0979257319840734, 3.100008299624608, 3.1031367375475574, 3.1049296364630266, 3.0976421029138703, 3.107048682148378, 3.1071822177457036, 3.098599957511143, 3.1048847429794515, 3.119603098439341, 3.122507224833393, 3.133765445381719, 3.124821070498628, 3.1168630470634513, 3.1187858215782085, 3.1316372692056578, 3.1292596681057594, 3.1243337436220013, 3.1298519605047668, 3.117846550167106, 3.128219765171259, 3.1198583215826674, 3.1353717447405445, 3.1198303414749917, 3.1119051915262346, 3.100079296595621, 3.096366332606467, 3.0970938540287434, 3.1019111512596567, 3.111156763735718, 3.093706595371064, 3.0852760538992676, 3.0530446475734823, 3.0248794862839543, 2.990034082344401, 2.9993262244421013, 2.991063547430056, 2.970886920442055, 2.96685352486914, 2.9644625900016632, 2.969132079940602, 2.9695767286389874, 2.970279106147803, 2.987659684466103, 2.9794935645770613, 2.9942357937182633, 3.0169109826193092, 3.0089624462137246, 3.0537897135690324, 3.0670084048009763, 3.0788803666295785, 3.0764929042430307, 3.0835604727826755, 3.0729126742643746, 3.0632481501001867, 3.064692125493376, 3.0507328564251983, 3.0401547876000716, 2.992091273149223, 3.000427971237309, 3.0192290834587765, 3.020142383223054, 3.004749796661399, 3.011262606321711, 3.017843113704447, 3.013414486860169, 3.0136008724150614, 3.0132743542115197, 3.001994626257956, 2.9458392328576837, 2.936520945667197, 2.913834417269839, 2.9280484234128896, 2.899412011840234, 2.903954001984535, 2.9018125820842164, 2.8965700350107686, 2.8928527810909945, 2.8587967541266495, 2.8827203392452185, 2.8743082508327267, 2.8871128653090823, 2.8878599827892533, 2.8981273417134887, 2.898948040374685, 2.886473895321761, 2.8768990353099055, 2.9075765636663338, 2.8887920863957133, 2.902522969168224, 2.88293436214836, 2.8959135655333577, 2.8449349747666735, 2.88937139862554, 2.904821467982078, 2.9185635089671833, 2.9403753761098925, 2.9243637393557167, 2.9700986515221497, 2.9718366956842726, 2.9660549632849156, 2.969895131715341, 2.9519099360463503, 2.943772847588836, 2.9463679722848144, 2.9216358790558234, 2.932311409204809, 2.93161855009327, 2.9294035197349717, 2.9088930326830935, 2.9072786443201997, 2.903947719983922, 2.8952399135844042, 2.9001817237067664, 2.9003938371462015, 2.89401800909732, 2.9096547552100396, 2.8968223690438264, 2.8874217677996596, 2.893660281040788, 2.902751470997465, 2.91348076273889, 2.8676168828853648, 2.87593057399528, 2.8823087352922836, 2.8722890877492717, 2.8498230079250755, 2.850352597863775, 2.859106620718366, 2.8652521498550554, 2.8307054995765912, 2.8553648817806585, 2.8702739252357903, 2.8522113301158516, 2.8667540696342195, 2.864890018005256, 2.860726762947461, 2.8312287425253175]}                                                     | 2025-08-22 17:14:04.740866+00:00 |      3.06     |   nan |      nan |    nan |        nan |        nan |\n",
            "| tether      | usdt     | Tether   | https://coin-images.coingecko.com/coins/images/325/large/Tether.png?1696501661              |        0.999983 |  167062258976 |                 4 |              167062258976 |   106613537094 |       1    |      0.999707 |         0.00013081 |                       0.01308 |             2.76899e+07 |                            0.01658 |          1.67065e+11 |    1.67065e+11 |    nan       |      1.32 |               -24.4193  | 2018-07-24T00:00:00.000Z |  0.572521   |            74.6669      | 2015-03-02T00:00:00.000Z |                                                                                  | 2025-08-22T17:12:49.795Z | {'price': [1.0005866174726743, 1.0006444577450908, 1.0006151802595562, 1.000423095471907, 1.0004864880509496, 1.0005283697074856, 1.0005612077923312, 1.0005295316205292, 1.0005782138095, 1.00055612238563, 1.0006594649852198, 1.0006974834479172, 1.0006533939618045, 1.0006892836919499, 1.0006883601094225, 1.0007083846887117, 1.000687550842173, 1.0007315580895098, 1.0007735243635585, 1.0007548195841915, 1.0007185387803463, 1.000699433543562, 1.0006358388822028, 1.0006316157368476, 1.0006683983670068, 1.000637560618589, 1.000625317281958, 1.0005558063135391, 1.000588120738585, 1.0006306026692637, 1.000629867776359, 1.0005617214803089, 1.0005674976937737, 1.000610913037258, 1.0005508663569074, 1.000582958920905, 1.000587223565168, 1.000594656665913, 1.00056100594982, 1.000569238776447, 1.0005385539928775, 1.0006142293760345, 1.0005950705818285, 1.0006526921101255, 1.0006376406399338, 1.0005951471956986, 1.0004077602350234, 1.00040419848505, 1.0004209374574338, 1.000322014547155, 1.0004645405974324, 1.0004280499594722, 1.000412344542045, 1.0004947034507927, 1.0004650485820847, 1.0005719611049722, 1.000564013376131, 1.0006248585237916, 1.0006649684450735, 1.000677844972997, 1.000653225764489, 1.0006653916544928, 1.000677190272163, 1.0007003841579467, 1.0006842736387558, 1.000675844432911, 1.0007004704732512, 1.000672620202369, 1.0006174656078497, 1.0006016642010902, 1.000581746135657, 1.0006195997463616, 1.0006128357731257, 1.0005490575411482, 1.000634627385272, 1.0005181513045818, 1.00056040210528, 1.0005787098886125, 1.0005711159576447, 1.0004970270421694, 1.0004834907199542, 1.0004320629001546, 1.0004734719661212, 1.0004921127816526, 1.000442851117242, 1.0004057157870765, 1.000446837163824, 1.0004178514348616, 1.0004133289718677, 1.0003845482781784, 1.0003596172176974, 1.000309264026071, 1.0003440599462665, 1.0003034049200545, 1.000398517186211, 1.0002637737127833, 1.0002885950871416, 1.0002387412405715, 1.0002666409192746, 1.0001300765212744, 1.0000351770314915, 1.0000353221860625, 1.0000348859474737, 0.9999285624647518, 0.9999660698057407, 0.9999422730648468, 0.9999055944905754, 0.9999388047292298, 0.9998226196770188, 0.9999640032956355, 0.9999233064259442, 0.9999274177939282, 0.9999414846934832, 0.9999244626177595, 0.9999172201906705, 0.9999803811490791, 1.000003377937723, 0.9999876996429067, 0.9999088888326523, 0.9999369843324727, 0.9999244915480815, 1.0001621421144469, 0.9999810514212268, 1.000007999108997, 1.0000011942078653, 0.9999870717126254, 1.0000648330390844, 1.0000622229869722, 0.9999902345762285, 0.9999939585520727, 1.0000350301365089, 1.0001198518341983, 1.000080473898308, 1.0001427942736523, 1.0000487163060483, 1.000043738329857, 1.0000242564818977, 0.9999683163541729, 0.9999504785259945, 0.999963992318641, 0.9999830725897598, 0.9999839682704119, 0.9999612996010786, 0.9999358312068576, 0.999862446666439, 0.9998733196991211, 0.9998820355832974, 0.9998793683243684, 0.9998655684652213, 0.9998592120605018, 0.9998304712121983, 0.9998528612013969, 0.9997434376584784, 0.9997165918194666, 0.9997575108702322, 0.9997327693590792, 0.9997672407796808, 0.9997845762020571, 0.9997555607198061, 0.9997471338354323, 0.9997924252608575, 0.9997599410659995, 0.999750116993641, 0.9997901248237355, 0.9997881916494108, 0.9998831972214022, 0.9998658625977284, 0.9998352347289513]}                  | 2025-08-22 17:14:04.740866+00:00 |      0.999983 |   nan |      nan |    nan |        nan |        nan |\n",
            "| binancecoin | bnb      | BNB      | https://coin-images.coingecko.com/coins/images/825/large/bnb-icon2_2x.png?1696501970        |      886.01     |  123410684083 |                 5 |              123410684083 |     1967811181 |     885.82 |    835.34     |        47.57       |                       5.67303 |             6.648e+09   |                            5.6936  |          1.39287e+08 |    1.39287e+08 |      2e+08   |    884.63 |                -0.03006 | 2025-08-22T16:45:53.883Z |  0.0398177  |             2.22093e+06 | 2017-10-19T00:00:00.000Z |                                                                                  | 2025-08-22T17:12:49.379Z | {'price': [844.9150105318392, 845.9801821389901, 834.9503298794888, 827.5766483557207, 819.016945317668, 819.0786350713237, 825.183246612455, 822.260463471477, 820.8777795691947, 824.9961882334959, 826.6134268936312, 831.8670693869968, 827.6300963722929, 834.2358416275658, 836.6263608549493, 832.4786503346238, 832.3853493672507, 836.7208834912805, 833.6938525141346, 832.2950788762221, 836.5250434293167, 836.2756499580541, 832.1722438736956, 828.9318014509815, 827.5622202493984, 830.9163925714116, 829.400669776495, 834.3102456946218, 832.5687088433455, 834.5714017634576, 834.1414275552055, 834.9494163415801, 835.2668249475036, 836.3268641954825, 835.8894565322364, 837.1729184414153, 839.2697859525013, 843.7369995492611, 849.1183433552208, 846.4347131999659, 845.6846858257562, 848.3869920012432, 848.9401104276998, 849.753827523214, 854.397281744164, 855.0826673662083, 861.6859116537981, 860.1388737728392, 862.0580046327117, 858.8875615765534, 858.1089471071645, 858.6283078263724, 860.6987132775639, 857.3539950486725, 856.1588348957921, 853.1595191504288, 853.3954424848702, 853.0887789710251, 854.1472704572792, 857.8368987320662, 856.7211993089303, 855.4323959961544, 848.0571086207498, 840.0024241184877, 835.1305370249867, 838.2110007923064, 833.9746321454672, 827.3189684285378, 830.9130888832407, 832.4330342859939, 834.2164517918798, 831.8082523679697, 831.57154720659, 838.4747406680893, 834.7774886456187, 838.5446666150987, 841.8564739432629, 839.3470439458908, 844.9656530473169, 845.5225521947987, 844.8719014414228, 846.4882937534984, 846.2455260006509, 854.0776043543295, 848.2298792758197, 847.3494306924397, 842.9509327568279, 850.6529056170535, 842.9324624826769, 846.8003629134587, 847.1136096083196, 841.6558069693701, 837.8964713226698, 841.4103074301152, 844.7251434003078, 844.5790888986776, 846.4763187849478, 844.4265214374434, 842.6901548747794, 830.0226759951146, 832.3732641643007, 827.6524933578717, 832.8429891056292, 826.6918497932384, 833.3443379189833, 834.2106361446832, 836.2122562651649, 834.1553951349866, 823.1852630685196, 826.8215645815728, 821.782051915435, 829.2665178380814, 836.1969586485891, 835.312577211568, 834.7805416755381, 835.7988440991452, 835.1325606168482, 834.4125774523508, 830.3391362192214, 833.9965830451962, 830.1187228701126, 832.5107674257222, 824.05306082146, 828.6107669926309, 833.13710754773, 841.0554355918849, 846.3201850348853, 852.8051908146339, 868.0134994090276, 873.6992750347687, 877.7647630927869, 876.2256699727225, 869.9903114730301, 874.3726727351823, 878.7452027830327, 868.8212759383243, 867.4289079201611, 864.1267733599758, 863.216375424868, 864.6653999068798, 861.338987102911, 858.3614449242148, 852.5903671219381, 850.4411771768059, 850.7182551978605, 850.4179393443305, 852.1454714047819, 850.1066934091505, 842.742939500666, 839.1880702630049, 842.157095631518, 840.2077504316665, 835.5973689601817, 837.8452963060854, 838.7528257188177, 841.3218194138669, 838.3671602294102, 840.9972747032278, 845.9842100735882, 849.8382046049619, 845.0546916729847, 848.8515573021399, 850.5409599017751, 848.2282683966818, 851.2348762977762, 851.8516455947922, 849.609635283174, 845.3363055742926]}                                                                                                                                                                 | 2025-08-22 17:14:04.740866+00:00 |    886.01     |   nan |      nan |    nan |        nan |        nan |\n",
            "------------------------------------------------------\n",
            "\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data.\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data.\n",
            "\n",
            "--- Inspecting history_df_ind before signal generation ---\n",
            "Columns: ['timestamp', 'close', 'rsi', 'ema_20', 'macd', 'bb_upper', 'bb_lower']\n",
            "Head:\n",
            " | timestamp                  |   close |   rsi |   ema_20 |   macd |   bb_upper |   bb_lower |\n",
            "|:---------------------------|--------:|------:|---------:|-------:|-----------:|-----------:|\n",
            "| 2025-07-23 18:09:13.433000 |  118121 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 19:09:14.123000 |  118402 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 20:08:03.711000 |  118452 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 21:04:35.426000 |  117982 |   nan |      nan |    nan |        nan |        nan |\n",
            "| 2025-07-23 22:02:10.782000 |  118133 |   nan |      nan |    nan |        nan |        nan |\n",
            "--------------------------------------------------------\n",
            "\n",
            "Generating signals for bitcoin historical data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Historical signals generated and saved.\n",
            "\n",
            "--- Results ---\n",
            "Historical signals (if generated):\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "\n",
            "Top 10 snapshot signals:\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8f15d75",
        "outputId": "ee93c745-16b8-43df-a2c7-6a37df78735c"
      },
      "source": [
        "# Execute the main script to test the updated signal generation logic.\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Signals generated and saved for top 10 snapshot data.\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data.\n",
            "Generating signals for bitcoin historical data...\n",
            "Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\n",
            "Historical signals generated and saved.\n",
            "\n",
            "--- Results ---\n",
            "Historical signals (if generated):\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "\n",
            "Top 10 snapshot signals:\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1e43d9a5"
      },
      "source": [
        "# Modify the signals/generate_signals.py file to implement signal logic for time-series data and return a DataFrame.\n",
        "with open('signal_bot/signals/generate_signals.py', 'w') as f:\n",
        "    f.write(\"\"\"# signals/generate_signals.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "\n",
        "def generate_signal(df):\n",
        "    \\\"\\\"\\\"\n",
        "    Generates trading signals based on technical indicators for time-series data.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).\n",
        "                           Assumes DataFrame is sorted by timestamp.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'signal' column added.\n",
        "    \\\"\\\"\\\"\n",
        "    # Ensure the DataFrame has required columns and is not empty\n",
        "    required_cols = ['rsi', 'macd_diff', 'close', 'bb_lower', 'bb_upper']\n",
        "    if df.empty or not all(col in df.columns for col in required_cols):\n",
        "        # If data is insufficient or missing columns, return a DataFrame with 'HOLD' signals\n",
        "        print(\"Warning: Insufficient data or missing columns for signal generation. Returning HOLD signals.\")\n",
        "        df['signal'] = 'HOLD'\n",
        "        return df[['signal']]\n",
        "\n",
        "    # Initialize signal column\n",
        "    df['signal'] = 'HOLD'\n",
        "\n",
        "    # Apply signal logic based on technical indicators\n",
        "    for i in range(1, len(df)): # Start from the second row to check previous values\n",
        "        # RSI signals\n",
        "        if df['rsi'].iloc[i] < 30 and df['rsi'].iloc[i-1] >= 30:\n",
        "            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', BUY_RSI_CROSS').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'BUY_RSI_CROSS'\n",
        "        elif df['rsi'].iloc[i] > 70 and df['rsi'].iloc[i-1] <= 70:\n",
        "            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', SELL_RSI_CROSS').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'SELL_RSI_CROSS'\n",
        "        # RSI extreme conditions (may add to existing signals)\n",
        "        elif df['rsi'].iloc[i] < 20:\n",
        "             df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', EXTREME_OVERSOLD').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'EXTREME_OVERSOLD'\n",
        "        elif df['rsi'].iloc[i] > 80:\n",
        "             df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', EXTREME_OVERBOUGHT').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'EXTREME_OVERBOUGHT'\n",
        "\n",
        "\n",
        "        # MACD signals (using macd_diff crossing 0)\n",
        "        if df['macd_diff'].iloc[i] > 0 and df['macd_diff'].iloc[i-1] <= 0:\n",
        "            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', BUY_MACD_CROSS').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'BUY_MACD_CROSS'\n",
        "        elif df['macd_diff'].iloc[i] < 0 and df['macd_diff'].iloc[i-1] >= 0:\n",
        "            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', SELL_MACD_CROSS').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'SELL_MACD_CROSS'\n",
        "\n",
        "        # Bollinger Bands signals (breakouts)\n",
        "        if df['close'].iloc[i] > df['bb_upper'].iloc[i] and df['close'].iloc[i-1] <= df['bb_upper'].iloc[i-1]:\n",
        "            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', BREAKOUT_UPPER').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'BREAKOUT_UPPER'\n",
        "        elif df['close'].iloc[i] < df['bb_lower'].iloc[i] and df['close'].iloc[i-1] >= df['bb_lower'].iloc[i-1]:\n",
        "            df.loc[df.index[i], 'signal'] = (df.loc[df.index[i], 'signal'] + ', BREAKOUT_LOWER').strip(', ') if df.loc[df.index[i], 'signal'] != 'HOLD' else 'BREAKOUT_LOWER'\n",
        "\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame\n",
        "\"\"\")"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d2bbb4df",
        "outputId": "d50927ca-1113-44f9-b30a-4a276db74d01"
      },
      "source": [
        "# Execute the main script\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Data fetched and saved.\n",
            "Computing technical indicators...\n",
            "Indicators computed and saved.\n",
            "Generating signals...\n",
            "Signals generated and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1515e034"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write('''# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from indicators.ta_utils import compute_indicators\n",
        "from signals.generate_signals import generate_signal\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    data = get_top_coins()\n",
        "    df_top10 = pd.DataFrame(data)\n",
        "    df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "    df_top10.to_csv(f\"{DATA_DIR}/top10_market_data.csv\", index=False)\n",
        "    print(\"Top 10 data fetched and saved.\")\n",
        "\n",
        "    print(\"Computing technical indicators for top 10 data...\")\n",
        "    # For now, we'll use the current price as 'close' for indicator calculation\n",
        "    df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "    df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "    df_ind_top10.to_csv(f\"{DATA_DIR}/top10_with_indicators.csv\", index=False)\n",
        "    print(\"Indicators computed and saved for top 10 data.\")\n",
        "\n",
        "    # --- Historical Data Processing ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    print(f\"Attempting to fetch and process historical data for {coin_id}...\")\n",
        "    history_data = get_coin_history(coin_id, days=\"30\")\n",
        "    print(\"Historical data fetched.\")\n",
        "\n",
        "    # Process historical price data into a DataFrame\n",
        "    # The 'prices' key contains a list of [timestamp, price]\n",
        "    if \"prices\" not in history_data:\n",
        "        print(f\"Warning: 'prices' key not found in historical data for {coin_id}.\")\n",
        "        history_df_signals = pd.DataFrame() # Ensure history_df_signals is defined even if historical data is missing\n",
        "    else:\n",
        "        history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\n",
        "        # Convert timestamp to datetime and set as index (or keep as column)\n",
        "        history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "        # Optional: Set timestamp as index for time-series analysis\n",
        "        # history_df = history_df.set_index(\"timestamp\")\n",
        "        print(\"Historical data processed into DataFrame.\")\n",
        "\n",
        "        # Compute indicators on the historical data DataFrame\n",
        "        print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "        history_df_ind = compute_indicators(history_df.copy())\n",
        "        history_df_ind.to_csv(f\"{DATA_DIR}/{coin_id}_historical_with_indicators.csv\", index=False)\n",
        "        print(\"Indicators computed and saved for historical data.\")\n",
        "\n",
        "        # Generate signals on the historical data\n",
        "        print(f\"Generating signals for {coin_id} historical data...\")\n",
        "        history_df_signals = generate_signal(history_df_ind)\n",
        "        history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)\n",
        "        print(\"Historical signals generated and saved.\")\n",
        "\n",
        "    print(\"Generating signals for top 10 snapshot data...\")\n",
        "    # Note: The current generate_signal function expects a single row or a dataframe\n",
        "    # with sufficient historical data. For this initial test with only current data,\n",
        "    # the signals will likely be 'HOLD' or based on limited info.\n",
        "    df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data\n",
        "    df_signals_top10.to_csv(f\"{DATA_DIR}/top10_signals.csv\", index=False)\n",
        "    print(\"Signals generated and saved for top 10 snapshot data.\")\n",
        "\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(\"\\\\n--- Results ---\")\n",
        "    print(\"Historical signals (if generated):\")\n",
        "    if not history_df_signals.empty:\n",
        "        print(history_df_signals.head().to_markdown(index=False)) # Use to_markdown for better printing\n",
        "    else:\n",
        "        print(\"No historical signals generated.\")\n",
        "\n",
        "    print(\"\\\\nTop 10 snapshot signals:\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False)) # Use to_markdown for better printing\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d87ac81e"
      },
      "source": [
        "# Modify ml_logger.py to check for required columns before selecting them.\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def log_ml_features(indicator_csv, output_csv):\n",
        "    \"\"\"Append new features to ML training dataset.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(indicator_csv)\n",
        "        print(f\"Successfully read {indicator_csv} for ML logging.\")\n",
        "        print(f\"Columns in {indicator_csv}: {df.columns.tolist()}\") # Add logging\n",
        "\n",
        "        required_features = [\n",
        "            \"id\", \"timestamp\", \"current_price\",\n",
        "            \"rsi\", \"ema_20\", \"macd_diff\",\n",
        "            \"bb_upper\", \"bb_lower\"\n",
        "        ]\n",
        "\n",
        "        # Check if all required features are in the DataFrame\n",
        "        missing_features = [col for col in required_features if col not in df.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing required features for ML logging in {indicator_csv}: {missing_features}. Skipping logging for this file.\")\n",
        "            # Optionally, return an empty DataFrame or None to indicate failure\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        features = df[required_features].copy()\n",
        "\n",
        "        if os.path.exists(output_csv):\n",
        "            existing = pd.read_csv(output_csv)\n",
        "            combined = pd.concat([existing, features]).drop_duplicates(subset=[\"id\", \"timestamp\"])\n",
        "        else:\n",
        "            combined = features\n",
        "\n",
        "        combined.to_csv(output_csv, index=False)\n",
        "        print(f\"ML features logged to {output_csv}.\")\n",
        "        return combined\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file for ML logging not found at {indicator_csv}. Skipping logging.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ML feature logging for {indicator_csv}: {e}. Skipping logging.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "# Overwrite the existing ml_logger.py file\n",
        "with open('signal_bot/ml_logger.py', 'w') as f:\n",
        "    f.write('''# ml_logger.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def log_ml_features(indicator_csv, output_csv):\n",
        "    \"\"\"Append new features to ML training dataset.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(indicator_csv)\n",
        "        # print(f\"Successfully read {indicator_csv} for ML logging.\") # Removed verbose logging\n",
        "        # print(f\"Columns in {indicator_csv}: {df.columns.tolist()}\") # Removed verbose logging\n",
        "\n",
        "        required_features = [\n",
        "            \"id\", \"timestamp\", \"current_price\",\n",
        "            \"rsi\", \"ema_20\", \"macd_diff\",\n",
        "            \"bb_upper\", \"bb_lower\"\n",
        "        ]\n",
        "\n",
        "        # Check if all required features are in the DataFrame\n",
        "        missing_features = [col for col in required_features if col not in df.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing required features for ML logging in {indicator_csv}: {missing_features}. Skipping logging for this file.\")\n",
        "            # Optionally, return an empty DataFrame or None to indicate failure\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        features = df[required_features].copy()\n",
        "\n",
        "        if os.path.exists(output_csv):\n",
        "            existing = pd.read_csv(output_csv)\n",
        "            combined = pd.concat([existing, features]).drop_duplicates(subset=[\"id\", \"timestamp\"])\n",
        "        else:\n",
        "            combined = features\n",
        "\n",
        "        combined.to_csv(output_csv, index=False)\n",
        "        # print(f\"ML features logged to {output_csv}.\") # Removed verbose logging\n",
        "        return combined\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file for ML logging not found at {indicator_csv}. Skipping logging.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ML feature logging for {indicator_csv}: {e}. Skipping logging.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "''')"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "31815cc0"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/anomaly_detector.py', 'w') as f:\n",
        "    f.write('''# anomaly_detector.py\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def load_market_snapshot(snapshot_csv):\n",
        "    \"\"\"Load full market snapshot from CSV.\"\"\"\n",
        "    df = pd.read_csv(snapshot_csv, parse_dates=[\"timestamp\"])\n",
        "    return df\n",
        "\n",
        "def prepare_features(df):\n",
        "    \"\"\"Create features for anomaly detection (e.g., price % change).\"\"\"\n",
        "    df[\"pct_change_24h\"] = df[\"price_change_percentage_24h\"].fillna(0)\n",
        "    return df[[\"id\", \"symbol\", \"name\", \"pct_change_24h\", \"timestamp\"]].copy()\n",
        "\n",
        "def run_isolation_forest(df, contamination=0.01):\n",
        "    \"\"\"Apply Isolation Forest to detect anomalies based on % change.\"\"\"\n",
        "    model = IsolationForest(contamination=contamination, random_state=42)\n",
        "    df[\"anomaly_score\"] = model.fit_predict(df[[\"pct_change_24h\"]])\n",
        "    return df\n",
        "\n",
        "def save_anomalies(df, output_dir=\"data\"):\n",
        "    \"\"\"Save anomalies to timestamped CSV.\"\"\"\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    anomalies = df[df[\"anomaly_score\"] == -1]\n",
        "    path = os.path.join(output_dir, f\"anomalies_{timestamp}.csv\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    anomalies.to_csv(path, index=False)\n",
        "    return path, anomalies\n",
        "\n",
        "def detect_anomalies(snapshot_csv, output_dir=\"data\"):\n",
        "    \"\"\"End-to-end anomaly detection workflow.\"\"\"\n",
        "    df = load_market_snapshot(snapshot_csv)\n",
        "    features = prepare_features(df)\n",
        "    with_scores = run_isolation_forest(features)\n",
        "    path, anomalies = save_anomalies(with_scores, output_dir)\n",
        "    return path, anomalies\n",
        "''')"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4a9b0f69"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot/data', exist_ok=True) # Ensure the data directory exists\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/indicator_engine.py', 'w') as f:\n",
        "    f.write('''# indicator_engine.py\n",
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "import os\n",
        "import ta  # pip install ta\n",
        "\n",
        "def compute_indicators(input_csv, output_csv):\n",
        "    df = pd.read_csv(input_csv, parse_dates=[\"timestamp\"])\n",
        "\n",
        "    # Ensure close price exists\n",
        "    df[\"close\"] = df[\"current_price\"]\n",
        "    df = df.sort_values(\"timestamp\")\n",
        "\n",
        "    df[\"rsi\"] = ta.momentum.RSIIndicator(df[\"close\"]).rsi()\n",
        "    df[\"ema_20\"] = ta.trend.EMAIndicator(df[\"close\"], window=20).ema_indicator()\n",
        "    df[\"macd_diff\"] = ta.trend.MACD(df[\"close\"]).macd_diff()\n",
        "\n",
        "    # Bollinger Bands (20 window, 2 std)\n",
        "    bb = ta.volatility.BollingerBands(df[\"close\"], window=20, window_dev=2)\n",
        "    df[\"bb_upper\"] = bb.bollinger_hband()\n",
        "    df[\"bb_lower\"] = bb.bollinger_lband()\n",
        "\n",
        "    df[\"processed_timestamp\"] = datetime.utcnow().isoformat()\n",
        "    os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "    df.to_csv(output_csv, index=False)\n",
        "    return df\n",
        "\n",
        "# Run engine\n",
        "# Using the DATA_DIR defined earlier in the notebook\n",
        "DATA_DIR = '/content/drive/MyDrive/crypto_signal_bot/data'\n",
        "input_path = f\"{DATA_DIR}/top10_market_data.csv\"\n",
        "output_path = f\"{DATA_DIR}/top10_with_indicators.csv\"\n",
        "\n",
        "# Check if the input file exists before attempting to read\n",
        "if os.path.exists(input_path):\n",
        "    df_ind = compute_indicators(input_path, output_path)\n",
        "    import ace_tools as tools; tools.display_dataframe_to_user(name=\"Top 10 with Indicators\", dataframe=df_ind)\n",
        "else:\n",
        "    print(f\"Error: Input file not found at {input_path}. Please ensure data has been fetched.\")\n",
        "\n",
        "''')"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3266dc3f",
        "outputId": "82509268-2657-41df-c7ff-0c99275e4de8"
      },
      "source": [
        "# Rewrite the content of signal_bot/main.py with correct indentation for the historical data handling\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from indicators.ta_utils import compute_indicators\n",
        "from signals.generate_signals import generate_signal\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data = get_top_coins()\n",
        "        if data is not None:\n",
        "            df_top10 = pd.DataFrame(data)\n",
        "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            df_top10.to_csv(f\"{DATA_DIR}/top10_market_data.csv\", index=False)\n",
        "            print(\"Top 10 data fetched and saved.\")\n",
        "\n",
        "            print(\"Computing technical indicators for top 10 data...\")\n",
        "            # For now, we'll use the current price as 'close' for indicator calculation\n",
        "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "            df_ind_top10.to_csv(f\"{DATA_DIR}/top10_with_indicators.csv\", index=False)\n",
        "            print(\"Indicators computed and saved for top 10 data.\")\n",
        "\n",
        "            print(\"Generating signals for top 10 snapshot data...\")\n",
        "            df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data\n",
        "            df_signals_top10.to_csv(f\"{DATA_DIR}/top10_signals.csv\", index=False)\n",
        "            print(\"Signals generated and saved for top 10 snapshot data.\")\n",
        "        else:\n",
        "            print(\"Failed to fetch top 10 coin data.\")\n",
        "            df_signals_top10 = pd.DataFrame() # Ensure df_signals_top10 is defined\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing top 10 data: {e}. Skipping top 10 data processing.\")\n",
        "        df_signals_top10 = pd.DataFrame() # Ensure df_signals_top10 is defined even on error\n",
        "\n",
        "\n",
        "    # --- Historical Data Processing with Error Handling ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals = pd.DataFrame() # Initialize as empty DataFrame\n",
        "\n",
        "    print(f\"Attempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "\n",
        "        if history_data is not None:\n",
        "            print(\"Historical data fetched.\")\n",
        "\n",
        "            # Process historical price data into a DataFrame\n",
        "            if \"prices\" not in history_data:\n",
        "                print(f\"Warning: 'prices' key not found in historical data for {coin_id}. Response keys: {history_data.keys()}\")\n",
        "            else:\n",
        "                history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\n",
        "                history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "                print(\"Historical data processed into DataFrame.\")\n",
        "\n",
        "                # Compute indicators on the historical data DataFrame\n",
        "                print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "                history_df_ind = compute_indicators(history_df.copy())\n",
        "                history_df_ind.to_csv(f\"{DATA_DIR}/{coin_id}_historical_with_indicators.csv\", index=False)\n",
        "                print(\"Indicators computed and saved for historical data.\")\n",
        "\n",
        "                # Generate signals on the historical data\n",
        "                print(f\"Generating signals for {coin_id} historical data...\")\n",
        "                history_df_signals = generate_signal(history_df_ind)\n",
        "                history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)\n",
        "                print(\"Historical signals generated and saved.\")\n",
        "        else:\n",
        "             print(\"Historical data fetching failed. Skipping historical data processing.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing historical data for {coin_id}: {e}. Skipping historical signals.\")\n",
        "        # history_df_signals is already initialized as empty\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(\"\\\\n--- Results ---\")\n",
        "    print(\"Historical signals (if generated):\")\n",
        "    if not history_df_signals.empty:\n",
        "        print(history_df_signals.head().to_markdown(index=False)) # Use to_markdown for better printing\n",
        "    else:\n",
        "        print(\"No historical signals generated.\")\n",
        "\n",
        "    print(\"\\\\nTop 10 snapshot signals:\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False)) # Use to_markdown for better printing\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)\n",
        "\n",
        "# Execute the script\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Signals generated and saved for top 10 snapshot data.\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data.\n",
            "Generating signals for bitcoin historical data...\n",
            "Historical signals generated and saved.\n",
            "\n",
            "--- Results ---\n",
            "Historical signals (if generated):\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "\n",
            "Top 10 snapshot signals:\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6d111616",
        "outputId": "aadfa3be-51c8-4997-d5d2-2ec0b6cb475c"
      },
      "source": [
        "# Modify the main.py script to handle None return from get_coin_history without using continue\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "modified_lines = []\n",
        "in_historical_processing_block = False\n",
        "for line in lines:\n",
        "    if 'history_data = get_coin_history(coin_id, days=\"30\")' in line:\n",
        "        modified_lines.append(line)\n",
        "        modified_lines.append('        if history_data is not None:\\n') # Start the conditional block\n",
        "        in_historical_processing_block = True\n",
        "    elif in_historical_processing_block and line.strip() == '': # End the conditional block at the end of the block\n",
        "         modified_lines.append('        # End of historical data processing block\\n')\n",
        "         in_historical_processing_block = False\n",
        "         modified_lines.append(line) # Keep the blank line\n",
        "    elif in_historical_processing_block and line.strip().startswith('except'): # End the conditional block before except\n",
        "         modified_lines.append('        # End of historical data processing block\\n')\n",
        "         in_historical_processing_block = False\n",
        "         modified_lines.append(line)\n",
        "    elif in_historical_processing_block:\n",
        "        # Indent the lines within the historical data processing block\n",
        "        modified_lines.append(f'            {line.lstrip()}')\n",
        "    elif 'coin_id = \"ethereum\"' in line:\n",
        "         modified_lines.append('    coin_id = \"bitcoin\" # Revert to bitcoin\\n') # Revert coin_id\n",
        "    else:\n",
        "        modified_lines.append(line)\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.writelines(modified_lines)\n",
        "\n",
        "# Execute the main script again to test the changes\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/signal_bot/main.py\", line 48\n",
            "    print(\"Historical data fetching failed. Skipping historical data processing.\")\n",
            "    ^^^^^\n",
            "IndentationError: expected an indented block after 'if' statement on line 47\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "24569ad4",
        "outputId": "49ec9dce-e9f6-4bc0-a4c4-16530e2958b0"
      },
      "source": [
        "# Modify the get_coin_history function to remove the 'interval' parameter\n",
        "with open('signal_bot/utils/coingecko_api.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "modified_lines = []\n",
        "for line in lines:\n",
        "    if 'params = {\"vs_currency\": \"usd\", \"days\": days, \"interval\": \"hourly\"}' in line:\n",
        "        modified_lines.append('    params = {\"vs_currency\": \"usd\", \"days\": days}\\n') # Remove interval parameter\n",
        "    else:\n",
        "        modified_lines.append(line)\n",
        "\n",
        "with open('signal_bot/utils/coingecko_api.py', 'w') as f:\n",
        "    f.writelines(modified_lines)\n",
        "\n",
        "# Modify the main.py script to handle None return from get_coin_history and try a different coin\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "modified_lines = []\n",
        "for line in lines:\n",
        "    if 'history_data = get_coin_history(coin_id, days=\"30\")' in line:\n",
        "        modified_lines.append(line)\n",
        "        modified_lines.append('        if history_data is None:\\n') # Add check for None\n",
        "        modified_lines.append('            print(\"Historical data fetching failed. Skipping historical data processing.\")\\n')\n",
        "        modified_lines.append('            history_df_signals = pd.DataFrame() # Ensure history_df_signals is defined\\n')\n",
        "        modified_lines.append('            continue # Skip the rest of the historical data processing block\\n')\n",
        "    elif 'coin_id = \"bitcoin\"' in line:\n",
        "         modified_lines.append('    coin_id = \"ethereum\" # Try fetching historical data for Ethereum\\n') # Change coin_id\n",
        "    else:\n",
        "        modified_lines.append(line)\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.writelines(modified_lines)\n",
        "\n",
        "# Execute the main script again to test the changes\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/signal_bot/main.py\", line 49\n",
            "    continue # Skip the rest of the historical data processing block\n",
            "    ^^^^^^^^\n",
            "SyntaxError: 'continue' not properly in loop\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ba0fa202",
        "outputId": "5a8f96f8-28e5-4e61-d1fd-f7917f0b6bd7"
      },
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "BASE_URL = \"https://api.coingecko.com/api/v3\"\n",
        "\n",
        "def get_top_coins(limit=10):\n",
        "    url = f\"{BASE_URL}/coins/markets\"\n",
        "    params = {\n",
        "        \"vs_currency\": \"usd\",\n",
        "        \"order\": \"market_cap_desc\",\n",
        "        \"per_page\": limit,\n",
        "        \"page\": 1,\n",
        "        \"sparkline\": \"true\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching top coins: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_coin_history(coin_id, days=\"30\"):\n",
        "    url = f\"{BASE_URL}/coins/{coin_id}/market_chart\"\n",
        "    params = {\"vs_currency\": \"usd\", \"days\": days} # Removed 'interval': 'hourly'\n",
        "    print(f\"Fetching historical data from URL: {url} with params: {params}\") # Log URL and params\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        print(f\"Historical data response keys: {data.keys()}\") # Log response keys\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching historical data for {coin_id}: {e}\")\n",
        "        if 'response' in locals() and response is not None:\n",
        "             print(f\"Response status code: {response.status_code}\")\n",
        "             print(f\"Response text: {response.text}\") # Log response text\n",
        "        return None\n",
        "\n",
        "# Overwrite the existing file with the modified content\n",
        "with open('signal_bot/utils/coingecko_api.py', 'w') as f:\n",
        "    f.write('''# utils/coingecko_api.py\n",
        "import requests\n",
        "\n",
        "BASE_URL = \"https://api.coingecko.com/api/v3\"\n",
        "\n",
        "def get_top_coins(limit=10):\n",
        "    url = f\"{BASE_URL}/coins/markets\"\n",
        "    params = {\n",
        "        \"vs_currency\": \"usd\",\n",
        "        \"order\": \"market_cap_desc\",\n",
        "        \"per_page\": limit,\n",
        "        \"page\": 1,\n",
        "        \"sparkline\": \"true\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching top coins: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_coin_history(coin_id, days=\"30\"):\n",
        "    url = f\"{BASE_URL}/coins/{coin_id}/market_chart\"\n",
        "    params = {\"vs_currency\": \"usd\", \"days\": days} # Removed 'interval': 'hourly'\n",
        "    print(f\"Fetching historical data from URL: {url} with params: {params}\") # Log URL and params\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        print(f\"Historical data response keys: {data.keys()}\") # Log response keys\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching historical data for {coin_id}: {e}\")\n",
        "        if 'response' in locals() and response is not None:\n",
        "             print(f\"Response status code: {response.status_code}\")\n",
        "             print(f\"Response text: {response.text}\") # Log response text\n",
        "        return None''')\n",
        "\n",
        "# Modify main.py to handle None from get_coin_history\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from indicators.ta_utils import compute_indicators\n",
        "from signals.generate_signals import generate_signal\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data = get_top_coins()\n",
        "        df_top10 = pd.DataFrame(data)\n",
        "        df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "        df_top10.to_csv(f\"{DATA_DIR}/top10_market_data.csv\", index=False)\n",
        "        print(\"Top 10 data fetched and saved.\")\n",
        "\n",
        "        print(\"Computing technical indicators for top 10 data...\")\n",
        "        # For now, we'll use the current price as 'close' for indicator calculation\n",
        "        df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "        df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "        df_ind_top10.to_csv(f\"{DATA_DIR}/top10_with_indicators.csv\", index=False)\n",
        "        print(\"Indicators computed and saved for top 10 data.\")\n",
        "\n",
        "        print(\"Generating signals for top 10 snapshot data...\")\n",
        "        # Note: The current generate_signal function expects a single row or a dataframe\n",
        "        # with sufficient historical data. For this initial test with only current data,\n",
        "        # the signals will likely be 'HOLD' or based on limited info.\n",
        "        df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data\n",
        "        df_signals_top10.to_csv(f\"{DATA_DIR}/top10_signals.csv\", index=False)\n",
        "        print(\"Signals generated and saved for top 10 snapshot data.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing top 10 data: {e}. Skipping top 10 data processing.\")\n",
        "        df_signals_top10 = pd.DataFrame() # Ensure df_signals_top10 is defined even on error\n",
        "\n",
        "    # --- Historical Data Processing with Error Handling ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals = pd.DataFrame() # Initialize as empty DataFrame\n",
        "\n",
        "    print(f\"Attempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "\n",
        "        if history_data is not None: # Check if data fetching was successful\n",
        "            print(\"Historical data fetched.\")\n",
        "\n",
        "            # Process historical price data into a DataFrame\n",
        "            if \"prices\" not in history_data:\n",
        "                print(f\"Warning: 'prices' key not found in historical data for {coin_id}. Response keys: {history_data.keys()}\")\n",
        "            else:\n",
        "                history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\n",
        "                history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "                print(\"Historical data processed into DataFrame.\")\n",
        "\n",
        "                # Compute indicators on the historical data DataFrame\n",
        "                print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "                history_df_ind = compute_indicators(history_df.copy())\n",
        "                history_df_ind.to_csv(f\"{DATA_DIR}/{coin_id}_historical_with_indicators.csv\", index=False)\n",
        "                print(\"Indicators computed and saved for historical data.\")\n",
        "\n",
        "                # Generate signals on the historical data\n",
        "                print(f\"Generating signals for {coin_id} historical data...\")\n",
        "                history_df_signals = generate_signal(history_df_ind)\n",
        "                history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)\n",
        "                print(\"Historical signals generated and saved.\")\n",
        "        else:\n",
        "             print(\"Historical data fetching failed. Skipping historical data processing.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing historical data for {coin_id}: {e}. Skipping historical signals.\")\n",
        "        # history_df_signals is already initialized as empty\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(\"\\\\n--- Results ---\")\n",
        "    print(\"Historical signals (if generated):\")\n",
        "    if not history_df_signals.empty:\n",
        "        print(history_df_signals.head().to_markdown(index=False)) # Use to_markdown for better printing\n",
        "    else:\n",
        "        print(\"No historical signals generated.\")\n",
        "\n",
        "    print(\"\\\\nTop 10 snapshot signals:\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False)) # Use to_markdown for better printing\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)\n",
        "\n",
        "\n",
        "# Execute the main script to trigger the historical data fetch with added logging\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Signals generated and saved for top 10 snapshot data.\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Fetching historical data from URL: https://api.coingecko.com/api/v3/coins/bitcoin/market_chart with params: {'vs_currency': 'usd', 'days': '30'}\n",
            "Historical data response keys: dict_keys(['prices', 'market_caps', 'total_volumes'])\n",
            "Historical data fetched.\n",
            "Historical data processed into DataFrame.\n",
            "Computing technical indicators for bitcoin historical data...\n",
            "Indicators computed and saved for historical data.\n",
            "Generating signals for bitcoin historical data...\n",
            "Historical signals generated and saved.\n",
            "\n",
            "--- Results ---\n",
            "Historical signals (if generated):\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "\n",
            "Top 10 snapshot signals:\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "520770b8",
        "outputId": "b2b3230c-b0f9-477c-de41-58b330a15516"
      },
      "source": [
        "# Execute the main script\n",
        "!python signal_bot/main.py"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Top 10 data fetched and saved.\n",
            "Computing technical indicators for top 10 data...\n",
            "Indicators computed and saved for top 10 data.\n",
            "Generating signals for top 10 snapshot data...\n",
            "Signals generated and saved for top 10 snapshot data.\n",
            "Attempting to fetch and process historical data for bitcoin...\n",
            "Historical data fetched.\n",
            "Warning: 'prices' key not found in historical data for bitcoin.\n",
            "\n",
            "--- Results ---\n",
            "Historical signals (if generated):\n",
            "No historical signals generated.\n",
            "\n",
            "Top 10 snapshot signals:\n",
            "| signal   |\n",
            "|:---------|\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n",
            "| HOLD     |\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0c43588c"
      },
      "source": [
        "# Rewrite the content of signal_bot/main.py to integrate modules and clean up for testing.\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from .indicators.ta_utils import compute_indicators\n",
        "# from .signals.generate_signals import generate_signal\n",
        "from .signals.signal_finder import find_signals\n",
        "from .anomaly_detector import detect_anomalies\n",
        "from .ml_logger import log_ml_features\n",
        "from .backtester import backtest_signals\n",
        "from .ml_model_trainer import train_ml_model\n",
        "from .logger import setup_logger, log_info, log_error\n",
        "from .exporter import export_to_excel, export_to_html # Import exporter functions\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    setup_logger()\n",
        "    log_info(\"Starting bot pipeline...\")\n",
        "\n",
        "    # --- Data Collection (using existing top 10 fetch for now) ---\n",
        "    log_info(\"--- Top 10 Data Pipeline ---\")\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data = get_top_coins()\n",
        "        if data is not None:\n",
        "            df_top10 = pd.DataFrame(data)\n",
        "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            top10_market_data_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "            df_top10.to_csv(top10_market_data_path, index=False)\n",
        "            log_info(\"Top 10 data fetched and saved.\")\n",
        "\n",
        "            log_info(\"Computing technical indicators for top 10 data...\")\n",
        "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "            top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "            df_ind_top10.to_csv(top10_indicators_path, index=False)\n",
        "            log_info(\"Indicators computed and saved for top 10 data.\")\n",
        "\n",
        "            log_info(\"Generating signals for top 10 snapshot data...\")\n",
        "            df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "            top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "            df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "            log_info(\"Signals generated and saved for top 10 snapshot data.\")\n",
        "\n",
        "            log_info(\"Logging ML features for top 10 data...\")\n",
        "            ml_log_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "            log_ml_features(top10_indicators_path, ml_log_path)\n",
        "            log_info(\"ML features logged for top 10 data.\")\n",
        "\n",
        "        else:\n",
        "            log_error(\"Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.\")\n",
        "            df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.\")\n",
        "        df_signals_top10 = pd.DataFrame()\n",
        "    log_info(\"--- Top 10 Data Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Anomaly Detection ---\n",
        "    log_info(\"\\\\n--- Anomaly Detection Pipeline ---\")\n",
        "    print(\"Attempting to run Anomaly Detection...\")\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if os.path.exists(full_snapshot_path):\n",
        "        try:\n",
        "            log_info(\"Running Anomaly Detection...\")\n",
        "            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "            log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "            if not anomalies.empty:\n",
        "                 print(\"Sample anomalies:\")\n",
        "                 print(anomalies.head().to_markdown(index=False))\n",
        "            else:\n",
        "                 print(\"No anomalies detected.\")\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error during Anomaly Detection: {e}.\")\n",
        "    else:\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.\")\n",
        "    log_info(\"--- Anomaly Detection Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Historical Data Processing ---\n",
        "    log_info(\"\\\\n--- Historical Data Pipeline ---\")\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals_display = pd.DataFrame()\n",
        "    historical_price_data_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_price.csv\")\n",
        "    historical_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\")\n",
        "    backtest_results_path = os.path.join(DATA_DIR, \"signal_backtest.csv\")\n",
        "\n",
        "    print(f\"Attempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "        if history_data is not None and \"prices\" in history_data:\n",
        "            log_info(\"Historical data fetched.\")\n",
        "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"current_price\"])\n",
        "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "            history_df.to_csv(historical_price_data_path, index=False)\n",
        "            log_info(\"Historical data processed and saved.\")\n",
        "\n",
        "            log_info(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "            history_df[\"close\"] = history_df[\"current_price\"]\n",
        "            history_df_ind = compute_indicators(history_df.copy())\n",
        "            history_historical_indicators_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_with_indicators.csv\")\n",
        "            history_df_ind.to_csv(history_historical_indicators_path, index=False)\n",
        "            log_info(\"Indicators computed and saved for historical data.\")\n",
        "\n",
        "            log_info(f\"Generating signals for {coin_id} historical data...\")\n",
        "            history_df_signals = find_signals(history_df_ind.copy())\n",
        "            history_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\")\n",
        "            history_df_signals.to_csv(historical_signals_path, index=False)\n",
        "            log_info(\"Historical signals generated and saved.\")\n",
        "            history_df_signals_display = history_df_signals\n",
        "\n",
        "            # --- Backtesting Historical Signals ---\n",
        "            log_info(f\"Attempting to backtest historical signals for {coin_id}...\")\n",
        "            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)\n",
        "            log_info(\"Backtesting completed.\")\n",
        "            if not backtest_results_df.empty:\n",
        "                print(\"Backtest Results (head):\")\n",
        "                print(backtest_results_df.head().to_markdown(index=False))\n",
        "\n",
        "                # --- Export Backtest Results --- # Added Export step\n",
        "                log_info(\"Exporting backtest results...\")\n",
        "                excel_output_path = os.path.join(DATA_DIR, \"signal_backtest_report.xlsx\")\n",
        "                html_output_path = os.path.join(DATA_DIR, \"signal_backtest_report.html\")\n",
        "                export_to_excel(backtest_results_path, excel_output_path)\n",
        "                export_to_html(backtest_results_path, html_output_path)\n",
        "                log_info(f\"Backtest results exported to {excel_output_path} and {html_output_path}.\")\n",
        "\n",
        "\n",
        "                # --- ML Model Training ---\n",
        "                log_info(\"Attempting to train ML model...\")\n",
        "                if os.path.exists(backtest_results_path):\n",
        "                    try:\n",
        "                        model, report = train_ml_model(backtest_results_path)\n",
        "                        log_info(\"ML model training completed.\")\n",
        "                        print(\"Classification Report:\")\n",
        "                        for label, metrics in report.items():\n",
        "                             if isinstance(metrics, dict):\n",
        "                                  print(f\"  {label}:\")\n",
        "                                  for metric, value in metrics.items():\n",
        "                                       print(f\"    {metric}: {value:.4f}\")\n",
        "                             else:\n",
        "                                  print(f\"  {label}: {metrics:.4f}\")\n",
        "\n",
        "                        # Optional: Save the trained model\n",
        "                        # import joblib\n",
        "                        # joblib.dump(model, os.path.join(DATA_DIR, 'ml_model.pkl'))\n",
        "                        # log_info(\"ML model saved.\")\n",
        "\n",
        "\n",
        "                    except ValueError as ve:\n",
        "                        log_error(f\"Error during ML training: {ve}. Skipping training.\")\n",
        "                    except Exception as e:\n",
        "                        log_error(f\"Error during ML training: {e}. Skipping training.\")\n",
        "                else:\n",
        "                    log_info(f\"Warning: Backtest results file not found at {backtest_results_path}. Skipping ML model training.\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                log_info(\"No backtest results to display. Skipping ML model training.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "             log_info(\"Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error processing historical data: {e}. Skipping historical signals.\")\n",
        "    log_info(\"--- Historical Data Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Display Results (for Top 10 snapshot) ---\n",
        "    print(\"\\\\n--- Results (Top 10 Snapshot) ---\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False))\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated due to processing errors or no data.\")\n",
        "\n",
        "    # --- Display Results (for Historical, if processed) ---\n",
        "    print(\"\\\\n--- Results (Historical Data) ---\")\n",
        "    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:\n",
        "         print(\"Historical signals (tail):\")\n",
        "         print(history_df_signals_display.tail().to_markdown(index=False))\n",
        "         print(\"\\nHistorical signals (sample):\")\n",
        "         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))\n",
        "    else:\n",
        "         print(\"No historical signals generated or historical data processing skipped.\")\n",
        "\n",
        "\n",
        "    log_info(\"Bot pipeline finished.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)"
      ],
      "execution_count": 120,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "91e8f7fc"
      },
      "source": [
        "# Rewrite the content of signal_bot/main.py with the corrected structure.\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from indicators.ta_utils import compute_indicators\n",
        "from signals.generate_signals import generate_signal\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    data = get_top_coins()\n",
        "    df_top10 = pd.DataFrame(data)\n",
        "    df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "    df_top10.to_csv(f\"{DATA_DIR}/top10_market_data.csv\", index=False)\n",
        "    print(\"Top 10 data fetched and saved.\")\n",
        "\n",
        "    print(\"Computing technical indicators for top 10 data...\")\n",
        "    # For now, we'll use the current price as 'close' for indicator calculation\n",
        "    df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "    df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "    df_ind_top10.to_csv(f\"{DATA_DIR}/top10_with_indicators.csv\", index=False)\n",
        "    print(\"Indicators computed and saved for top 10 data.\")\n",
        "\n",
        "    # --- Historical Data Processing with Error Handling ---\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals = pd.DataFrame() # Initialize as empty DataFrame\n",
        "\n",
        "    print(f\"Attempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "        print(\"Historical data fetched.\")\n",
        "\n",
        "        # Process historical price data into a DataFrame\n",
        "        if \"prices\" not in history_data:\n",
        "            print(f\"Warning: 'prices' key not found in historical data for {coin_id}.\")\n",
        "        else:\n",
        "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\n",
        "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "            print(\"Historical data processed into DataFrame.\")\n",
        "\n",
        "            # Compute indicators on the historical data DataFrame\n",
        "            print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "            history_df_ind = compute_indicators(history_df.copy())\n",
        "            history_df_ind.to_csv(f\"{DATA_DIR}/{coin_id}_historical_with_indicators.csv\", index=False)\n",
        "            print(\"Indicators computed and saved for historical data.\")\n",
        "\n",
        "            # Generate signals on the historical data\n",
        "            print(f\"Generating signals for {coin_id} historical data...\")\n",
        "            history_df_signals = generate_signal(history_df_ind)\n",
        "            history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)\n",
        "            print(\"Historical signals generated and saved.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error processing historical data for {coin_id}: {e}. Skipping historical signals.\")\n",
        "        # history_df_signals is already initialized as empty\n",
        "\n",
        "    # --- Top 10 Signal Generation (always executed) ---\n",
        "    print(\"Generating signals for top 10 snapshot data...\")\n",
        "    # Note: The current generate_signal function expects a single row or a dataframe\n",
        "    # with sufficient historical data. For this initial test with only current data,\n",
        "    # the signals will likely be 'HOLD' or based on limited info.\n",
        "    df_signals_top10 = generate_signal(df_ind_top10) # Apply signal generation to the top 10 data\n",
        "    df_signals_top10.to_csv(f\"{DATA_DIR}/top10_signals.csv\", index=False)\n",
        "    print(\"Signals generated and saved for top 10 snapshot data.\")\n",
        "\n",
        "    # --- Display Results ---\n",
        "    print(\"\\\\n--- Results ---\")\n",
        "    print(\"Historical signals (if generated):\")\n",
        "    if not history_df_signals.empty:\n",
        "        print(history_df_signals.head())\n",
        "    else:\n",
        "        print(\"No historical signals generated.\")\n",
        "\n",
        "    print(\"\\\\nTop 10 snapshot signals:\")\n",
        "    print(df_signals_top10.head())\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "22257560",
        "outputId": "2b2af726-8eb8-451b-cca9-e735630aa00c"
      },
      "source": [
        "# Open the signal_bot/main.py file to read its current content.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    current_lines = f.readlines()\n",
        "\n",
        "print(\"Current content of signal_bot/main.py:\")\n",
        "print(\"\".join(current_lines))\n",
        "\n",
        "# Now, based on the printed content, manually identify the correct lines\n",
        "# to use for finding the historical data block and the top 10 signal generation.\n",
        "\n",
        "# Assuming the lines are:\n",
        "# history_fetch_line_content = '    history_data = get_coin_history(coin_id, days=\"30\")\\n'\n",
        "# top10_signal_generation_line_content = '    df_signals = generate_signal(df_ind)\\n' # This line needs adjustment\n",
        "\n",
        "# Re-read the file to modify\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Find the line where historical data is fetched\n",
        "    history_fetch_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'history_data = get_coin_history(coin_id, days=\"30\")' in line:\n",
        "            history_fetch_line_index = i\n",
        "            break\n",
        "\n",
        "    # Find the line where top 10 signals are generated (this marks the end of the historical processing block)\n",
        "    # Based on previous executions, the line might be '    df_signals = generate_signal(df_ind)\\n'\n",
        "    top10_signal_generation_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'df_signals = generate_signal(df_ind)' in line:\n",
        "            top10_signal_generation_line_index = i\n",
        "            break\n",
        "\n",
        "\n",
        "    if history_fetch_line_index != -1 and top10_signal_generation_line_index != -1:\n",
        "        # Extract the historical data processing block\n",
        "        historical_processing_lines = main_function_lines[history_fetch_line_index:top10_signal_generation_line_index]\n",
        "\n",
        "        # Wrap the historical data processing block in a try-except block\n",
        "        wrapped_lines = [\n",
        "            '    try:\\n'\n",
        "        ]\n",
        "        for line in historical_processing_lines:\n",
        "            wrapped_lines.append(f'        {line}') # Indent the original lines\n",
        "        wrapped_lines.extend([\n",
        "            '    except Exception as e:\\n', # Catch a general exception for robustness\n",
        "            '        print(f\"Error processing historical data: {e}. Skipping historical signals.\")\\n',\n",
        "            '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\\n',\n",
        "            '        print(\"Historical signals generated (if successful):\")\\n', # Add print statements here\n",
        "            '        if not history_df_signals.empty:\\n',\n",
        "            '            print(history_df_signals.head())\\n',\n",
        "            '        else:\\n',\n",
        "            '            print(\"No historical signals generated.\")\\n',\n",
        "            '\\n'\n",
        "        ])\n",
        "\n",
        "        # Replace the original historical processing lines with the wrapped ones\n",
        "        modified_main_function_lines = (\n",
        "            main_function_lines[:history_fetch_line_index] +\n",
        "            wrapped_lines +\n",
        "            main_function_lines[top10_signal_generation_line_index:]\n",
        "        )\n",
        "\n",
        "        # Find the line where top 10 signals are saved\n",
        "        top10_signals_save_line_index = -1\n",
        "        for i, line in enumerate(modified_main_function_lines):\n",
        "            if 'df_signals.to_csv(f\"{DATA_DIR}/signals.csv\", index=False)' in line:\n",
        "                top10_signals_save_line_index = i\n",
        "                break\n",
        "\n",
        "        if top10_signals_save_line_index != -1:\n",
        "             # Add print statements after the top 10 signal saving\n",
        "            display_lines = [\n",
        "                '\\n',\n",
        "                '    print(\"Top 10 snapshot signals generated:\")\\n',\n",
        "                '    print(df_signals.head())\\n',\n",
        "                '\\n'\n",
        "            ]\n",
        "\n",
        "            modified_main_function_lines = (\n",
        "                modified_main_function_lines[:top10_signals_save_line_index + 1] +\n",
        "                display_lines +\n",
        "                modified_main_function_lines[top10_signals_save_line_index + 1:]\n",
        "            )\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the historical data fetch or top 10 signal generation line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")\n"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Current content of signal_bot/main.py:\n",
            "from utils.coingecko_api import get_coin_history\n",
            "# main.py\n",
            "import pandas as pd\n",
            "import os\n",
            "from utils.coingecko_api import get_top_coins\n",
            "from indicators.ta_utils import compute_indicators\n",
            "from signals.generate_signals import generate_signal\n",
            "\n",
            "DATA_DIR = 'signal_bot/data'\n",
            "os.makedirs(DATA_DIR, exist_ok=True)\n",
            "\n",
            "def main():\n",
            "    print(\"Fetching top 10 coin data...\")\n",
            "    data = get_top_coins()\n",
            "    df = pd.DataFrame(data)\n",
            "    df[\"timestamp\"] = pd.Timestamp.utcnow()\n",
            "    df.to_csv(f\"{DATA_DIR}/top10_market_data.csv\", index=False)\n",
            "    print(\"Data fetched and saved.\")\n",
            "\n",
            "    print(\"Computing technical indicators...\")\n",
            "    # For now, we'll use the current price as 'close' for indicator calculation\n",
            "    # In a real scenario, we'd fetch historical data for proper indicator calculation\n",
            "    df[\"close\"] = df[\"current_price\"]\n",
            "\n",
            "    # Fetch historical data for a specific coin (e.g., bitcoin)\n",
            "    coin_id = \"bitcoin\"\n",
            "    print(f\"Fetching historical data for {coin_id}...\")\n",
            "    try:\n",
            "        history_data = get_coin_history(coin_id, days=\"30\")\n",
            "        print(\"Historical data fetched.\")\n",
            "    \n",
            "        # Process historical price data into a DataFrame\n",
            "        # The 'prices' key contains a list of [timestamp, price]\n",
            "    try:\n",
            "            if \"prices\" not in history_data:\n",
            "                print(\"Error: Historical data not available.\")\n",
            "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\n",
            "            # Convert timestamp to datetime and set as index (or keep as column)\n",
            "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
            "            # Optional: Set timestamp as index for time-series analysis\n",
            "            # history_df = history_df.set_index(\"timestamp\")\n",
            "            print(\"Historical data processed into DataFrame.\")\n",
            "        \n",
            "            # Compute indicators on the historical data DataFrame\n",
            "            print(f\"Computing technical indicators for {coin_id} historical data...\")\n",
            "            history_df_ind = compute_indicators(history_df.copy())\n",
            "            history_df_ind.to_csv(f\"{DATA_DIR}/{coin_id}_historical_with_indicators.csv\", index=False)\n",
            "            print(\"Indicators computed and saved for historical data.\")\n",
            "        \n",
            "            # Note: Signal generation logic currently needs adjustment for time-series data\n",
            "            # The next step would be to adapt generate_signal to work with history_df_ind\n",
            "            # For now, we will keep the original signal generation on the top 10 snapshot for demonstration\n",
            "        \n",
            "            df_ind = compute_indicators(df.copy())\n",
            "            df_ind.to_csv(f\"{DATA_DIR}/top10_with_indicators.csv\", index=False)\n",
            "            print(\"Indicators computed and saved.\")\n",
            "        \n",
            "            print(\"Generating signals...\")\n",
            "            # Note: The current generate_signal function expects a single row or a dataframe\n",
            "            # with sufficient historical data. For this initial test with only current data,\n",
            "            # the signals will likely be 'HOLD' or based on limited info.\n",
            "            history_df_signals = generate_signal(history_df_ind)\n",
            "            history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)\n",
            "    except Exception as e:\n",
            "        print(f\"Error processing historical data: {e}. Skipping historical signals.\")\n",
            "        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\n",
            "    except Exception as e:\n",
            "        print(f\"Error processing historical data: {e}. Skipping historical signals.\")\n",
            "        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\n",
            "    print(\"Historical signals generated:\")\n",
            "    print(history_df_signals.head())\n",
            "\n",
            "    print(\"Historical signals generated and saved.\")\n",
            "    print(\"Signals generated and saved.\")\n",
            "\n",
            "if __name__ == \"__main__\":\n",
            "    main()\n",
            "\n",
            "Could not find the historical data fetch or top 10 signal generation line in main function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c7988ea",
        "outputId": "ca9637fe-4ae0-4ad4-9fb5-2d5bddc1deef"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Find the line where historical data is fetched\n",
        "    history_fetch_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'history_data = get_coin_history(coin_id, days=\"30\")' in line:\n",
        "            history_fetch_line_index = i\n",
        "            break\n",
        "\n",
        "    # Find the line where top 10 signals are generated (this marks the end of the historical processing block)\n",
        "    top10_signal_generation_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'df_signals = generate_signal(df_ind)' in line:\n",
        "            top10_signal_generation_line_index = i\n",
        "            break\n",
        "\n",
        "    if history_fetch_line_index != -1 and top10_signal_generation_line_index != -1:\n",
        "        # Extract the historical data processing block\n",
        "        historical_processing_lines = main_function_lines[history_fetch_line_index:top10_signal_generation_line_index]\n",
        "\n",
        "        # Wrap the historical data processing block in a try-except block\n",
        "        wrapped_lines = [\n",
        "            '    try:\\n'\n",
        "        ]\n",
        "        for line in historical_processing_lines:\n",
        "            wrapped_lines.append(f'        {line}') # Indent the original lines\n",
        "        wrapped_lines.extend([\n",
        "            '    except Exception as e:\\n', # Catch a general exception for robustness\n",
        "            '        print(f\"Error processing historical data: {e}. Skipping historical signals.\")\\n',\n",
        "            '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\\n',\n",
        "            '        print(\"Historical signals generated (if successful):\")\\n', # Add print statements here\n",
        "            '        if not history_df_signals.empty:\\n',\n",
        "            '            print(history_df_signals.head())\\n',\n",
        "            '        else:\\n',\n",
        "            '            print(\"No historical signals generated.\")\\n',\n",
        "            '\\n'\n",
        "        ])\n",
        "\n",
        "        # Replace the original historical processing lines with the wrapped ones\n",
        "        modified_main_function_lines = (\n",
        "            main_function_lines[:history_fetch_line_index] +\n",
        "            wrapped_lines +\n",
        "            main_function_lines[top10_signal_generation_line_index:]\n",
        "        )\n",
        "\n",
        "        # Find the line where top 10 signals are saved\n",
        "        top10_signals_save_line_index = -1\n",
        "        for i, line in enumerate(modified_main_function_lines):\n",
        "            if 'df_signals.to_csv(f\"{DATA_DIR}/signals.csv\", index=False)' in line:\n",
        "                top10_signals_save_line_index = i\n",
        "                break\n",
        "\n",
        "        if top10_signals_save_line_index != -1:\n",
        "             # Add print statements after the top 10 signal saving\n",
        "            display_lines = [\n",
        "                '\\n',\n",
        "                '    print(\"Top 10 snapshot signals generated:\")\\n',\n",
        "                '    print(df_signals.head())\\n',\n",
        "                '\\n'\n",
        "            ]\n",
        "\n",
        "            modified_main_function_lines = (\n",
        "                modified_main_function_lines[:top10_signals_save_line_index + 1] +\n",
        "                display_lines +\n",
        "                modified_main_function_lines[top10_signals_save_line_index + 1:]\n",
        "            )\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the historical data fetch or top 10 signal generation line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find the historical data fetch or top 10 signal generation line in main function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cbee795c",
        "outputId": "8ef2d97e-69d5-4470-b6da-cef6834c3650"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Find the try block\n",
        "    try_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if line.strip() == 'try:':\n",
        "            try_line_index = i\n",
        "            break\n",
        "\n",
        "    if try_line_index != -1:\n",
        "        # Find the except block\n",
        "        except_line_index = -1\n",
        "        for i in range(try_line_index + 1, len(main_function_lines)):\n",
        "            if line.strip().startswith('except'):\n",
        "                 except_line_index = i\n",
        "                 break\n",
        "\n",
        "        # If both try and except are found, we need to re-evaluate the block\n",
        "        if except_line_index != -1:\n",
        "             # Find the end of the except block (before the next non-except/non-blank line)\n",
        "             except_block_end_index = except_line_index + 1\n",
        "             while except_block_end_index < len(main_function_lines) and (main_function_lines[except_block_end_index].strip() == '' or main_function_lines[except_block_end_index].strip().startswith('#') or main_function_lines[except_block_end_index].strip().startswith('print') or main_function_lines[except_block_end_index].strip().startswith('history_df_signals =')):\n",
        "                 except_block_end_index += 1\n",
        "\n",
        "\n",
        "             # Extract the lines within the try-except block (including try and except)\n",
        "             try_except_block = main_function_lines[try_line_index:except_block_end_index]\n",
        "\n",
        "             # Reconstruct the try-except block with correct indentation\n",
        "             reconstructed_block = ['    try:\\n'] # Ensure correct indentation for try\n",
        "             in_except_block = False\n",
        "             for line in try_except_block[1:]:\n",
        "                  if line.strip().startswith('except'):\n",
        "                     reconstructed_block.append(line)\n",
        "                     in_except_block = True\n",
        "                  elif in_except_block:\n",
        "                      reconstructed_block.append(f'    {line.lstrip()}') # Ensure correct indentation for except block content\n",
        "                  else:\n",
        "                      reconstructed_block.append(f'        {line.lstrip()}') # Ensure correct indentation for try block content\n",
        "\n",
        "\n",
        "             # Replace the original try-except block with the reconstructed one\n",
        "             modified_main_function_lines = (\n",
        "                main_function_lines[:try_line_index] +\n",
        "                reconstructed_block +\n",
        "                main_function_lines[except_block_end_index:]\n",
        "             )\n",
        "\n",
        "             # Reconstruct the full lines\n",
        "             lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "             # Write the modified content back to the file\n",
        "             with open('signal_bot/main.py', 'w') as f:\n",
        "                 f.writelines(lines)\n",
        "        else:\n",
        "            print(\"Could not find the except block in main function.\")\n",
        "    else:\n",
        "        print(\"Could not find the try block in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find the except block in main function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9f4c19d",
        "outputId": "98e51cf7-05a8-454b-e951-2b5dc693ef31"
      },
      "source": [
        "!python signal_bot/main.py"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  File \"/content/signal_bot/main.py\", line 34\n",
            "    try:\n",
            "    ^^^\n",
            "SyntaxError: expected 'except' or 'finally' block\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53921d51"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Remove the 'return' statement from the historical data check\n",
        "    modified_main_function_lines = [line for line in main_function_lines if 'return # Exit the function if data is not available' not in line]\n",
        "\n",
        "    # Find the start of the historical data processing block (after the check for 'prices')\n",
        "    history_df_creation_line_index = -1\n",
        "    for i, line in enumerate(modified_main_function_lines):\n",
        "         if 'history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])' in line:\n",
        "             history_df_creation_line_index = i\n",
        "             break\n",
        "\n",
        "\n",
        "    # Find the end of the historical data processing block (after saving historical signals)\n",
        "    history_signals_save_line_index = -1\n",
        "    for i, line in enumerate(modified_main_function_lines):\n",
        "        if 'history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)' in line:\n",
        "            history_signals_save_line_index = i\n",
        "            break\n",
        "\n",
        "    if history_df_creation_line_index != -1 and history_signals_save_line_index != -1:\n",
        "        # Extract the historical data processing block (including the check for 'prices' but excluding the return)\n",
        "        historical_processing_lines = modified_main_function_lines[history_df_creation_line_index - 2:history_signals_save_line_index + 1] # Adjust index to include the 'if \"prices\" not in history_data:' and print lines\n",
        "\n",
        "        # Wrap the historical data processing block in a try-except block\n",
        "        wrapped_lines = [\n",
        "            '    try:\\n'\n",
        "        ]\n",
        "        for line in historical_processing_lines:\n",
        "            wrapped_lines.append(f'    {line}') # Indent the original lines\n",
        "        wrapped_lines.extend([\n",
        "            '    except Exception as e:\\n', # Catch a general exception for robustness\n",
        "            '        print(f\"Error processing historical data: {e}. Skipping historical signals.\")\\n',\n",
        "            '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\\n',\n",
        "        ])\n",
        "\n",
        "        # Replace the original historical processing lines with the wrapped ones\n",
        "        modified_main_function_lines = (\n",
        "            modified_main_function_lines[:history_df_creation_line_index - 2] + # Adjust index\n",
        "            wrapped_lines +\n",
        "            modified_main_function_lines[history_signals_save_line_index + 1:]\n",
        "        )\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the historical data creation or save line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "760fcbe7",
        "outputId": "4a655fc8-9d01-44a2-d306-b5550c0dc4cb"
      },
      "source": [
        "!python signal_bot/main.py"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Data fetched and saved.\n",
            "Computing technical indicators...\n",
            "Fetching historical data for bitcoin...\n",
            "Historical data fetched.\n",
            "Error: Historical data not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b534d242"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Find the start of the historical data processing block\n",
        "    history_fetch_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'history_data = get_coin_history(coin_id, days=\"30\")' in line:\n",
        "            history_fetch_line_index = i\n",
        "            break\n",
        "\n",
        "    # Find the end of the historical data processing block (after saving historical signals)\n",
        "    history_signals_save_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)' in line:\n",
        "            history_signals_save_line_index = i\n",
        "            break\n",
        "\n",
        "    if history_fetch_line_index != -1 and history_signals_save_line_index != -1:\n",
        "        # Extract the historical data processing block\n",
        "        historical_processing_lines = main_function_lines[history_fetch_line_index:history_signals_save_line_index + 1]\n",
        "\n",
        "        # Wrap the historical data processing block in a try-except block\n",
        "        wrapped_lines = [\n",
        "            '    try:\\n'\n",
        "        ]\n",
        "        for line in historical_processing_lines:\n",
        "            wrapped_lines.append(f'    {line}') # Indent the original lines\n",
        "        wrapped_lines.extend([\n",
        "            '    except Exception as e:\\n', # Catch a general exception for robustness\n",
        "            '        print(f\"Error processing historical data: {e}. Skipping historical signals.\")\\n',\n",
        "            '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\\n',\n",
        "        ])\n",
        "\n",
        "        # Replace the original historical processing lines with the wrapped ones\n",
        "        modified_main_function_lines = (\n",
        "            main_function_lines[:history_fetch_line_index] +\n",
        "            wrapped_lines +\n",
        "            main_function_lines[history_signals_save_line_index + 1:]\n",
        "        )\n",
        "\n",
        "        # Find the line where top 10 signals are generated\n",
        "        top10_signal_generation_line_index = -1\n",
        "        for i, line in enumerate(modified_main_function_lines):\n",
        "            if 'df_signals = generate_signal(df_ind)' in line:\n",
        "                top10_signal_generation_line_index = i\n",
        "                break\n",
        "\n",
        "        if top10_signal_generation_line_index != -1:\n",
        "             # Add print statements to confirm signal generation and display head\n",
        "            display_lines = [\n",
        "                '\\n',\n",
        "                '    print(\"Historical signals generated (if successful):\")\\n',\n",
        "                '    if not history_df_signals.empty:\\n',\n",
        "                '        print(history_df_signals.head())\\n',\n",
        "                '    else:\\n',\n",
        "                '        print(\"No historical signals generated.\")\\n',\n",
        "                '\\n',\n",
        "                '    print(\"Top 10 snapshot signals generated:\")\\n',\n",
        "                '    print(df_signals.head())\\n',\n",
        "                '\\n'\n",
        "            ]\n",
        "\n",
        "            # Insert the display lines after the top 10 signal generation and saving lines\n",
        "            top10_signals_save_line_index = -1\n",
        "            for i in range(top10_signal_generation_line_index + 1, len(modified_main_function_lines)):\n",
        "                 if 'df_signals.to_csv' in modified_main_function_lines[i]:\n",
        "                     top10_signals_save_line_index = i\n",
        "                     break\n",
        "\n",
        "            if top10_signals_save_line_index != -1:\n",
        "                modified_main_function_lines = (\n",
        "                    modified_main_function_lines[:top10_signals_save_line_index + 1] +\n",
        "                    display_lines +\n",
        "                    modified_main_function_lines[top10_signals_save_line_index + 1:]\n",
        "                )\n",
        "            else:\n",
        "                 print(\"Could not find the top 10 signals save line in main function.\")\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the historical data fetch or save line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cef93b4d",
        "outputId": "845deb75-8f60-4c71-d363-930d0a4d3967"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Find the start of the historical data processing block\n",
        "    history_fetch_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'history_data = get_coin_history(coin_id, days=\"30\")' in line:\n",
        "            history_fetch_line_index = i\n",
        "            break\n",
        "\n",
        "    if history_fetch_line_index != -1:\n",
        "        # Find the end of the historical data processing block (before the original signal generation for df_ind)\n",
        "        original_signal_generation_line_index = -1\n",
        "        for i, line in enumerate(main_function_lines):\n",
        "             if 'df_signals = generate_signal(df_ind)' in line:\n",
        "                 original_signal_generation_line_index = i\n",
        "                 break\n",
        "\n",
        "        if original_signal_generation_line_index != -1:\n",
        "             # Extract the historical data processing block\n",
        "            historical_processing_lines = main_function_lines[history_fetch_line_index:original_signal_generation_line_index]\n",
        "\n",
        "            # Wrap the historical data processing block in a try-except block\n",
        "            wrapped_lines = [\n",
        "                '    try:\\n'\n",
        "            ]\n",
        "            for line in historical_processing_lines:\n",
        "                wrapped_lines.append(f'    {line}') # Indent the original lines\n",
        "            wrapped_lines.extend([\n",
        "                '    except KeyError:\\n',\n",
        "                '        print(\"Error processing historical data. Skipping historical signals.\")\\n',\n",
        "                '        history_df_signals = pd.DataFrame() # Create an empty DataFrame to avoid NameError later\\n',\n",
        "            ])\n",
        "\n",
        "\n",
        "            # Replace the original historical processing lines with the wrapped ones\n",
        "            modified_main_function_lines = (\n",
        "                main_function_lines[:history_fetch_line_index] +\n",
        "                wrapped_lines +\n",
        "                main_function_lines[original_signal_generation_line_index:]\n",
        "            )\n",
        "\n",
        "            # Reconstruct the full lines\n",
        "            lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "            # Write the modified content back to the file\n",
        "            with open('signal_bot/main.py', 'w') as f:\n",
        "                f.writelines(lines)\n",
        "        else:\n",
        "            print(\"Could not find the original signal generation line in main function.\")\n",
        "    else:\n",
        "        print(\"Could not find the historical data fetch line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not find the original signal generation line in main function.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4e1d033c",
        "outputId": "08f804a4-ce2f-4027-dd20-af00bc072316"
      },
      "source": [
        "!python signal_bot/main.py"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Data fetched and saved.\n",
            "Computing technical indicators...\n",
            "Fetching historical data for bitcoin...\n",
            "Historical data fetched.\n",
            "Error: Historical data not available.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2a60cf7b"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Find the line where historical data is fetched\n",
        "    history_fetch_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'history_data = get_coin_history(coin_id, days=\"30\")' in line:\n",
        "            history_fetch_line_index = i\n",
        "            break\n",
        "\n",
        "    if history_fetch_line_index != -1:\n",
        "        # Remove the inspect line if it exists\n",
        "        main_function_lines = [line for line in main_function_lines if 'Keys in history_data:' not in line]\n",
        "\n",
        "        # Find the line where the history_df DataFrame is created\n",
        "        history_df_creation_line_index = -1\n",
        "        for i, line in enumerate(main_function_lines):\n",
        "            if 'history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])' in line:\n",
        "                history_df_creation_line_index = i\n",
        "                break\n",
        "\n",
        "        if history_df_creation_line_index != -1:\n",
        "            # Add a check for the 'prices' key before creating the DataFrame\n",
        "            check_lines = [\n",
        "                '    if \"prices\" not in history_data:\\n',\n",
        "                '        print(\"Error: Historical data not available.\")\\n',\n",
        "                '        return # Exit the function if data is not available\\n',\n",
        "            ]\n",
        "\n",
        "            # Insert the check lines before the history_df creation line\n",
        "            modified_main_function_lines = (\n",
        "                main_function_lines[:history_df_creation_line_index] +\n",
        "                check_lines +\n",
        "                main_function_lines[history_df_creation_line_index:]\n",
        "            )\n",
        "\n",
        "            # Reconstruct the full lines\n",
        "            lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "            # Write the modified content back to the file\n",
        "            with open('signal_bot/main.py', 'w') as f:\n",
        "                f.writelines(lines)\n",
        "        else:\n",
        "             print(\"Could not find the history_df creation line in main function.\")\n",
        "    else:\n",
        "        print(\"Could not find the historical data fetch line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bb84976f",
        "outputId": "6fb3d06f-5668-425d-919e-379b28db044d"
      },
      "source": [
        "!python signal_bot/main.py"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Data fetched and saved.\n",
            "Computing technical indicators...\n",
            "Fetching historical data for bitcoin...\n",
            "Keys in history_data: dict_keys(['status'])\n",
            "Historical data fetched.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/signal_bot/main.py\", line 68, in <module>\n",
            "    main()\n",
            "  File \"/content/signal_bot/main.py\", line 34, in main\n",
            "    history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\n",
            "                              ~~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'prices'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c794af8f"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Find the line where historical data is fetched\n",
        "    history_fetch_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'history_data = get_coin_history(coin_id, days=\"30\")' in line:\n",
        "            history_fetch_line_index = i\n",
        "            break\n",
        "\n",
        "    if history_fetch_line_index != -1:\n",
        "        # Add a print statement to inspect the keys of history_data\n",
        "        inspect_line = '    print(f\"Keys in history_data: {history_data.keys()}\")\\n'\n",
        "\n",
        "\n",
        "        # Insert the inspect line after the history data fetch line\n",
        "        modified_main_function_lines = (\n",
        "            main_function_lines[:history_fetch_line_index + 1] +\n",
        "            [inspect_line] +\n",
        "            main_function_lines[history_fetch_line_index + 1:]\n",
        "        )\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the historical data fetch line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2f697db4",
        "outputId": "0cb4a561-44c6-45df-c36b-018eaf7ba595"
      },
      "source": [
        "!python signal_bot/main.py"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Data fetched and saved.\n",
            "Computing technical indicators...\n",
            "Fetching historical data for bitcoin...\n",
            "Historical data fetched.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/signal_bot/main.py\", line 67, in <module>\n",
            "    main()\n",
            "  File \"/content/signal_bot/main.py\", line 33, in main\n",
            "    history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\n",
            "                              ~~~~~~~~~~~~^^^^^^^^^^\n",
            "KeyError: 'prices'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ff68c79"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Add import statement for get_coin_history\n",
        "import_line = 'from utils.coingecko_api import get_coin_history\\n'\n",
        "\n",
        "# Find the position to insert imports (after existing imports)\n",
        "insert_pos = 0\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip().startswith('import') or line.strip().startswith('from'):\n",
        "        insert_pos = i + 1\n",
        "    elif not line.strip(): # Skip empty lines\n",
        "        continue\n",
        "    else:\n",
        "        break # Stop if we encounter non-import code\n",
        "\n",
        "# Insert the new import line if not already present\n",
        "if import_line not in lines:\n",
        "    lines.insert(insert_pos, import_line)\n",
        "\n",
        "# Write the modified content back to the file\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.writelines(lines)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ecfa241f",
        "outputId": "fc3c33cd-a1c5-41e1-f3d7-bc38d5de312f"
      },
      "source": [
        "!python signal_bot/main.py"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching top 10 coin data...\n",
            "Data fetched and saved.\n",
            "Computing technical indicators...\n",
            "Fetching historical data for bitcoin...\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/signal_bot/main.py\", line 66, in <module>\n",
            "    main()\n",
            "  File \"/content/signal_bot/main.py\", line 27, in main\n",
            "    history_data = get_coin_history(coin_id, days=\"30\")\n",
            "                   ^^^^^^^^^^^^^^^^\n",
            "NameError: name 'get_coin_history' is not defined\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "00cd4433"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Find the line where historical signals are saved\n",
        "    save_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'history_df_signals.to_csv' in line:\n",
        "            save_line_index = i\n",
        "            break\n",
        "\n",
        "    if save_line_index != -1:\n",
        "        # Add lines to print confirmation and the head of the dataframe\n",
        "        display_lines = [\n",
        "            '    print(\"Historical signals generated:\")\\n',\n",
        "            '    print(history_df_signals.head())\\n',\n",
        "            '\\n'\n",
        "        ]\n",
        "\n",
        "        # Insert the new lines after the save line\n",
        "        modified_main_function_lines = (\n",
        "            main_function_lines[:save_line_index + 1] +\n",
        "            display_lines +\n",
        "            main_function_lines[save_line_index + 1:]\n",
        "        )\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the historical signals save line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "78595b56"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Identify the line where signals are currently generated for df_ind\n",
        "    signal_generation_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'df_signals = generate_signal(df_ind)' in line:\n",
        "            signal_generation_line_index = i\n",
        "            break\n",
        "\n",
        "    if signal_generation_line_index != -1:\n",
        "        # Modify the line to use history_df_ind and update the output filename\n",
        "        modified_line = '    history_df_signals = generate_signal(history_df_ind)\\n'\n",
        "        save_line = '    history_df_signals.to_csv(f\"{DATA_DIR}/{coin_id}_historical_signals.csv\", index=False)\\n'\n",
        "        print_line = '    print(\"Historical signals generated and saved.\")\\n'\n",
        "\n",
        "\n",
        "        # Replace the original signal generation and saving lines with the new ones\n",
        "        # Find the save line after the signal generation line\n",
        "        save_line_index = -1\n",
        "        for i in range(signal_generation_line_index + 1, len(main_function_lines)):\n",
        "            if 'df_signals.to_csv' in main_function_lines[i]:\n",
        "                save_line_index = i\n",
        "                break\n",
        "\n",
        "        if save_line_index != -1:\n",
        "             # Delete the original signal generation and save lines\n",
        "            del main_function_lines[save_line_index]\n",
        "            del main_function_lines[signal_generation_line_index]\n",
        "\n",
        "            # Insert the new lines\n",
        "            main_function_lines.insert(signal_generation_line_index, modified_line)\n",
        "            main_function_lines.insert(signal_generation_line_index + 1, save_line)\n",
        "            main_function_lines.insert(signal_generation_line_index + 2, print_line)\n",
        "\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the signal generation line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5COiUU3asQfa"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Identify the line where indicators are computed for df\n",
        "    indicator_computation_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'df_ind = compute_indicators(df.copy())' in line:\n",
        "            indicator_computation_line_index = i\n",
        "            break\n",
        "\n",
        "    if indicator_computation_line_index != -1:\n",
        "        # New lines to insert before indicator computation\n",
        "        new_lines = [\n",
        "            '\\n',\n",
        "            '    # Fetch historical data for a specific coin (e.g., bitcoin)\\n',\n",
        "            '    coin_id = \"bitcoin\"\\n',\n",
        "            '    print(f\"Fetching historical data for {coin_id}...\")\\n',\n",
        "            '    history_data = get_coin_history(coin_id, days=\"30\")\\n',\n",
        "            '    print(\"Historical data fetched.\")\\n',\n",
        "            '\\n',\n",
        "            '    # Process historical price data into a DataFrame\\n',\n",
        "            '    # The \\'prices\\' key contains a list of [timestamp, price]\\n',\n",
        "            '    history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\\n',\n",
        "            '    # Convert timestamp to datetime and set as index (or keep as column)\\n',\n",
        "            '    history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\\n',\n",
        "            '    # Optional: Set timestamp as index for time-series analysis\\n',\n",
        "            '    # history_df = history_df.set_index(\"timestamp\")\\n',\n",
        "            '    print(\"Historical data processed into DataFrame.\")\\n',\n",
        "            '\\n',\n",
        "            '    # Compute indicators on the historical data DataFrame\\n',\n",
        "            '    print(f\"Computing technical indicators for {coin_id} historical data...\")\\n',\n",
        "            '    history_df_ind = compute_indicators(history_df.copy())\\n',\n",
        "            '    history_df_ind.to_csv(f\"{DATA_DIR}/{coin_id}_historical_with_indicators.csv\", index=False)\\n',\n",
        "            '    print(\"Indicators computed and saved for historical data.\")\\n',\n",
        "            '\\n',\n",
        "            '    # Note: Signal generation logic currently needs adjustment for time-series data\\n',\n",
        "            '    # The next step would be to adapt generate_signal to work with history_df_ind\\n',\n",
        "            '    # For now, we will keep the original signal generation on the top 10 snapshot for demonstration\\n',\n",
        "            '\\n'\n",
        "        ]\n",
        "\n",
        "        # Insert the new lines before the indicator computation line\n",
        "        modified_main_function_lines = (\n",
        "            main_function_lines[:indicator_computation_line_index] +\n",
        "            new_lines +\n",
        "            main_function_lines[indicator_computation_line_index:]\n",
        "        )\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the indicator computation line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "a9bd4785",
        "outputId": "fb06d6cd-9b77-499c-adbe-cbb4fd89465e"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Identify the line where indicators are computed for df\n",
        "    indicator_computation_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'df_ind = compute_indicators(df.copy())' in line:\n",
        "            indicator_computation_line_index = i\n",
        "            break\n",
        "\n",
        "    if indicator_computation_line_index != -1:\n",
        "        # New lines to insert before indicator computation\n",
        "        new_lines = [\n",
        "            '\\n',\n",
        "            '    # Fetch historical data for a specific coin (e.g., bitcoin)\\n',\n",
        "            '    coin_id = \"bitcoin\"\\n',\n",
        "            '    print(f\"Fetching historical data for {coin_id}...\")\\n',\n",
        "            '    history_data = get_coin_history(coin_id, days=\"30\")\\n',\n",
        "            '    print(\"Historical data fetched.\")\\n',\n",
        "            '\\n',\n",
        "            '    # Process historical price data into a DataFrame\\n',\n",
        "            '    # The 'prices' key contains a list of [timestamp, price]\\n',\n",
        "            '    history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\\n',\n",
        "            '    # Convert timestamp to datetime and set as index (or keep as column)\\n',\n",
        "            '    history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\\n',\n",
        "            '    # Optional: Set timestamp as index for time-series analysis\\n',\n",
        "            '    # history_df = history_df.set_index(\"timestamp\")\\n',\n",
        "            '    print(\"Historical data processed into DataFrame.\")\\n',\n",
        "            '\\n',\n",
        "            '    # Compute indicators on the historical data DataFrame\\n',\n",
        "            '    print(f\"Computing technical indicators for {coin_id} historical data...\")\\n',\n",
        "            '    history_df_ind = compute_indicators(history_df.copy())\\n',\n",
        "            '    history_df_ind.to_csv(f\"{DATA_DIR}/{coin_id}_historical_with_indicators.csv\", index=False)\\n',\n",
        "            '    print(\"Indicators computed and saved for historical data.\")\\n',\n",
        "            '\\n',\n",
        "            '    # Note: Signal generation logic currently needs adjustment for time-series data\\n',\n",
        "            '    # The next step would be to adapt generate_signal to work with history_df_ind\\n',\n",
        "            '    # For now, we will keep the original signal generation on the top 10 snapshot for demonstration\\n',\n",
        "            '\\n'\n",
        "        ]\n",
        "\n",
        "        # Insert the new lines before the indicator computation line\n",
        "        modified_main_function_lines = (\n",
        "            main_function_lines[:indicator_computation_line_index] +\n",
        "            new_lines +\n",
        "            main_function_lines[indicator_computation_line_index:]\n",
        "        )\n",
        "\n",
        "        # Reconstruct the full lines\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the indicator computation line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")\n"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax. Perhaps you forgot a comma? (ipython-input-683266729.py, line 40)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-683266729.py\"\u001b[0;36m, line \u001b[0;32m40\u001b[0m\n\u001b[0;31m    '    # The 'prices' key contains a list of [timestamp, price]\\n',\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax. Perhaps you forgot a comma?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "043a6a37",
        "outputId": "ecc295fc-7782-4503-e9be-cb1e41c2f777"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "# Find the start and end of the main function\n",
        "main_start = -1\n",
        "main_end = -1\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip() == 'def main():':\n",
        "        main_start = i\n",
        "    if main_start != -1 and line.strip() == '' and i + 1 < len(lines) and lines[i+1].strip().startswith('if __name__'):\n",
        "        main_end = i\n",
        "        break\n",
        "    # Handle case where main is the last function\n",
        "    if main_start != -1 and i == len(lines) - 1:\n",
        "        main_end = i + 1\n",
        "\n",
        "if main_start != -1 and main_end != -1:\n",
        "    # Extract the main function lines\n",
        "    main_function_lines = lines[main_start + 1:main_end]\n",
        "\n",
        "    # Identify the line where indicators are computed for df\n",
        "    indicator_computation_line_index = -1\n",
        "    for i, line in enumerate(main_function_lines):\n",
        "        if 'df_ind = compute_indicators(df.copy())' in line:\n",
        "            indicator_computation_line_index = i\n",
        "            break\n",
        "\n",
        "    if indicator_computation_line_index != -1:\n",
        "        # New lines to insert before indicator computation\n",
        "        new_lines = [\n",
        "            '\\n',\n",
        "            '    # Fetch historical data for a specific coin (e.g., bitcoin)\\n',\n",
        "            '    coin_id = \"bitcoin\"\\n',\n",
        "            '    print(f\"Fetching historical data for {coin_id}...\")\\n',\n",
        "            '    history_data = get_coin_history(coin_id, days=\"30\")\\n',\n",
        "            '    print(\"Historical data fetched.\")\\n',\n",
        "            '\\n',\n",
        "            '    # Process historical price data into a DataFrame\\n',\n",
        "            \"    # The 'prices' key contains a list of [timestamp, price]\\n\", # Corrected line\n",
        "            '    history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"close\"])\\n',\n",
        "            '    # Convert timestamp to datetime and set as index (or keep as column)\\n',\n",
        "            '    history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\\n',\n",
        "            '    # Optional: Set timestamp as index for time-series analysis\\n',\n",
        "            '    # history_df = history_df\\n', # Corrected line (incomplete in original)\n",
        "            '\\n' # Added a newline to complete the block\n",
        "        ]\n",
        "\n",
        "        # Insert the new lines before the indicator computation line\n",
        "        modified_main_function_lines = (\n",
        "            main_function_lines[:indicator_computation_line_index] +\n",
        "            new_lines +\n",
        "            main_function_lines[indicator_computation_line_index:]\n",
        "        )\n",
        "\n",
        "        # Reconstruct the full lines (assuming this code block was meant to write back to main.py)\n",
        "        # This part was incomplete in the user's provided code, adding the logic to write back.\n",
        "        lines = lines[:main_start + 1] + modified_main_function_lines + lines[main_end:]\n",
        "\n",
        "        # Write the modified content back to the file\n",
        "        with open('signal_bot/main.py', 'w') as f:\n",
        "            f.writelines(lines)\n",
        "    else:\n",
        "        print(\"Could not find the indicator computation line in main function.\")\n",
        "else:\n",
        "    print(\"Could not find the main function in the file.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: 'signal_bot/main.py'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2079273585.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Open the signal_bot/main.py file for editing.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'signal_bot/main.py'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mlines\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadlines\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Find the start and end of the main function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'signal_bot/main.py'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "44394091",
        "outputId": "d9fe92d4-c3e2-4bbe-ff40-e80dc6427931"
      },
      "source": [
        "# Execute the main script again\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 188\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 188)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a1f269ca",
        "outputId": "e79011e6-9ebd-408b-82d8-c93d0613efed"
      },
      "source": [
        "# Execute the main script to regenerate data files\n",
        "!python -m signal_bot.main"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 188\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 188)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "57aa2de6",
        "outputId": "99baa3ab-d0fe-4f59-e070-e001d5e1279e"
      },
      "source": [
        "# Regenerate the signal_bot/main.py file with its latest integrated content.\n",
        "import os\n",
        "\n",
        "# Ensure the signal_bot directory exists\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "os.makedirs('signal_bot/data', exist_ok=True) # Ensure data directory exists as main uses it\n",
        "\n",
        "# Content for main.py (latest integrated version)\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from .indicators.ta_utils import compute_indicators\n",
        "from .signals.signal_finder import find_signals\n",
        "from .anomaly_detector import detect_anomalies\n",
        "from .ml_logger import log_ml_features\n",
        "from .backtester import backtest_signals\n",
        "from .ml_model_trainer import train_ml_model\n",
        "from .logger import setup_logger, log_info, log_error\n",
        "from .exporter import export_to_excel, export_to_html\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    setup_logger()\n",
        "    log_info(\"Starting bot pipeline...\")\n",
        "\n",
        "    # --- Data Collection (using existing top 10 fetch for now) ---\n",
        "    log_info(\"--- Top 10 Data Pipeline ---\")\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data = get_top_coins()\n",
        "        if data is not None:\n",
        "            df_top10 = pd.DataFrame(data)\n",
        "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            top10_market_data_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "            df_top10.to_csv(top10_market_data_path, index=False)\n",
        "            log_info(\"Top 10 data fetched and saved.\")\n",
        "\n",
        "            log_info(\"Computing technical indicators for top 10 data...\")\n",
        "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "            top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "            df_ind_top10.to_csv(top10_indicators_path, index=False)\n",
        "            log_info(\"Indicators computed and saved for top 10 data.\")\n",
        "\n",
        "            log_info(\"Generating signals for top 10 snapshot data...\")\n",
        "            df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "            top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "            df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "            log_info(\"Signals generated and saved for top 10 snapshot data.\")\n",
        "\n",
        "            log_info(\"Logging ML features for top 10 data...\")\n",
        "            ml_log_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "            log_ml_features(top10_indicators_path, ml_log_path)\n",
        "            log_info(\"ML features logged for top 10 data.\")\n",
        "\n",
        "        else:\n",
        "            log_error(\"Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.\")\n",
        "            df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.\")\n",
        "        df_signals_top10 = pd.DataFrame()\n",
        "    log_info(\"--- Top 10 Data Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Anomaly Detection ---\n",
        "    log_info(\"\\\\n--- Anomaly Detection Pipeline ---\")\n",
        "    print(\"Attempting to run Anomaly Detection...\")\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if os.path.exists(full_snapshot_path):\n",
        "        try:\n",
        "            log_info(\"Running Anomaly Detection...\")\n",
        "            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "            log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "            if not anomalies.empty:\n",
        "                 print(\"Sample anomalies:\")\n",
        "                 print(anomalies.head().to_markdown(index=False))\n",
        "            else:\n",
        "                 print(\"No anomalies detected.\")\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error during Anomaly Detection: {e}.\")\n",
        "    else:\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.\")\n",
        "    log_info(\"--- Anomaly Detection Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Historical Data Processing ---\n",
        "    log_info(\"\\\\n--- Historical Data Pipeline ---\")\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals_display = pd.DataFrame()\n",
        "    historical_price_data_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_price.csv\")\n",
        "    historical_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\")\n",
        "    backtest_results_path = os.path.join(DATA_DIR, \"signal_backtest.csv\")\n",
        "\n",
        "\n",
        "    print(f\"Attempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "        if history_data is not None and \"prices\" in history_data:\n",
        "            log_info(\"Historical data fetched.\")\n",
        "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"current_price\"])\n",
        "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "            history_df.to_csv(historical_price_data_path, index=False)\n",
        "            log_info(\"Historical data processed and saved.\")\n",
        "\n",
        "            log_info(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "            history_df[\"close\"] = history_df[\"current_price\"]\n",
        "            history_df_ind = compute_indicators(history_df.copy())\n",
        "            history_historical_indicators_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_with_indicators.csv\")\n",
        "            history_df_ind.to_csv(history_historical_indicators_path, index=False)\n",
        "            log_info(\"Indicators computed and saved for historical data.\")\n",
        "\n",
        "            log_info(f\"Generating signals for {coin_id} historical data...\")\n",
        "            history_df_signals = find_signals(history_df_ind.copy())\n",
        "            history_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\")\n",
        "            history_df_signals.to_csv(history_signals_path, index=False)\n",
        "            log_info(\"Historical signals generated and saved.\")\n",
        "            history_df_signals_display = history_df_signals\n",
        "\n",
        "            # --- Backtesting Historical Signals ---\n",
        "            log_info(f\"Attempting to backtest historical signals for {coin_id}...\")\n",
        "            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)\n",
        "            log_info(\"Backtesting completed.\")\n",
        "            if not backtest_results_df.empty:\n",
        "                print(\"Backtest Results (head):\")\n",
        "                print(backtest_results_df.head().to_markdown(index=False))\n",
        "\n",
        "                # --- Export Backtest Results ---\n",
        "                log_info(\"Exporting backtest results...\")\n",
        "                excel_output_path = os.path.join(DATA_DIR, \"signal_backtest_report.xlsx\")\n",
        "                html_output_path = os.path.join(DATA_DIR, \"signal_backtest_report.html\")\n",
        "                export_to_excel(backtest_results_path, excel_output_path)\n",
        "                export_to_html(backtest_results_path, html_output_path)\n",
        "                log_info(f\"Backtest results exported to {excel_output_path} and {html_output_path}.\")\n",
        "\n",
        "\n",
        "                # --- ML Model Training ---\n",
        "                log_info(\"Attempting to train ML model...\")\n",
        "                if os.path.exists(backtest_results_path):\n",
        "                    try:\n",
        "                        model, report = train_ml_model(backtest_results_path)\n",
        "                        log_info(\"ML model training completed.\")\n",
        "                        print(\"Classification Report:\")\n",
        "                        for label, metrics in report.items():\n",
        "                             if isinstance(metrics, dict):\n",
        "                                  print(f\"  {label}:\")\n",
        "                                  for metric, value in metrics.items():\n",
        "                                       print(f\"    {metric}: {value:.4f}\")\n",
        "                             else:\n",
        "                                  print(f\"  {label}: {metrics:.4f}\")\n",
        "\n",
        "                        # Optional: Save the trained model\n",
        "                        # import joblib\n",
        "                        # joblib.dump(model, os.path.join(DATA_DIR, 'ml_model.pkl'))\n",
        "                        # log_info(\"ML model saved.\")\n",
        "\n",
        "\n",
        "                    except ValueError as ve:\n",
        "                        log_error(f\"Error during ML training: {ve}. Skipping training.\")\n",
        "                    except Exception as e:\n",
        "                        log_error(f\"Error during ML training: {e}. Skipping training.\")\n",
        "                else:\n",
        "                    log_info(f\"Warning: Backtest results file not found at {backtest_results_path}. Skipping ML model training.\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                log_info(\"No backtest results to display. Skipping ML model training.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "             log_info(\"Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error processing historical data: {e}. Skipping historical signals.\")\n",
        "    log_info(\"--- Historical Data Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Display Results (for Top 10 snapshot) ---\n",
        "    print(\"\\\\n--- Results (Top 10 Snapshot) ---\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False))\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated due to processing errors or no data.\")\n",
        "\n",
        "    # --- Display Results (for Historical, if processed) ---\n",
        "    print(\"\\\\n--- Results (Historical Data) ---\")\n",
        "    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:\n",
        "         print(\"Historical signals (tail):\")\n",
        "         print(history_df_signals_display.tail().to_markdown(index=False))\n",
        "         print(\"\\nHistorical signals (sample):\")\n",
        "         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))\n",
        "    else:\n",
        "         print(\"No historical signals generated or historical data processing skipped.\")\n",
        "\n",
        "\n",
        "    log_info(\"Bot pipeline finished.\") # This print statement might be the issue. Let's check the end of the string.\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\" # The string literal ends here. Ensure the last lines are correct.\n",
        "\n",
        "# Write the content to signal_bot/main.py\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)\n",
        "\n",
        "print(\"signal_bot/main.py regenerated successfully.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signal_bot/main.py regenerated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SDqUbinSHubN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "877e0060"
      },
      "source": [
        "# Open the signal_bot/main.py file for editing.\n",
        "# Add import statements at the beginning of the file for the get_top_coins function from utils.coingecko_api,\n",
        "# the compute_indicators function from indicators.ta_utils, and the generate_signal function from signals.generate_signals.\n",
        "\n",
        "with open('signal_bot/main.py', 'r') as f:\n",
        "    lines = f.readlines()\n",
        "\n",
        "import_lines = [\n",
        "    'from utils.coingecko_api import get_top_coins\\n',\n",
        "    'from indicators.ta_utils import compute_indicators\\n',\n",
        "    'from signals.generate_signals import generate_signal\\n'\n",
        "]\n",
        "\n",
        "# Find the position to insert imports (after existing imports or at the beginning)\n",
        "insert_pos = 0\n",
        "for i, line in enumerate(lines):\n",
        "    if line.strip().startswith('import') or line.strip().startswith('from'):\n",
        "        insert_pos = i + 1\n",
        "    elif not line.strip(): # Skip empty lines\n",
        "        continue\n",
        "    else:\n",
        "        break # Stop if we encounter non-import code\n",
        "\n",
        "# Insert the new import lines, avoiding duplicates\n",
        "for imp_line in import_lines:\n",
        "    if imp_line not in lines:\n",
        "        lines.insert(insert_pos, imp_line)\n",
        "        insert_pos += 1 # Adjust insertion position for the next import\n",
        "\n",
        "# Write the modified content back to the file\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.writelines(lines)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5dbf56ab-c8f3-4a0c-e81a-6e8871ca8beb",
        "id": "TMqAp64jHy3P"
      },
      "source": [
        "# Run the Streamlit dashboard again\n",
        "!streamlit run signal_bot/dashboard.py"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/bin/bash: line 1: streamlit: command not found\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68ed741e",
        "outputId": "1ea5ab98-b9a8-42fd-fbf2-408b10de97d2"
      },
      "source": [
        "# Step 3: Run the Streamlit dashboard\n",
        "# Ensure the previous cell has finished executing before running this one.\n",
        "# This cell will stay in a 'working' state while the dashboard server is running.\n",
        "print(\"Attempting to run Streamlit dashboard...\")\n",
        "!python -m streamlit run signal_bot/dashboard.py"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to run Streamlit dashboard...\n",
            "Usage: streamlit run [OPTIONS] TARGET [ARGS]...\n",
            "Try 'streamlit run --help' for help.\n",
            "\n",
            "Error: Invalid value: File does not exist: signal_bot/dashboard.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "11f1127d",
        "outputId": "f2138f9b-cccb-4836-9b6a-5d3d48ede3c0"
      },
      "source": [
        "# Regenerate the signal_bot/dashboard.py file with its latest content.\n",
        "import os\n",
        "\n",
        "# Ensure the signal_bot directory exists\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "os.makedirs('signal_bot/data', exist_ok=True) # Ensure data directory exists as dashboard reads from it\n",
        "\n",
        "# Content for dashboard.py (latest integrated version)\n",
        "dashboard_py_content = \"\"\"# dashboard.py\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Define DATA_DIR\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "\n",
        "st.title(\"Crypto Signal Dashboard\")\n",
        "\n",
        "tab1, tab2, tab3 = st.tabs([\"Signals\", \"Anomalies\", \"Indicators\"])\n",
        "\n",
        "# Ensure files exist before trying to read\n",
        "signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "anomalies_dir = DATA_DIR\n",
        "indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "\n",
        "with tab1:\n",
        "    st.subheader(\"Signal Feed (Top 10)\")\n",
        "    if os.path.exists(signals_path):\n",
        "        try:\n",
        "            signals = pd.read_csv(signals_path)\n",
        "            st.dataframe(signals)\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading signals data: {e}\")\n",
        "    else:\n",
        "        st.info(f\"Signal data not found at {signals_path}. Run the main pipeline first.\")\n",
        "\n",
        "with tab2:\n",
        "    st.subheader(\"Anomalies\")\n",
        "    anomaly_files = [f for f in os.listdir(anomalies_dir) if f.startswith('anomalies_') and f.endswith('.csv')]\n",
        "    anomaly_files.sort(reverse=True)\n",
        "\n",
        "    if anomaly_files:\n",
        "        latest_anomaly_file = os.path.join(anomalies_dir, anomaly_files[0])\n",
        "        st.info(f\"Displaying latest anomalies from: {latest_anomaly_file}\")\n",
        "        try:\n",
        "            anomalies = pd.read_csv(latest_anomaly_file)\n",
        "            st.dataframe(anomalies)\n",
        "        except Exception as e:\n",
        "             st.error(f\"Error loading anomalies data from {latest_anomaly_file}: {e}\")\n",
        "    else:\n",
        "        st.info(f\"No anomaly files found in {anomalies_dir}. Run the main pipeline with anomaly detection enabled.\")\n",
        "\n",
        "with tab3:\n",
        "    st.subheader(\"Top 10 Indicators\")\n",
        "    if os.path.exists(indicators_path):\n",
        "        try:\n",
        "            indicators = pd.read_csv(indicators_path)\n",
        "            chart_cols = [\"rsi\", \"macd_diff\"]\n",
        "            available_chart_cols = [col for col in chart_cols if col in indicators.columns and pd.api.types.is_numeric_dtype(indicators[col])]\n",
        "\n",
        "            if available_chart_cols:\n",
        "                 st.subheader(\"Indicator Trends (RSI, MACD Diff)\")\n",
        "                 if not indicators[available_chart_cols].dropna().empty:\n",
        "                      st.line_chart(indicators[available_chart_cols])\n",
        "                 else:\n",
        "                      st.info(f\"Indicator data for charting ({available_chart_cols}) contains only NaN values or is empty. Cannot display chart.\")\n",
        "            elif chart_cols:\n",
        "                 st.warning(f\"Required numeric columns for charting not found or are not numeric: {chart_cols}\")\n",
        "            else:\n",
        "                 st.info(\"No indicator data available for charting.\")\n",
        "\n",
        "            st.subheader(\"Full Indicator Data\")\n",
        "            st.dataframe(indicators)\n",
        "\n",
        "        except Exception as e:\n",
        "            st.error(f\"Error loading indicators data: {e}\")\n",
        "    else:\n",
        "        st.info(f\"Indicator data not found at {indicators_path}. Run the main pipeline first.\")\n",
        "\n",
        "st.sidebar.subheader(\"How to run the dashboard:\")\n",
        "st.sidebar.markdown(\"1. Ensure you have Streamlit installed (`pip install streamlit`).\")\n",
        "st.sidebar.markdown(\"2. Run the main pipeline (`python -m signal_bot.main`) to generate data files.\")\n",
        "st.sidebar.markdown(\"3. Open a terminal in your project's root directory (`/content/` in Colab).\")\n",
        "st.sidebar.markdown(\"4. Run the command: `streamlit run signal_bot/dashboard.py`\")\n",
        "st.sidebar.markdown(\"5. A local URL will be provided to view the dashboard.\")\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to signal_bot/dashboard.py\n",
        "with open('signal_bot/dashboard.py', 'w') as f:\n",
        "    f.write(dashboard_py_content)\n",
        "\n",
        "print(\"signal_bot/dashboard.py regenerated successfully.\")"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signal_bot/dashboard.py regenerated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3319a23",
        "outputId": "ece6993a-5023-4840-c3fd-451cd5968f1b"
      },
      "source": [
        "# Step 2: Run the main pipeline to generate data files\n",
        "# This will fetch data, compute indicators, generate signals, backtest, and log\n",
        "print(\"Running main pipeline...\")\n",
        "!python -m signal_bot.main\n",
        "print(\"Main pipeline execution completed.\")"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running main pipeline...\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 189\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 189)\n",
            "Main pipeline execution completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cdab08b6",
        "outputId": "505f22a3-5dcc-4f66-978a-b3473efe08d9"
      },
      "source": [
        "# Step 1: Install dependencies (including streamlit)\n",
        "# Ensure required libraries are installed\n",
        "!pip install -q -r signal_bot/requirements.txt\n",
        "print(\"Dependencies installed.\")\n",
        "\n",
        "# Optional: Verify streamlit installation\n",
        "!pip show streamlit"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dependencies installed.\n",
            "Name: streamlit\n",
            "Version: 1.48.1\n",
            "Summary: A faster way to build and share data apps\n",
            "Home-page: https://streamlit.io\n",
            "Author: Snowflake Inc\n",
            "Author-email: hello@streamlit.io\n",
            "License: Apache License 2.0\n",
            "Location: /usr/local/lib/python3.12/dist-packages\n",
            "Requires: altair, blinker, cachetools, click, gitpython, numpy, packaging, pandas, pillow, protobuf, pyarrow, pydeck, requests, tenacity, toml, tornado, typing-extensions, watchdog\n",
            "Required-by: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "673d7f63",
        "outputId": "ec74e7aa-ab42-41b5-d04b-22ec4f38541d"
      },
      "source": [
        "# Regenerate the signal_bot/main.py file with its latest integrated content (fixing syntax error).\n",
        "import os\n",
        "\n",
        "# Ensure the signal_bot directory and data directory exist\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "os.makedirs('signal_bot/data', exist_ok=True)\n",
        "\n",
        "# Content for main.py (latest integrated version)\n",
        "main_py_content = \"\"\"# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins, get_coin_history\n",
        "from .indicators.ta_utils import compute_indicators\n",
        "# from .signals.generate_signals import generate_signal\n",
        "from .signals.signal_finder import find_signals\n",
        "from .anomaly_detector import detect_anomalies\n",
        "from .ml_logger import log_ml_features\n",
        "from .backtester import backtest_signals\n",
        "from .ml_model_trainer import train_ml_model\n",
        "from .logger import setup_logger, log_info, log_error\n",
        "from .exporter import export_to_excel, export_to_html\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    setup_logger()\n",
        "    log_info(\"Starting bot pipeline...\")\n",
        "\n",
        "    # --- Data Collection (using existing top 10 fetch for now) ---\n",
        "    log_info(\"--- Top 10 Data Pipeline ---\")\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    try:\n",
        "        data = get_top_coins()\n",
        "        if data is not None:\n",
        "            df_top10 = pd.DataFrame(data)\n",
        "            df_top10[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            top10_market_data_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "            df_top10.to_csv(top10_market_data_path, index=False)\n",
        "            log_info(\"Top 10 data fetched and saved.\")\n",
        "\n",
        "            log_info(\"Computing technical indicators for top 10 data...\")\n",
        "            df_top10[\"close\"] = df_top10[\"current_price\"]\n",
        "            df_ind_top10 = compute_indicators(df_top10.copy())\n",
        "            top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "            df_ind_top10.to_csv(top10_indicators_path, index=False)\n",
        "            log_info(\"Indicators computed and saved for top 10 data.\")\n",
        "\n",
        "            log_info(\"Generating signals for top 10 snapshot data...\")\n",
        "            df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "            top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "            df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "            log_info(\"Signals generated and saved for top 10 snapshot data.\")\n",
        "\n",
        "            log_info(\"Logging ML features for top 10 data...\")\n",
        "            ml_log_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "            log_ml_features(top10_indicators_path, ml_log_path)\n",
        "            log_info(\"ML features logged for top 10 data.\")\n",
        "\n",
        "        else:\n",
        "            log_error(\"Failed to fetch top 10 coin data. Skipping subsequent steps for top 10.\")\n",
        "            df_signals_top10 = pd.DataFrame()\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error processing top 10 data pipeline: {e}. Skipping subsequent steps for top 10.\")\n",
        "        df_signals_top10 = pd.DataFrame()\n",
        "    log_info(\"--- Top 10 Data Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Anomaly Detection ---\n",
        "    log_info(\"\\\\n--- Anomaly Detection Pipeline ---\")\n",
        "    print(\"Attempting to run Anomaly Detection...\")\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if os.path.exists(full_snapshot_path):\n",
        "        try:\n",
        "            log_info(\"Running Anomaly Detection...\")\n",
        "            path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "            log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "            if not anomalies.empty:\n",
        "                 print(\"Sample anomalies:\")\n",
        "                 print(anomalies.head().to_markdown(index=False))\n",
        "            else:\n",
        "                 print(\"No anomalies detected.\")\n",
        "        except Exception as e:\n",
        "            log_error(f\"Error during Anomaly Detection: {e}.\")\n",
        "    else:\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping Anomaly Detection.\")\n",
        "    log_info(\"--- Anomaly Detection Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Historical Data Processing ---\n",
        "    log_info(\"\\\\n--- Historical Data Pipeline ---\")\n",
        "    coin_id = \"bitcoin\"\n",
        "    history_df_signals_display = pd.DataFrame()\n",
        "    historical_price_data_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_price.csv\")\n",
        "    historical_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\")\n",
        "    backtest_results_path = os.path.join(DATA_DIR, \"signal_backtest.csv\")\n",
        "\n",
        "\n",
        "    print(f\"Attempting to fetch and process historical data for {coin_id}...\")\n",
        "    try:\n",
        "        history_data = get_coin_history(coin_id, days=\"30\")\n",
        "        if history_data is not None and \"prices\" in history_data:\n",
        "            log_info(\"Historical data fetched.\")\n",
        "            history_df = pd.DataFrame(history_data[\"prices\"], columns=[\"timestamp\", \"current_price\"])\n",
        "            history_df[\"timestamp\"] = pd.to_datetime(history_df[\"timestamp\"], unit=\"ms\")\n",
        "            history_df.to_csv(historical_price_data_path, index=False)\n",
        "            log_info(\"Historical data processed and saved.\")\n",
        "\n",
        "            log_info(f\"Computing technical indicators for {coin_id} historical data...\")\n",
        "            history_df[\"close\"] = history_df[\"current_price\"]\n",
        "            history_df_ind = compute_indicators(history_df.copy())\n",
        "            history_historical_indicators_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_with_indicators.csv\")\n",
        "            history_df_ind.to_csv(history_historical_indicators_path, index=False)\n",
        "            log_info(\"Indicators computed and saved for historical data.\")\n",
        "\n",
        "            log_info(f\"Generating signals for {coin_id} historical data...\")\n",
        "            history_df_signals = find_signals(history_df_ind.copy())\n",
        "            history_signals_path = os.path.join(DATA_DIR, f\"{coin_id}_historical_signals.csv\")\n",
        "            history_df_signals.to_csv(history_signals_path, index=False)\n",
        "            log_info(\"Historical signals generated and saved.\")\n",
        "            history_df_signals_display = history_df_signals\n",
        "\n",
        "            # --- Backtesting Historical Signals ---\n",
        "            log_info(f\"Attempting to backtest historical signals for {coin_id}...\")\n",
        "            backtest_results_df = backtest_signals(historical_signals_path, historical_price_data_path)\n",
        "            log_info(\"Backtesting completed.\")\n",
        "            if not backtest_results_df.empty:\n",
        "                print(\"Backtest Results (head):\")\n",
        "                print(backtest_results_df.head().to_markdown(index=False))\n",
        "\n",
        "                # --- Export Backtest Results ---\n",
        "                log_info(\"Exporting backtest results...\")\n",
        "                excel_output_path = os.path.join(DATA_DIR, \"signal_backtest_report.xlsx\")\n",
        "                html_output_path = os.path.join(DATA_DIR, \"signal_backtest_report.html\")\n",
        "                export_to_excel(backtest_results_path, excel_output_path)\n",
        "                export_to_html(backtest_results_path, html_output_path)\n",
        "                log_info(f\"Backtest results exported to {excel_output_path} and {html_output_path}.\")\n",
        "\n",
        "\n",
        "                # --- ML Model Training ---\n",
        "                log_info(\"Attempting to train ML model...\")\n",
        "                if os.path.exists(backtest_results_path):\n",
        "                    try:\n",
        "                        model, report = train_ml_model(backtest_results_path)\n",
        "                        log_info(\"ML model training completed.\")\n",
        "                        print(\"Classification Report:\")\n",
        "                        for label, metrics in report.items():\n",
        "                             if isinstance(metrics, dict):\n",
        "                                  print(f\"  {label}:\")\n",
        "                                  for metric, value in metrics.items():\n",
        "                                       print(f\"    {metric}: {value:.4f}\")\n",
        "                             else:\n",
        "                                  print(f\"  {label}: {metrics:.4f}\")\n",
        "\n",
        "                        # Optional: Save the trained model\n",
        "                        # import joblib\n",
        "                        # joblib.dump(model, os.path.join(DATA_DIR, 'ml_model.pkl'))\n",
        "                        # log_info(\"ML model saved.\")\n",
        "\n",
        "\n",
        "                    except ValueError as ve:\n",
        "                        log_error(f\"Error during ML training: {ve}. Skipping training.\")\n",
        "                    except Exception as e:\n",
        "                        log_error(f\"Error during ML training: {e}. Skipping training.\")\n",
        "                else:\n",
        "                    log_info(f\"Warning: Backtest results file not found at {backtest_results_path}. Skipping ML model training.\")\n",
        "\n",
        "\n",
        "            else:\n",
        "                log_info(\"No backtest results to display. Skipping ML model training.\")\n",
        "\n",
        "\n",
        "        else:\n",
        "             log_info(\"Historical data fetching failed or 'prices' key not found. Skipping historical data processing and backtesting.\")\n",
        "\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error processing historical data: {e}. Skipping historical signals.\")\n",
        "    log_info(\"--- Historical Data Pipeline Finished ---\")\n",
        "\n",
        "\n",
        "    # --- Display Results (for Top 10 snapshot) ---\n",
        "    print(\"\\\\n--- Results (Top 10 Snapshot) ---\")\n",
        "    if not df_signals_top10.empty:\n",
        "        print(df_signals_top10.head().to_markdown(index=False))\n",
        "    else:\n",
        "        print(\"No top 10 snapshot signals generated due to processing errors or no data.\")\n",
        "\n",
        "    # --- Display Results (for Historical, if processed) ---\n",
        "    print(\"\\\\n--- Results (Historical Data) ---\")\n",
        "    if 'history_df_signals_display' in locals() and not history_df_signals_display.empty:\n",
        "         print(\"Historical signals (tail):\")\n",
        "         print(history_df_signals_display.tail().to_markdown(index=False))\n",
        "         print(\"\\nHistorical signals (sample):\")\n",
        "         print(history_df_signals_display.sample(min(5, len(history_df_signals_display))).to_markdown(index=False))\n",
        "    else:\n",
        "         print(\"No historical signals generated or historical data processing skipped.\")\n",
        "\n",
        "\n",
        "    log_info(\"Bot pipeline finished.\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "\"\"\"\n",
        "\n",
        "# Write the content to signal_bot/main.py\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write(main_py_content)\n",
        "\n",
        "print(\"signal_bot/main.py regenerated successfully.\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signal_bot/main.py regenerated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "450a401c",
        "outputId": "ea0551d9-205e-4fd7-d5ce-bdba875220d6"
      },
      "source": [
        "# Regenerate requirements.txt with all known dependencies\n",
        "import os\n",
        "\n",
        "DATA_DIR = 'signal_bot' # Requirements.txt is at the root of signal_bot\n",
        "\n",
        "# Ensure signal_bot directory exists\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "requirements_content = \"\"\"pandas\n",
        "ta\n",
        "scikit-learn\n",
        "streamlit\n",
        "apscheduler\n",
        "requests\n",
        "\"\"\"\n",
        "\n",
        "with open(os.path.join(DATA_DIR, 'requirements.txt'), 'w') as f:\n",
        "    f.write(requirements_content)\n",
        "\n",
        "print(\"signal_bot/requirements.txt regenerated successfully.\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "signal_bot/requirements.txt regenerated successfully.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35c72584",
        "outputId": "d80504d9-d84b-4bd2-b8a0-66a0c645e2e6"
      },
      "source": [
        "# Step 3: Run the Streamlit dashboard\n",
        "# Ensure the previous cell has finished executing before running this one.\n",
        "# This cell will stay in a 'working' state while the dashboard server is running.\n",
        "print(\"Attempting to run Streamlit dashboard...\")\n",
        "!streamlit run signal_bot/dashboard.py"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to run Streamlit dashboard...\n",
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://35.240.212.151:8501\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n",
            "\u001b[34m  Stopping...\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b41870e1",
        "outputId": "8331536e-8cb0-4944-a35e-33c9217bc917"
      },
      "source": [
        "# Execute scheduler.py to trigger manual test execution of jobs\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scheduler...\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 103, in <module>\n",
            "    scheduler.start() # Uncommented to start the scheduler\n",
            "    ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/apscheduler/schedulers/blocking.py\", line 19, in start\n",
            "    self._main_loop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/apscheduler/schedulers/blocking.py\", line 28, in _main_loop\n",
            "    self._event.wait(wait_seconds)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 655, in wait\n",
            "    signaled = self._cond.wait(timeout)\n",
            "               ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 359, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 106
        },
        "id": "1ee193b6",
        "outputId": "e84faa7d-fca2-4084-a69a-f419a3eabf4e"
      },
      "source": [
        "# Regenerate signal_bot/utils/coingecko_api.py\n",
        "import os\n",
        "\n",
        "os.makedirs('signal_bot/utils', exist_ok=True)\n",
        "\n",
        "coingecko_api_content = \"\"\"# utils/coingecko_api.py\n",
        "import requests\n",
        "\n",
        "BASE_URL = \"https://api.coingecko.com/api/v3\"\n",
        "\n",
        "def get_top_coins(limit=10):\n",
        "    url = f\"{BASE_URL}/coins/markets\"\n",
        "    params = {\n",
        "        \"vs_currency\": \"usd\",\n",
        "        \"order\": \"market_cap_desc\",\n",
        "        \"per_page\": limit,\n",
        "        \"page\": 1,\n",
        "        \"sparkline\": \"true\"\n",
        "    }\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching top coins: {e}\")\n",
        "        return None\n",
        "\n",
        "def get_coin_history(coin_id, days=\"30\"):\n",
        "    url = f\"{BASE_URL}/coins/{coin_id}/market_chart\"\n",
        "    params = {\"vs_currency\": \"usd\", \"days\": days} # Removed 'interval': 'hourly'\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        print(f\"Error fetching historical data for {coin_id}: {e}\")\n",
        "        if 'response' in locals() and response is not None:\n",
        "             print(f\"Response status code: {response.status_code}\")\n",
        "             print(f\"Response text: {response.text}\")\n",
        "        return None\n",
        "\"\"\"\n",
        "\n",
        "with open('signal_bot/utils/coingecko_api.py', 'w') as f:\n",
        "    f.write(coingecko_api_content)\n",
        "\n",
        "print(\"signal_bot/utils/coingecko_api.py regenerated.\")\n",
        "\n",
        "# Regenerate signal_bot/indicators/ta_utils.py\n",
        "os.makedirs('signal_bot/indicators', exist_ok=True)\n",
        "\n",
        "ta_utils_content = \"\"\"# indicators/ta_utils.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "\n",
        "def compute_indicators(df):\n",
        "    df = df.copy()\n",
        "    # Ensure required columns are present before computing indicators\n",
        "    if 'close' not in df.columns:\n",
        "        print(\"Error: 'close' column not found for indicator computation.\")\n",
        "        return df # Return original df or handle error appropriately\n",
        "\n",
        "    df['rsi'] = ta.momentum.RSIIndicator(df['close']).rsi()\n",
        "    df['ema_20'] = ta.trend.EMAIndicator(df['close'], window=20).ema_indicator()\n",
        "    df['macd'] = ta.trend.MACD(df['close']).macd_diff() # Use macd_diff directly\n",
        "    bb = ta.volatility.BollingerBands(df['close'], window=20)\n",
        "    df['bb_upper'] = bb.bollinger_hband()\n",
        "    df['bb_lower'] = bb.bollinger_lband()\n",
        "    return df\n",
        "\"\"\"\n",
        "with open('signal_bot/indicators/ta_utils.py', 'w') as f:\n",
        "    f.write(ta_utils_content)\n",
        "print(\"signal_bot/indicators/ta_utils.py regenerated.\")\n",
        "\n",
        "\n",
        "# Regenerate signal_bot/signals/generate_signals.py (Keeping it for potential future use, although signal_finder is used)\n",
        "os.makedirs('signal_bot/signals', exist_ok=True)\n",
        "\n",
        "generate_signals_content = \"\"\"# signals/generate_signals.py\n",
        "import pandas as pd\n",
        "\n",
        "def generate_signal(df):\n",
        "    signals = []\n",
        "    # This is a basic signal function, find_signals is more developed\n",
        "    if df.empty or 'rsi' not in df.columns or 'macd' not in df.columns or 'close' not in df.columns or 'ema_20' not in df.columns:\n",
        "        df['signal'] = 'HOLD'\n",
        "        return df[['signal']]\n",
        "\n",
        "    latest_row = df.iloc[-1]\n",
        "\n",
        "    sigs = []\n",
        "    if latest_row[\"rsi\"] < 30:\n",
        "        sigs.append(\"BUY_RSI\")\n",
        "    # Add other basic signals if needed\n",
        "    signals.append(\", \".join(sigs) if sigs else \"HOLD\")\n",
        "\n",
        "    df['signal'] = signals # Assign the generated signals (assuming single row for simplicity)\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame\n",
        "\"\"\"\n",
        "with open('signal_bot/signals/generate_signals.py', 'w') as f:\n",
        "    f.write(generate_signals_content)\n",
        "print(\"signal_bot/signals/generate_signals.py regenerated.\")\n",
        "\n",
        "# Regenerate signal_bot/signals/signal_finder.py (Corrected version with NaN handling)\n",
        "signal_finder_content = \"\"\"# signals/signal_finder.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "def find_signals(df):\n",
        "    \"\"\"Detect signals based on indicator thresholds.\"\"\"\n",
        "    # print(\"\\\\n--- Inside find_signals ---\") # Debug print\n",
        "    # print(\"Input DataFrame columns:\", df.columns.tolist()) # Debug print\n",
        "    # print(\"Input DataFrame head:\\\\n\", df.head().to_markdown(index=False)) # Debug print\n",
        "\n",
        "\n",
        "    # Ensure DataFrame has required columns\n",
        "    required_cols = [\"rsi\", \"macd_diff\", \"current_price\", \"bb_upper\", \"bb_lower\"]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        print(f\"Warning: Missing required columns for signal generation: {required_cols}. Returning HOLD signals.\")\n",
        "        df[\"signal\"] = \"HOLD\"\n",
        "        return df\n",
        "\n",
        "    # Initialize signal column\n",
        "    df[\"signal\"] = \"HOLD\"\n",
        "\n",
        "    # Add a column for previous MACD diff for crossover check\n",
        "    df[\"macd_diff_prev\"] = df[\"macd_diff\"].shift(1)\n",
        "\n",
        "    # Check if there are any rows with valid indicator values\n",
        "    # Dropping rows with NaNs in required indicator columns for this check\n",
        "    if df[required_cols].dropna().empty:\n",
        "         # print(\"Warning: All required indicator values are NaN. Returning HOLD signals.\")\n",
        "         if 'macd_diff_prev' in df.columns:\n",
        "              df = df.drop(columns=[\"macd_diff_prev\"])\n",
        "         # print(\"--- Exiting find_signals ---\\\\n\") # Debug print\n",
        "         return df\n",
        "\n",
        "\n",
        "    for index, row in df.iterrows():\n",
        "        sigs = []\n",
        "\n",
        "        # Skip rows with NaN indicators for signal generation\n",
        "        if pd.isna(row[\"rsi\"]) or pd.isna(row[\"macd_diff\"]) or pd.isna(row[\"current_price\"]) or pd.isna(row[\"bb_upper\"]) or pd.isna(row[\"bb_lower\"]):\n",
        "            continue\n",
        "\n",
        "        # RSI signals\n",
        "        if row[\"rsi\"] < 30:\n",
        "            sigs.append(\"BUY_RSI_OVERSOLD\")\n",
        "        if row[\"rsi\"] > 70:\n",
        "            sigs.append(\"SELL_RSI_OVERBOUGHT\")\n",
        "\n",
        "        # MACD signals - check for cross above/below 0\n",
        "        if not pd.isna(row[\"macd_diff_prev\"]):\n",
        "            if row[\"macd_diff\"] > 0 and row[\"macd_diff_prev\"] <= 0:\n",
        "                sigs.append(\"BUY_MACD_CROSS\")\n",
        "            elif row[\"macd_diff\"] < 0 and row[\"macd_diff_prev\"] >= 0:\n",
        "                sigs.append(\"SELL_MACD_CROSS\")\n",
        "\n",
        "        # Bollinger Bands signals\n",
        "        if row[\"current_price\"] > row[\"bb_upper\"]:\n",
        "            sigs.append(\"OVERBOUGHT_VOLATILE\")\n",
        "        if row[\"current_price\"] < row[\"bb_lower\"]:\n",
        "            sigs.append(\"POTENTIAL_BREAKOUT\")\n",
        "\n",
        "        # Combine signals for the current row\n",
        "        df.loc[index, \"signal\"] = \", \".join(sigs) if sigs else \"HOLD\"\n",
        "\n",
        "    # Drop the temporary column\n",
        "    df = df.drop(columns=[\"macd_diff_prev\"])\n",
        "    # print(\"--- Exiting find_signals ---\\\\n\") # Debug print\n",
        "\n",
        "    return df\n",
        "\"\"\"\n",
        "with open('signal_bot/signals/signal_finder.py', 'w') as f:\n",
        "    f.write(signal_finder_content)\n",
        "print(\"signal_bot/signals/signal_finder.py regenerated.\")\n",
        "\n",
        "\n",
        "# Regenerate signal_bot/anomaly_detector.py\n",
        "anomaly_detector_content = \"\"\"# anomaly_detector.py\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from datetime import datetime\n",
        "import os\n",
        "\n",
        "def load_market_snapshot(snapshot_csv):\n",
        "    \"\"\"Load full market snapshot from CSV.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(snapshot_csv, parse_dates=[\"timestamp\"])\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Market snapshot file not found at {snapshot_csv}.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading market snapshot from {snapshot_csv}: {e}\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on other errors\n",
        "\n",
        "\n",
        "def prepare_features(df):\n",
        "    \"\"\"Create features for anomaly detection (e.g., price % change).\"\"\"\n",
        "    if df.empty or \"price_change_percentage_24h\" not in df.columns:\n",
        "         print(\"Warning: DataFrame is empty or missing 'price_change_percentage_24h' for anomaly detection.\")\n",
        "         return pd.DataFrame() # Return empty if essential data is missing\n",
        "\n",
        "    # Ensure the column is numeric, coerce errors to NaN\n",
        "    df[\"price_change_percentage_24h\"] = pd.to_numeric(df[\"price_change_percentage_24h\"], errors='coerce')\n",
        "    df = df.dropna(subset=[\"price_change_percentage_24h\"]) # Drop rows where conversion failed\n",
        "\n",
        "    if df.empty:\n",
        "         print(\"Warning: No valid data remaining after cleaning for anomaly detection.\")\n",
        "         return pd.DataFrame() # Return empty if no valid data\n",
        "\n",
        "    df[\"pct_change_24h\"] = df[\"price_change_percentage_24h\"].fillna(0) # Fill any remaining NaNs after dropna (unlikely here but safe)\n",
        "    return df[[\"id\", \"symbol\", \"name\", \"pct_change_24h\", \"timestamp\"]].copy()\n",
        "\n",
        "def run_isolation_forest(df, contamination=0.01):\n",
        "    \"\"\"Apply Isolation Forest to detect anomalies based on % change.\"\"\"\n",
        "    if df.empty or \"pct_change_24h\" not in df.columns:\n",
        "         print(\"Warning: No features available for Isolation Forest.\")\n",
        "         return pd.DataFrame() # Return empty if no features\n",
        "\n",
        "    # Ensure contamination is valid\n",
        "    if not (0 < contamination < 0.5):\n",
        "         print(f\"Warning: Invalid contamination value {contamination}. Using default (0.1) or adjusting.\")\n",
        "         # Adjust contamination if necessary, or use a default. Let's ensure it's between 0 and 0.5\n",
        "         contamination = max(0.001, min(0.499, contamination)) # Clamp value if outside reasonable range\n",
        "\n",
        "\n",
        "    # Ensure there are enough samples for Isolation Forest\n",
        "    if len(df) < 2: # Isolation Forest needs at least 2 samples\n",
        "         print(\"Warning: Not enough data points for Isolation Forest (need at least 2).\")\n",
        "         df[\"anomaly_score\"] = 1 # Tag all as not anomaly or handle as appropriate\n",
        "         return df\n",
        "\n",
        "\n",
        "    model = IsolationForest(contamination=contamination, random_state=42)\n",
        "    try:\n",
        "        # Reshape the feature data for the model\n",
        "        X = df[[\"pct_change_24h\"]].values\n",
        "        df[\"anomaly_score\"] = model.fit_predict(X)\n",
        "        return df\n",
        "    except Exception as e:\n",
        "         print(f\"Error running Isolation Forest: {e}\")\n",
        "         df[\"anomaly_score\"] = 1 # Tag all as not anomaly on error\n",
        "         return df\n",
        "\n",
        "\n",
        "def save_anomalies(df, output_dir=\"data\"):\n",
        "    \"\"\"Save anomalies to timestamped CSV.\"\"\"\n",
        "    if df.empty or \"anomaly_score\" not in df.columns:\n",
        "         print(\"Warning: No anomalies to save.\")\n",
        "         return None, pd.DataFrame()\n",
        "\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    anomalies = df[df[\"anomaly_score\"] == -1].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
        "    if anomalies.empty:\n",
        "         print(\"No anomalies found to save.\")\n",
        "         return None, pd.DataFrame()\n",
        "\n",
        "\n",
        "    path = os.path.join(output_dir, f\"anomalies_{timestamp}.csv\")\n",
        "    os.makedirs(output_dir, exist_ok=True)\n",
        "    try:\n",
        "        anomalies.to_csv(path, index=False)\n",
        "        print(f\"Anomalies saved to {path}\")\n",
        "        return path, anomalies\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving anomalies to {path}: {e}\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "\n",
        "def detect_anomalies(snapshot_csv, output_dir=\"data\"):\n",
        "    \"\"\"End-to-end anomaly detection workflow.\"\"\"\n",
        "    print(f\"Starting anomaly detection from {snapshot_csv}\")\n",
        "    df = load_market_snapshot(snapshot_csv)\n",
        "    if df.empty:\n",
        "         print(\"Anomaly detection skipped due to no data.\")\n",
        "         return None, pd.DataFrame()\n",
        "\n",
        "    features = prepare_features(df)\n",
        "    if features.empty:\n",
        "         print(\"Anomaly detection skipped due to no valid features.\")\n",
        "         return None, pd.DataFrame()\n",
        "\n",
        "    with_scores = run_isolation_forest(features)\n",
        "    if with_scores.empty: # Check if Isolation Forest returned empty\n",
        "         print(\"Anomaly detection skipped after Isolation Forest due to no results.\")\n",
        "         return None, pd.DataFrame()\n",
        "\n",
        "    path, anomalies = save_anomalies(with_scores, output_dir)\n",
        "\n",
        "    # Return the path and the anomalies DataFrame\n",
        "    return path, anomalies\n",
        "\n",
        "# Example Usage (can be removed or commented out)\n",
        "# if __name__ == '__main__':\n",
        "#     # Assuming signal_bot/data/full_market_snapshot.csv exists\n",
        "#     DATA_DIR = 'signal_bot/data'\n",
        "#     snapshot_path = os.path.join(DATA_DIR, 'full_market_snapshot.csv')\n",
        "#     if os.path.exists(snapshot_path):\n",
        "#         anomaly_file_path, detected_anomalies = detect_anomalies(snapshot_path, DATA_DIR)\n",
        "#         if anomaly_file_path:\n",
        "#             print(f\"Detected {len(detected_anomalies)} anomalies.\")\n",
        "#             # print(detected_anomalies.head().to_markdown(index=False))\n",
        "#     else:\n",
        "#          print(f\"Input file not found for anomaly detection: {snapshot_path}\")\n",
        "\n",
        "\"\"\"\n",
        "with open('signal_bot/anomaly_detector.py', 'w') as f:\n",
        "    f.write(anomaly_detector_content)\n",
        "print(\"signal_bot/anomaly_detector.py regenerated.\")\n",
        "\n",
        "# Regenerate signal_bot/ml_logger.py\n",
        "ml_logger_content = \"\"\"# ml_logger.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "def log_ml_features(indicator_csv, output_csv):\n",
        "    \"\"\"Append new features to ML training dataset.\"\"\"\n",
        "    try:\n",
        "        df = pd.read_csv(indicator_csv)\n",
        "\n",
        "        required_features = [\n",
        "            \"id\", \"timestamp\", \"current_price\",\n",
        "            \"rsi\", \"ema_20\", \"macd\", # Use 'macd' as computed in ta_utils\n",
        "            \"bb_upper\", \"bb_lower\"\n",
        "        ]\n",
        "\n",
        "        # Check if all required features are in the DataFrame and are numeric\n",
        "        missing_features = [col for col in required_features if col not in df.columns]\n",
        "        if missing_features:\n",
        "            print(f\"Warning: Missing required features for ML logging in {indicator_csv}: {missing_features}. Skipping logging for this file.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Ensure numeric columns are numeric, handle potential errors\n",
        "        numeric_cols = [\"current_price\", \"rsi\", \"ema_20\", \"macd\", \"bb_upper\", \"bb_lower\"]\n",
        "        for col in numeric_cols:\n",
        "             if col in df.columns:\n",
        "                  df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Drop rows where essential numeric features could not be converted\n",
        "        df = df.dropna(subset=[col for col in numeric_cols if col in df.columns])\n",
        "\n",
        "        if df.empty:\n",
        "            print(f\"Warning: No valid numeric data remaining for ML logging from {indicator_csv} after cleaning. Skipping logging.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "\n",
        "        features = df[required_features].copy()\n",
        "\n",
        "        if os.path.exists(output_csv):\n",
        "            existing = pd.read_csv(output_csv)\n",
        "            # Ensure timestamp is datetime for correct dropping duplicates\n",
        "            existing['timestamp'] = pd.to_datetime(existing['timestamp'])\n",
        "            features['timestamp'] = pd.to_datetime(features['timestamp'])\n",
        "\n",
        "            combined = pd.concat([existing, features]).drop_duplicates(subset=[\"id\", \"timestamp\"]).sort_values(by=[\"id\", \"timestamp\"])\n",
        "        else:\n",
        "            combined = features\n",
        "\n",
        "        # Ensure output directory exists\n",
        "        os.makedirs(os.path.dirname(output_csv) if os.path.dirname(output_csv) else '.', exist_ok=True)\n",
        "\n",
        "        combined.to_csv(output_csv, index=False)\n",
        "        return combined\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input file for ML logging not found at {indicator_csv}. Skipping logging.\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ML feature logging for {indicator_csv}: {e}. Skipping logging.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "# Example Usage (can be removed or commented out)\n",
        "# if __name__ == '__main__':\n",
        "#     # Assuming data/top10_with_indicators.csv exists\n",
        "#     DATA_DIR = 'signal_bot/data'\n",
        "#     indicators_path = os.path.join(DATA_DIR, 'top10_with_indicators.csv')\n",
        "#     ml_output_path = os.path.join(DATA_DIR, 'ml_training.csv')\n",
        "#     if os.path.exists(indicators_path):\n",
        "#         log_ml_features(indicators_path, ml_output_path)\n",
        "#     else:\n",
        "#          print(f\"Input file not found for ML logging: {indicators_path}\")\n",
        "\n",
        "\"\"\"\n",
        "with open('signal_bot/ml_logger.py', 'w') as f:\n",
        "    f.write(ml_logger_content)\n",
        "print(\"signal_bot/ml_logger.py regenerated.\")\n",
        "\n",
        "\n",
        "# Regenerate signal_bot/backtester.py\n",
        "backtester_content = \"\"\"# backtester.py\n",
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np # Import numpy for isna\n",
        "\n",
        "def backtest_signals(signal_csv, price_csv, threshold=0.05, window=6):\n",
        "    \"\"\"\n",
        "    Backtests trading signals against historical price data.\n",
        "\n",
        "    Args:\n",
        "        signal_csv (str): Path to the CSV file containing signals.\n",
        "        price_csv (str): Path to the CSV file containing historical price data.\n",
        "        threshold (float): The percentage price increase considered a successful BUY signal.\n",
        "        window (int): The number of future price points (rows) to consider after a signal.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame containing backtest results.\n",
        "    \"\"\"\n",
        "    print(\"\\\\n--- Inside backtest_signals ---\")\n",
        "    try:\n",
        "        signals = pd.read_csv(signal_csv, parse_dates=[\"timestamp\"])\n",
        "        prices = pd.read_csv(price_csv, parse_dates=[\"timestamp\"])\n",
        "        print(f\"Successfully loaded {signal_csv} and {price_csv} for backtesting.\")\n",
        "\n",
        "    except FileNotFoundError as e:\n",
        "        print(f\"Error loading data for backtesting: {e}\")\n",
        "        print(\"--- Exiting backtest_signals ---\\\\n\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading CSV files for backtesting: {e}\")\n",
        "        print(\"--- Exiting backtest_signals ---\\\\n\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "    results = []\n",
        "    # Ensure data is sorted for correct future price lookup\n",
        "    signals = signals.sort_values(\"timestamp\")\n",
        "    prices = prices.sort_values(\"timestamp\")\n",
        "\n",
        "    # Filter for BUY signals using a more explicit method\n",
        "    if 'signal' not in signals.columns:\n",
        "         print(\"Warning: 'signal' column not found in signals CSV. Cannot backtest.\")\n",
        "         print(\"--- Exiting backtest_signals ---\\\\n\")\n",
        "         return pd.DataFrame()\n",
        "\n",
        "    # Ensure the 'signal' column is treated as string\n",
        "    signals['signal'] = signals['signal'].astype(str).fillna('') # Fill NaN with empty string for contains check\n",
        "\n",
        "    # Use a boolean mask with apply and lambda for filtering\n",
        "    buy_signals = signals[signals['signal'].apply(lambda x: 'BUY' in x)].copy()\n",
        "\n",
        "    print(f\"Found {len(buy_signals)} BUY signals for backtesting after filtering.\")\n",
        "\n",
        "\n",
        "    if buy_signals.empty:\n",
        "        print(\"No BUY signals found for backtesting.\")\n",
        "        print(\"--- Exiting backtest_signals ---\\\\n\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    # Ensure prices DataFrame has necessary columns\n",
        "    if \"id\" not in prices.columns or \"timestamp\" not in prices.columns or \"current_price\" not in prices.columns:\n",
        "         print(\"Error: Required columns ('id', 'timestamp', 'current_price') missing in prices DataFrame for lookup.\")\n",
        "         print(\"--- Exiting backtest_signals ---\\\\n\")\n",
        "         return pd.DataFrame()\n",
        "\n",
        "\n",
        "    # Iterate through BUY signals and find future prices\n",
        "    for index, signal in buy_signals.iterrows():\n",
        "        # print(f\"Backtesting signal: {signal['signal']} at {signal['timestamp']}\") # Debug print (can be verbose)\n",
        "        coin_id = signal[\"id\"]\n",
        "        signal_time = signal[\"timestamp\"]\n",
        "\n",
        "        # Get the price at the signal time from the prices DataFrame\n",
        "        price_at_signal_row = prices[\n",
        "            (prices[\"id\"] == coin_id) &\n",
        "            (prices[\"timestamp\"] == signal_time)\n",
        "        ]\n",
        "        if price_at_signal_row.empty:\n",
        "             # This can happen if signals timestamp doesn't exactly match a price timestamp\n",
        "             # Find the closest price point at or before the signal time as an alternative\n",
        "             closest_price_row = prices[\n",
        "                 (prices[\"id\"] == coin_id) &\n",
        "                 (prices[\"timestamp\"] <= signal_time)\n",
        "             ].sort_values(\"timestamp\", ascending=False).head(1)\n",
        "\n",
        "             if closest_price_row.empty:\n",
        "                  # print(f\"Warning: Price at or before signal time not found for {coin_id} at {signal_time}. Skipping.\") # Debug print\n",
        "                  continue # Skip this signal if no price found\n",
        "             price_at_signal = closest_price_row.iloc[0][\"current_price\"]\n",
        "             # print(f\"Using closest price {price_at_signal} at {closest_price_row.iloc[0]['timestamp']} for signal at {signal_time}\") # Debug print\n",
        "\n",
        "        else:\n",
        "             price_at_signal = price_at_signal_row.iloc[0][\"current_price\"]\n",
        "             # print(f\"Using exact price {price_at_signal} at {signal_time}\") # Debug print\n",
        "\n",
        "\n",
        "        # Filter for future prices of this coin within the window (next 'window' data points)\n",
        "        # Find the index of the current signal in the sorted prices DataFrame for this coin\n",
        "        price_indices_for_coin = prices[prices[\"id\"] == coin_id].sort_values(\"timestamp\").index\n",
        "        signal_price_index_loc = price_indices_for_coin[price_indices_for_coin >= price_at_signal_row.index[0] if not price_at_signal_row.empty else price_indices_for_coin[price_indices_for_coin <= closest_price_row.index[0]].max()] # Find index of exact or closest price\n",
        "\n",
        "        if pd.isna(signal_price_index_loc): # Handle case where closest price lookup failed\n",
        "             # print(f\"Warning: Could not find price index for signal at {signal_time}. Skipping.\") # Debug print\n",
        "             continue\n",
        "\n",
        "\n",
        "        # Select the next 'window' rows starting from the row after the signal price index\n",
        "        try:\n",
        "            start_idx_in_full_prices = prices.index.get_loc(signal_price_index_loc) # Get the integer location in the full prices df\n",
        "            future_prices = prices.iloc[start_idx_in_full_prices + 1 : start_idx_in_full_prices + 1 + window] # Get the next 'window' rows\n",
        "            # print(f\"Selected {len(future_prices)} future prices for window.\") # Debug print\n",
        "        except IndexError:\n",
        "             # print(f\"Warning: Not enough future data points within window for {coin_id} at index {signal_price_index_loc}. Skipping backtest for this signal.\") # Debug print\n",
        "             continue # Not enough future data\n",
        "\n",
        "\n",
        "        if not future_prices.empty:\n",
        "            # Find the maximum price within the future window\n",
        "            if \"current_price\" not in future_prices.columns:\n",
        "                 print(\"Error: 'current_price' column not found in future prices DataFrame subset.\")\n",
        "                 continue\n",
        "            max_price = future_prices[\"current_price\"].max()\n",
        "            return_pct = (max_price - price_at_signal) / price_at_signal\n",
        "\n",
        "            result = {\n",
        "                \"id\": coin_id,\n",
        "                \"timestamp\": signal_time,\n",
        "                \"signal\": signal[\"signal\"],\n",
        "                \"price_at_signal\": price_at_signal,\n",
        "                \"max_future_price\": max_price,\n",
        "                \"return_pct\": return_pct,\n",
        "                \"success\": return_pct >= threshold\n",
        "            }\n",
        "            results.append(result)\n",
        "        # else: # No future prices found within the window (handled by continue in try block)\n",
        "             # print(f\"Warning: No future price data found within window for {coin_id} at {signal_time}. Skipping backtest for this signal.\") # Debug print\n",
        "\n",
        "\n",
        "    if not results:\n",
        "         print(\"No backtest results generated (either no BUY signals processed or no successful trades based on criteria).\")\n",
        "         print(\"--- Exiting backtest_signals ---\\\\n\")\n",
        "         return pd.DataFrame()\n",
        "\n",
        "    result_df = pd.DataFrame(results)\n",
        "    output_path = \"data/signal_backtest.csv\"\n",
        "    os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True)\n",
        "\n",
        "    try:\n",
        "        result_df.to_csv(output_path, index=False)\n",
        "        print(f\"Backtest results saved to {output_path}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving backtest results to {output_path}: {e}\")\n",
        "\n",
        "    print(\"--- Exiting backtest_signals ---\\\\n\")\n",
        "    return result_df\n",
        "\"\"\"\n",
        "with open('signal_bot/backtester.py', 'w') as f:\n",
        "    f.write(backtester_content)\n",
        "print(\"signal_bot/backtester.py regenerated.\")\n",
        "\n",
        "\n",
        "# Regenerate signal_bot/ml_model_trainer.py\n",
        "ml_model_trainer_content = \"\"\"# ml_model_trainer.py\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report\n",
        "import os\n",
        "\n",
        "def train_ml_model(training_csv):\n",
        "    \"\"\"\n",
        "    Trains an ML model (RandomForestClassifier) on historical signal outcomes.\n",
        "\n",
        "    Args:\n",
        "        training_csv (str): Path to the CSV file containing training data\n",
        "                            (e.g., backtest results with 'success' column).\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple containing the trained model and the classification report (dict).\n",
        "               Returns (None, None) if training fails or data is insufficient.\n",
        "    \"\"\"\n",
        "    print(\"\\\\n--- Inside train_ml_model ---\")\n",
        "    try:\n",
        "        if not os.path.exists(training_csv):\n",
        "            print(f\"Error: Training data file not found at {training_csv}.\")\n",
        "            print(\"--- Exiting train_ml_model ---\\\\n\")\n",
        "            return None, None\n",
        "\n",
        "        df = pd.read_csv(training_csv)\n",
        "        print(f\"Successfully loaded training data from {training_csv}.\")\n",
        "\n",
        "        # Ensure required columns exist and are numeric\n",
        "        required_features = [\"rsi\", \"ema_20\", \"macd\", \"bb_upper\", \"bb_lower\"] # Use 'macd' as computed\n",
        "        required_label = \"success\"\n",
        "\n",
        "        if not all(col in df.columns for col in required_features + [required_label]):\n",
        "            missing = [col for col in required_features + [required_label] if col not in df.columns]\n",
        "            print(f\"Error: Missing required columns for training: {missing}. Skipping training.\")\n",
        "            print(\"--- Exiting train_ml_model ---\\\\n\")\n",
        "            return None, None\n",
        "\n",
        "        # Ensure feature columns are numeric, coerce errors\n",
        "        for col in required_features:\n",
        "             df[col] = pd.to_numeric(df[col], errors='coerce')\n",
        "\n",
        "        # Drop rows with NaN in required features or label\n",
        "        df = df.dropna(subset=required_features + [required_label])\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"Error: No valid data remaining after dropping NaNs for training. Skipping training.\")\n",
        "            print(\"--- Exiting train_ml_model ---\\\\n\")\n",
        "            return None, None\n",
        "\n",
        "        print(f\"Using {len(df)} valid rows for training.\")\n",
        "\n",
        "\n",
        "        # Define features (X) and label (y)\n",
        "        X = df[required_features]\n",
        "        y = df[required_label].astype(int) # Ensure label is integer\n",
        "\n",
        "\n",
        "        # Check if there are at least two classes in the label (for classification)\n",
        "        if y.nunique() < 2:\n",
        "            print(f\"Error: Training data contains only one class ({y.iloc[0]}) in the '{required_label}' column. Need at least two classes for classification. Skipping training.\")\n",
        "            print(\"--- Exiting train_ml_model ---\\\\n\")\n",
        "            return None, None\n",
        "\n",
        "        # Check if there are enough samples for train/test split\n",
        "        if len(df) < 2: # Need at least 2 samples for split\n",
        "             print(\"Error: Not enough data points for train/test split (need at least 2). Skipping training.\")\n",
        "             print(\"--- Exiting train_ml_model ---\\\\n\")\n",
        "             return None, None\n",
        "\n",
        "        # Perform train/test split\n",
        "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y) # Use stratify if classes are imbalanced\n",
        "\n",
        "        print(f\"Training model on {len(X_train)} samples, testing on {len(X_test)} samples.\")\n",
        "\n",
        "        # Initialize and train the model\n",
        "        model = RandomForestClassifier(random_state=42)\n",
        "        model.fit(X_train, y_train)\n",
        "\n",
        "        # Evaluate the model\n",
        "        y_pred = model.predict(X_test)\n",
        "        report = classification_report(y_test, y_pred, output_dict=True, zero_division=0) # Use zero_division=0 to handle cases with no predicted samples for a class\n",
        "\n",
        "        print(\"ML model training successful.\")\n",
        "        print(\"--- Exiting train_ml_model ---\\\\n\")\n",
        "        return model, report\n",
        "\n",
        "    except ValueError as ve:\n",
        "        print(f\"ValueError during ML training: {ve}. Skipping training.\")\n",
        "        print(\"--- Exiting train_ml_model ---\\\\n\")\n",
        "        return None, None\n",
        "    except Exception as e:\n",
        "        print(f\"Error during ML training: {e}. Skipping training.\")\n",
        "        print(\"--- Exiting train_ml_model ---\\\\n\")\n",
        "        return None, None\n",
        "\n",
        "# Example Usage (can be removed or commented out)\n",
        "# if __name__ == '__main__':\n",
        "#     # Assuming data/signal_backtest.csv exists with a 'success' column\n",
        "#     DATA_DIR = 'signal_bot/data'\n",
        "#     training_data_path = os.path.join(DATA_DIR, 'signal_backtest.csv')\n",
        "#     if os.path.exists(training_data_path):\n",
        "#         trained_model, training_report = train_ml_model(training_data_path)\n",
        "#         if trained_model:\n",
        "#             print(\"\\\\nTraining Report Summary:\")\n",
        "#             # Print classification report in a readable format\n",
        "#             for label, metrics in training_report.items():\n",
        "#                  if isinstance(metrics, dict):\n",
        "#                       print(f\"  {label}:\")\n",
        "#                       for metric, value in metrics.items():\n",
        "#                            print(f\"    {metric}: {value:.4f}\")\n",
        "#                  else:\n",
        "#                       print(f\"  {label}: {metrics:.4f}\")\n",
        "#     else:\n",
        "#          print(f\"Training data not found: {training_data_path}\")\n",
        "\n",
        "\"\"\"\n",
        "with open('signal_bot/ml_model_trainer.py', 'w') as f:\n",
        "    f.write(ml_model_trainer_content)\n",
        "print(\"signal_bot/ml_model_trainer.py regenerated.\")\n",
        "\n",
        "# Regenerate signal_bot/exporter.py\n",
        "exporter_content = \"\"\"# exporter.py\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def export_to_excel(csv_path, output_path):\n",
        "    \"\"\"Exports data from a CSV to an Excel file.\"\"\"\n",
        "    print(f\"Attempting to export {csv_path} to Excel at {output_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True) # Ensure output directory exists\n",
        "        df.to_excel(output_path, index=False)\n",
        "        print(f\"Successfully exported {csv_path} to Excel at {output_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input CSV file not found at {csv_path} for Excel export.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error exporting to Excel from {csv_path}: {e}\")\n",
        "\n",
        "\n",
        "def export_to_html(csv_path, output_path):\n",
        "    \"\"\"Exports data from a CSV to an HTML file.\"\"\"\n",
        "    print(f\"Attempting to export {csv_path} to HTML at {output_path}\")\n",
        "    try:\n",
        "        df = pd.read_csv(csv_path)\n",
        "        os.makedirs(os.path.dirname(output_path) if os.path.dirname(output_path) else '.', exist_ok=True) # Ensure output directory exists\n",
        "        df.to_html(output_path, index=False)\n",
        "        print(f\"Successfully exported {csv_path} to HTML at {output_path}\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Input CSV file not found at {csv_path} for HTML export.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error exporting to HTML from {csv_path}: {e}\")\n",
        "\n",
        "# Example Usage (can be removed or commented out)\n",
        "# if __name__ == '__main__':\n",
        "#     # Assuming data/signal_backtest.csv exists after running main.py\n",
        "#     DATA_DIR = 'signal_bot/data'\n",
        "#     backtest_csv = os.path.join(DATA_DIR, 'signal_backtest.csv')\n",
        "#     excel_output = os.path.join(DATA_DIR, 'signal_backtest.xlsx')\n",
        "#     html_output = os.path.join(DATA_DIR, 'signal_backtest.html')\n",
        "#\n",
        "#     if os.path.exists(backtest_csv):\n",
        "#          export_to_excel(backtest_csv, excel_output)\n",
        "#          export_to_html(backtest_csv, html_output)\n",
        "#     else:\n",
        "#          print(f\"Input file not found: {backtest_csv}\")\n",
        "\n",
        "\"\"\"\n",
        "with open('signal_bot/exporter.py', 'w') as f:\n",
        "    f.write(exporter_content)\n",
        "print(\"signal_bot/exporter.py regenerated.\")\n",
        "\n",
        "\n",
        "# Regenerate signal_bot/logger.py\n",
        "logger_content = \"\"\"# logger.py\n",
        "import logging\n",
        "import os\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def setup_logger(log_file=\"bot.log\"):\n",
        "    \"\"\"Sets up the root logger.\"\"\"\n",
        "    log_path = os.path.join(DATA_DIR, log_file)\n",
        "    # Use basicConfig for simplicity, force=True allows reconfiguring in Colab\n",
        "    logging.basicConfig(\n",
        "        filename=log_path,\n",
        "        level=logging.INFO,\n",
        "        format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "        force=True\n",
        "    )\n",
        "    # Add a handler to print logs to the console as well\n",
        "    console_handler = logging.StreamHandler()\n",
        "    console_handler.setLevel(logging.INFO)\n",
        "    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "    console_handler.setFormatter(formatter)\n",
        "\n",
        "    # Avoid adding duplicate console handlers if basicConfig is called multiple times\n",
        "    # Check if a StreamHandler is already attached to the root logger\n",
        "    root_logger = logging.getLogger()\n",
        "    if not any(isinstance(h, logging.StreamHandler) for h in root_logger.handlers):\n",
        "         root_logger.addHandler(console_handler)\n",
        "\n",
        "\n",
        "def log_info(message):\n",
        "    \"\"\"Logs an informational message.\"\"\"\n",
        "    logging.info(message)\n",
        "\n",
        "def log_error(message):\n",
        "    \"\"\"Logs an error message.\"\"\"\n",
        "    logging.error(message)\n",
        "\n",
        "# Example usage (can be removed or commented out in final version)\n",
        "# if __name__ == '__main__':\n",
        "#     setup_logger()\n",
        "#     log_info(\"Logger setup complete.\")\n",
        "#     log_error(\"This is a test error message.\")\n",
        "\"\"\"\n",
        "with open('signal_bot/logger.py', 'w') as f:\n",
        "    f.write(logger_content)\n",
        "print(\"signal_bot/logger.py regenerated.\")\n",
        "\n",
        "# Regenerate signal_bot/dataset_manager.py\n",
        "dataset_manager_content = \"\"\"# dataset_manager.py\n",
        "import pandas as pd\n",
        "\n",
        "def clean_and_normalize(df):\n",
        "    \"\"\"Clean and normalize DataFrame columns and data types.\"\"\"\n",
        "    if df.empty:\n",
        "        print(\"Warning: Input DataFrame is empty for cleaning and normalization.\")\n",
        "        return df\n",
        "\n",
        "    # Convert column names to lowercase and replace spaces with underscores\n",
        "    df.columns = [col.lower().replace(\" \", \"_\") for col in df.columns]\n",
        "\n",
        "    # Drop rows where essential columns are missing\n",
        "    required_subset_cols = [\"id\", \"current_price\"]\n",
        "    if not all(col in df.columns for col in required_subset_cols):\n",
        "         print(f\"Warning: Required columns for dropna subset missing: {required_subset_cols}. Skipping dropna subset.\")\n",
        "    else:\n",
        "         df = df.dropna(subset=required_subset_cols)\n",
        "\n",
        "    # Convert timestamp column to datetime, coerce errors to NaT\n",
        "    if \"timestamp\" in df.columns:\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], errors='coerce')\n",
        "        # Optionally drop rows where timestamp conversion failed\n",
        "        # df = df.dropna(subset=[\"timestamp\"])\n",
        "    else:\n",
        "         print(\"Warning: 'timestamp' column not found for datetime conversion.\")\n",
        "\n",
        "\n",
        "    # Sort by id and timestamp\n",
        "    if \"id\" in df.columns and \"timestamp\" in df.columns:\n",
        "        df = df.sort_values(by=[\"id\", \"timestamp\"])\n",
        "    elif \"id\" in df.columns:\n",
        "        df = df.sort_values(by=\"id\")\n",
        "    elif \"timestamp\" in df.columns:\n",
        "        df = df.sort_values(by=\"timestamp\")\n",
        "    else:\n",
        "        print(\"Warning: 'id' and 'timestamp' columns not found for sorting.\")\n",
        "\n",
        "\n",
        "    return df\n",
        "\"\"\"\n",
        "with open('signal_bot/dataset_manager.py', 'w') as f:\n",
        "    f.write(dataset_manager_content)\n",
        "print(\"signal_bot/dataset_manager.py regenerated.\")\n",
        "\n",
        "\n",
        "# Regenerate signal_bot/data_collector.py\n",
        "data_collector_content = \"\"\"# signal_bot/data_collector.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins\n",
        "from .dataset_manager import clean_and_normalize # Import clean_and_normalize\n",
        "from .logger import setup_logger, log_info, log_error # Import logger\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def collect_data(limit=250):\n",
        "    \"\"\"\n",
        "    Fetches a snapshot of market data for a specified number of top coins\n",
        "    and saves it to full_market_snapshot.csv.\n",
        "\n",
        "    Args:\n",
        "        limit (int): The number of top coins to fetch data for. Note CoinGecko\n",
        "                     /coins/markets has a per_page limit (typically 250).\n",
        "                     Fetching more than the limit might require pagination.\n",
        "                     Current implementation fetches up to 'limit' on page 1.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame or None: DataFrame with market data if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    # setup_logger() # Setup logger for this function if not setup globally\n",
        "\n",
        "    log_info(f\"Fetching market data for top {limit} coins for full snapshot...\")\n",
        "    try:\n",
        "        data = get_top_coins(limit=limit)\n",
        "        if data is not None:\n",
        "            df = pd.DataFrame(data)\n",
        "\n",
        "            # Add current timestamp to each row\n",
        "            df[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "\n",
        "            # Clean and normalize the collected data\n",
        "            df = clean_and_normalize(df)\n",
        "            log_info(\"Data collected and cleaned.\")\n",
        "\n",
        "            # Ensure output directory exists (already done by DATA_DIR os.makedirs)\n",
        "            full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "\n",
        "            # Append data to existing file or create new\n",
        "            if os.path.exists(full_snapshot_path):\n",
        "                 df.to_csv(full_snapshot_path, mode='a', header=False, index=False)\n",
        "                 log_info(f\"Market snapshot appended to {full_snapshot_path}.\")\n",
        "            else:\n",
        "                 df.to_csv(full_snapshot_path, index=False)\n",
        "                 log_info(f\"New market snapshot created at {full_snapshot_path}.\")\n",
        "\n",
        "            return df\n",
        "        else:\n",
        "            log_error(\"Failed to fetch market data for full snapshot.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error during full market data collection: {e}\")\n",
        "        return None\n",
        "\n",
        "# Example of how to run the data collector as a script\n",
        "if __name__ == '__main__':\n",
        "    # setup_logger() # Setup logger if running this script directly\n",
        "    collect_data(limit=250) # Fetch data for top 250 coins for the snapshot\n",
        "\"\"\"\n",
        "with open('signal_bot/data_collector.py', 'w') as f:\n",
        "    f.write(data_collector_content)\n",
        "print(\"signal_bot/data_collector.py regenerated.\")\n",
        "\n",
        "\n",
        "# Regenerate signal_bot/scheduler.py\n",
        "scheduler_content = \"\"\"# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    # In a full implementation, data collection for top 10 would be here as well\n",
        "    # For now, we'll fetch top 10 data within the pipeline job\n",
        "    log_info(\"Fetching top 10 coin data within pipeline job...\")\n",
        "    try:\n",
        "        top10_data = collect_data(limit=10) # Reuse collect_data for top 10, assuming limit works\n",
        "        if top10_data is not None:\n",
        "             top10_data.to_csv(top10_input_path, index=False) # Save as top10_market_data.csv\n",
        "             log_info(\"Top 10 data fetched and saved within pipeline job.\")\n",
        "        else:\n",
        "             log_error(\"Failed to fetch top 10 data within pipeline job. Skipping.\")\n",
        "             return # Exit job if data fetching fails\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error fetching top 10 data within pipeline job: {e}. Skipping.\")\n",
        "        return # Exit job on error\n",
        "\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found after collection. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        # Read from the saved CSV to ensure consistency, although top10_data DF is available\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generating signals for top 10...\")\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    # Ensure the full market snapshot is available - it's generated by full_data_collection_job\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found for anomaly detection. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        # collect_data will fetch the full snapshot (limit=250) and append to full_market_snapshot.csv\n",
        "        collected_df = collect_data(limit=250) # Assuming collect_data handles saving/appending\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "# scheduler.start() # Comment out to prevent immediate blocking execution\n",
        "\n",
        "# Manual test execution of individual jobs for debugging in Colab:\n",
        "# setup_logger()\n",
        "# print(\"\\\\n--- Manual Pipeline Job Test ---\")\n",
        "# pipeline_job()\n",
        "# print(\"\\\\n--- Manual Full Data Collection Job Test ---\")\n",
        "# full_data_collection_job()\n",
        "# print(\"\\\\n--- Manual Anomaly Job Test ---\")\n",
        "# anomaly_job()\n",
        "''')"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "unterminated triple-quoted string literal (detected at line 1018) (ipython-input-3726481119.py, line 895)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-3726481119.py\"\u001b[0;36m, line \u001b[0;32m895\u001b[0m\n\u001b[0;31m    scheduler_content = \"\"\"# scheduler.py\u001b[0m\n\u001b[0m                        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m unterminated triple-quoted string literal (detected at line 1018)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e41a98ed",
        "outputId": "8a2eca6b-5f13-4efe-f2b0-44592fe0b371"
      },
      "source": [
        "!git clone github.com/9to5ninja-projects/GeckoBot.git"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: repository 'github.com/9to5ninja-projects/GeckoBot.git' does not exist\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NyowmmZ7VFWd",
        "outputId": "bb7a2f88-9a3d-43e3-e149-7efe49e5c6b1"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f0f52a4f",
        "outputId": "327e13b6-fd09-4b8b-ae52-cff423bfabeb"
      },
      "source": [
        "# Ensure you are in the cloned repository directory\n",
        "# Check current directory - should be 'GeckoBot'\n",
        "%pwd\n",
        "\n",
        "# Push the changes to your GitHub repository again\n",
        "# You WILL LIKELY be prompted for your GitHub username and password/Personal Access Token.\n",
        "# Enter your GitHub username when prompted for \"Username for 'https://github.com':\"\n",
        "# Enter your Personal Access Token when prompted for \"Password for 'https://YOUR_GITHUB_USERNAME@github.com':\"\n",
        "!git push origin main # Or 'master', depending on your default branch name\n",
        "\n",
        "print(\"Attempted to push changes again.\")\n",
        "print(\"Please check the output above for any prompts for credentials or error messages.\")\n",
        "print(\"After successful authentication and push, check your GitHub repository on the web.\")"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "Attempted to push changes again.\n",
            "Please check the output above for any prompts for credentials or error messages.\n",
            "After successful authentication and push, check your GitHub repository on the web.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a74048a",
        "outputId": "0327dbd7-7188-42eb-8f17-10460b86429f"
      },
      "source": [
        "# Push the changes to your GitHub repository\n",
        "# You may be prompted for your GitHub username and password/Personal Access Token\n",
        "!git push origin main # Or 'master', depending on your default branch name\n",
        "\n",
        "print(\"Changes pushed to GitHub.\")"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: could not read Username for 'https://github.com': No such device or address\n",
            "Changes pushed to GitHub.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "edff7715",
        "outputId": "fd7029be-5dbc-4dd2-a302-eaa9e2a84433"
      },
      "source": [
        "# Commit the changes\n",
        "# Replace \"Initial commit\" or provide a meaningful commit message\n",
        "!git commit -m \"Add/Update project files from Colab\"\n",
        "\n",
        "print(\"Changes committed.\")"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[main 118433e] Add/Update project files from Colab\n",
            " 19 files changed, 883 insertions(+)\n",
            " create mode 100644 signal_bot/__pycache__/anomaly_detector.cpython-312.pyc\n",
            " create mode 100644 signal_bot/__pycache__/data_collector.cpython-312.pyc\n",
            " create mode 100644 signal_bot/__pycache__/logger.cpython-312.pyc\n",
            " create mode 100644 signal_bot/__pycache__/ml_logger.cpython-312.pyc\n",
            " create mode 100644 signal_bot/__pycache__/scheduler.cpython-312.pyc\n",
            " create mode 100644 signal_bot/anomaly_detector.py\n",
            " create mode 100644 signal_bot/dashboard.py\n",
            " create mode 100644 signal_bot/data_collector.py\n",
            " create mode 100644 signal_bot/indicators/__pycache__/ta_utils.cpython-312.pyc\n",
            " create mode 100644 signal_bot/indicators/ta_utils.py\n",
            " create mode 100644 signal_bot/logger.py\n",
            " create mode 100644 signal_bot/main.py\n",
            " create mode 100644 signal_bot/ml_logger.py\n",
            " create mode 100644 signal_bot/requirements.txt\n",
            " create mode 100644 signal_bot/scheduler.py\n",
            " create mode 100644 signal_bot/signals/__pycache__/signal_finder.cpython-312.pyc\n",
            " create mode 100644 signal_bot/signals/signal_finder.py\n",
            " create mode 100644 signal_bot/utils/__pycache__/coingecko_api.cpython-312.pyc\n",
            " create mode 100644 signal_bot/utils/coingecko_api.py\n",
            "Changes committed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b1a7b161",
        "outputId": "26724a6e-348d-44fe-e10e-565e769c4ca0"
      },
      "source": [
        "# Add all changes to the staging area\n",
        "!git add .\n",
        "\n",
        "print(\"All changes added to staging.\")"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "All changes added to staging.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f893b355",
        "outputId": "1fc317e2-1f63-44a1-e9da-6efbf423cec4"
      },
      "source": [
        "# Navigate into the cloned repository directory\n",
        "%cd GeckoBot\n",
        "\n",
        "print(\"Changed directory to GeckoBot.\")"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/GeckoBot\n",
            "Changed directory to GeckoBot.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "847c5744",
        "outputId": "c87f3534-2279-40f4-c91f-565c05fafea8"
      },
      "source": [
        "# Copy your project files into the cloned repository directory\n",
        "# Ensure signal_bot directory exists in the current environment\n",
        "import os\n",
        "import shutil\n",
        "\n",
        "source_dir = 'signal_bot'\n",
        "dest_dir = 'GeckoBot/signal_bot' # Copy into the cloned repo\n",
        "\n",
        "# Ensure source directory exists\n",
        "if not os.path.exists(source_dir):\n",
        "    print(f\"Error: Source directory not found: {source_dir}. Cannot copy files.\")\n",
        "else:\n",
        "    # Remove destination directory if it exists to avoid merging issues\n",
        "    if os.path.exists(dest_dir):\n",
        "        print(f\"Removing existing destination directory: {dest_dir}\")\n",
        "        shutil.rmtree(dest_dir)\n",
        "\n",
        "    # Copy the signal_bot directory\n",
        "    shutil.copytree(source_dir, dest_dir)\n",
        "    print(f\"Copied {source_dir} to {dest_dir}.\")\n",
        "\n",
        "    # Also copy .env if it exists at the root\n",
        "    env_file = '.env'\n",
        "    if os.path.exists(env_file):\n",
        "        shutil.copy(env_file, 'GeckoBot/.env')\n",
        "        print(f\"Copied {env_file} to GeckoBot/.\")\n",
        "    else:\n",
        "        print(\".env file not found at the root. Skipping copy.\")"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Copied signal_bot to GeckoBot/signal_bot.\n",
            ".env file not found at the root. Skipping copy.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "748b7481",
        "outputId": "2b567eb1-14e7-4dde-c64f-e05733c84488"
      },
      "source": [
        "# Clone your GitHub repository\n",
        "# Replace YOUR_GITHUB_USERNAME with '9to5ninja-projects' and YOUR_REPO_NAME with 'GeckoBot'\n",
        "!git clone https://github.com/9to5ninja-projects/GeckoBot.git\n",
        "\n",
        "print(\"Repository cloned.\")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'GeckoBot'...\n",
            "remote: Enumerating objects: 3, done.\u001b[K\n",
            "remote: Counting objects: 100% (3/3), done.\u001b[K\n",
            "remote: Total 3 (delta 0), reused 0 (delta 0), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (3/3), done.\n",
            "Repository cloned.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "60a8d752",
        "outputId": "b785426e-e265-418a-80d1-3edca480d183"
      },
      "source": [
        "# Configure Git (replace with your name and email)\n",
        "!git config --global user.name \"YOUR NAME\"\n",
        "!git config --global user.email \"YOUR EMAIL\"\n",
        "\n",
        "print(\"Git configured.\")"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Git configured.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e4acdf61",
        "outputId": "50e9cc75-c243-48ec-a38a-c1976ca5c4b5"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the anomaly_detector.py file\n",
        "with open('signal_bot/anomaly_detector.py', 'w') as f:\n",
        "    f.write('''# anomaly_detector.py\n",
        "import pandas as pd\n",
        "from sklearn.ensemble import IsolationForest\n",
        "from datetime import datetime\n",
        "import os\n",
        "from .logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "def load_market_snapshot(snapshot_csv):\n",
        "    \"\"\"Load full market snapshot from CSV.\"\"\"\n",
        "    setup_logger()\n",
        "    log_info(f\"Loading market snapshot from {snapshot_csv} for anomaly detection.\")\n",
        "    try:\n",
        "        df = pd.read_csv(snapshot_csv, parse_dates=[\"timestamp\"])\n",
        "        log_info(\"Market snapshot loaded successfully.\")\n",
        "        return df\n",
        "    except FileNotFoundError:\n",
        "        log_error(f\"Error: Snapshot file not found at {snapshot_csv}.\")\n",
        "        return pd.DataFrame()\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error loading market snapshot from {snapshot_csv}: {e}\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "\n",
        "def prepare_features(df):\n",
        "    \"\"\"Create features for anomaly detection (e.g., price % change).\"\"\"\n",
        "    setup_logger()\n",
        "    log_info(\"Preparing features for anomaly detection.\")\n",
        "    if df.empty:\n",
        "        log_info(\"Input DataFrame is empty, cannot prepare features.\")\n",
        "        return pd.DataFrame()\n",
        "    # Ensure required columns exist before accessing them\n",
        "    required_cols = [\"price_change_percentage_24h\", \"id\", \"symbol\", \"name\", \"timestamp\"]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "         missing = [col for col in required_cols if col not in df.columns]\n",
        "         log_error(f\"Missing required columns for feature preparation: {missing}. Cannot prepare features.\")\n",
        "         return pd.DataFrame()\n",
        "\n",
        "    df[\"pct_change_24h\"] = df[\"price_change_percentage_24h\"].fillna(0)\n",
        "    log_info(\"Features prepared.\")\n",
        "    return df[[\"id\", \"symbol\", \"name\", \"pct_change_24h\", \"timestamp\"]].copy()\n",
        "\n",
        "\n",
        "def run_isolation_forest(df, contamination=0.01):\n",
        "    \"\"\"Apply Isolation Forest to detect anomalies based on % change.\"\"\"\n",
        "    setup_logger()\n",
        "    log_info(\"Running Isolation Forest for anomaly detection.\")\n",
        "    if df.empty or \"pct_change_24h\" not in df.columns:\n",
        "        log_info(\"Input DataFrame is empty or missing 'pct_change_24h', skipping Isolation Forest.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    try:\n",
        "        # Ensure there's enough data points for IsolationForest (at least 2)\n",
        "        if len(df) < 2:\n",
        "             log_info(f\"Not enough data points ({len(df)}) for Isolation Forest (requires at least 2). Skipping.\")\n",
        "             df[\"anomaly_score\"] = 1 # Mark as not an anomaly if insufficient data\n",
        "             return df\n",
        "\n",
        "        # Fit and predict anomalies\n",
        "        model = IsolationForest(contamination=contamination, random_state=42)\n",
        "        df[\"anomaly_score\"] = model.fit_predict(df[[\"pct_change_24h\"]])\n",
        "        log_info(\"Isolation Forest completed.\")\n",
        "        return df\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error running Isolation Forest: {e}\")\n",
        "        df[\"anomaly_score\"] = 1 # Default to not an anomaly on error\n",
        "        return df\n",
        "\n",
        "\n",
        "def save_anomalies(df, output_dir=\"data\"):\n",
        "    \"\"\"Save anomalies to timestamped CSV.\"\"\"\n",
        "    setup_logger()\n",
        "    if df.empty or \"anomaly_score\" not in df.columns:\n",
        "        log_info(\"Input DataFrame is empty or missing 'anomaly_score', skipping saving anomalies.\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    anomalies = df[df[\"anomaly_score\"] == -1]\n",
        "    if anomalies.empty:\n",
        "        log_info(\"No anomalies detected to save.\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    timestamp = datetime.utcnow().strftime(\"%Y%m%dT%H%M%SZ\")\n",
        "    path = os.path.join(output_dir, f\"anomalies_{timestamp}.csv\")\n",
        "    try:\n",
        "        os.makedirs(output_dir, exist_ok=True)\n",
        "        anomalies.to_csv(path, index=False)\n",
        "        log_info(f\"Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "        return path, anomalies\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error saving anomalies to {path}: {e}\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "\n",
        "def detect_anomalies(snapshot_csv, output_dir=\"data\"):\n",
        "    \"\"\"End-to-end anomaly detection workflow.\"\"\"\n",
        "    setup_logger()\n",
        "    log_info(\"Starting anomaly detection workflow.\")\n",
        "    df = load_market_snapshot(snapshot_csv)\n",
        "    if df.empty:\n",
        "        log_info(\"Anomaly detection workflow skipped due to empty data.\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    features = prepare_features(df)\n",
        "    if features.empty:\n",
        "        log_info(\"Anomaly detection workflow skipped due to empty features.\")\n",
        "        return None, pd.DataFrame()\n",
        "\n",
        "    with_scores = run_isolation_forest(features)\n",
        "    if with_scores.empty:\n",
        "         log_info(\"Anomaly detection workflow skipped after Isolation Forest.\")\n",
        "         return None, pd.DataFrame()\n",
        "\n",
        "    path, anomalies = save_anomalies(with_scores, output_dir)\n",
        "    log_info(\"Anomaly detection workflow finished.\")\n",
        "    return path, anomalies\n",
        "''')\n",
        "\n",
        "# Now that anomaly_detector.py exists, try running the scheduler again.\n",
        "# Ensure scheduler.start() is uncommented in scheduler.py.\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently\n",
        "    # For now, we rely on the full_data_collection_job which runs hourly.\n",
        "    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.\n",
        "    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        # compute_indicators expects input_csv and output_csv paths\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        # find_signals expects a DataFrame\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        # log_ml_features expects indicator_csv and output_csv paths\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        # detect_anomalies expects snapshot_csv and output_dir\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        # collect_data returns a DataFrame\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "scheduler.start() # Uncommented to start the scheduler\n",
        "''')\n",
        "\n",
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting scheduler...\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 103, in <module>\n",
            "    scheduler.start() # Uncommented to start the scheduler\n",
            "    ^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/apscheduler/schedulers/blocking.py\", line 19, in start\n",
            "    self._main_loop()\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/apscheduler/schedulers/blocking.py\", line 28, in _main_loop\n",
            "    self._event.wait(wait_seconds)\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 655, in wait\n",
            "  File \"/usr/lib/python3.12/threading.py\", line 359, in wait\n",
            "    gotit = waiter.acquire(True, timeout)\n",
            "            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "KeyboardInterrupt\n",
            "^C\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08dd05ee",
        "outputId": "20050359-eb9a-40df-c37c-a6c39357bd66"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the ml_logger.py file\n",
        "with open('signal_bot/ml_logger.py', 'w') as f:\n",
        "    f.write('''# ml_logger.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "from .logger import setup_logger, log_info, log_error # Import logger\n",
        "\n",
        "\n",
        "def log_ml_features(indicator_csv, output_csv):\n",
        "    \"\"\"Append new features to ML training dataset.\"\"\"\n",
        "    setup_logger() # Ensure logger is setup\n",
        "    log_info(f\"Attempting to log ML features from {indicator_csv} to {output_csv}...\")\n",
        "\n",
        "    try:\n",
        "        if not os.path.exists(indicator_csv):\n",
        "            log_info(f\"Warning: Input file for ML logging not found at {indicator_csv}. Skipping logging.\")\n",
        "            return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "        df = pd.read_csv(indicator_csv)\n",
        "        log_info(f\"Successfully read {indicator_csv} for ML logging. Columns: {df.columns.tolist()}\")\n",
        "\n",
        "        required_features = [\n",
        "            \"id\", \"timestamp\", \"current_price\",\n",
        "            \"rsi\", \"ema_20\", \"macd_diff\",\n",
        "            \"bb_upper\", \"bb_lower\", \"signal\" # Added signal as a required feature\n",
        "        ]\n",
        "\n",
        "        # Check if all required features are in the DataFrame\n",
        "        missing_features = [col for col in required_features if col not in df.columns]\n",
        "        if missing_features:\n",
        "            log_info(f\"Warning: Missing required features for ML logging in {indicator_csv}: {missing_features}. Skipping logging for this file.\")\n",
        "            return pd.DataFrame() # Return empty DataFrame if features are missing\n",
        "\n",
        "        features = df[required_features].copy()\n",
        "\n",
        "        # Add a placeholder 'success' column for ML training.\n",
        "        # In a real scenario, this would be populated by backtesting or manual labeling.\n",
        "        if \"signal\" in features.columns and \"BUY\" in features[\"signal\"].str.upper().str:\n",
        "             # For simplicity, label any row with a BUY signal as needing potential backtest/labeling\n",
        "             # A more sophisticated approach would look at future price movement\n",
        "             features[\"success\"] = pd.NA # Mark as needing labeling\n",
        "        else:\n",
        "             features[\"success\"] = 0 # Assume not a successful trade if no BUY signal\n",
        "\n",
        "        if os.path.exists(output_csv):\n",
        "            existing = pd.read_csv(output_csv)\n",
        "            # Ensure timestamp is datetime for proper merging/deduplication\n",
        "            existing[\"timestamp\"] = pd.to_datetime(existing[\"timestamp\"])\n",
        "            features[\"timestamp\"] = pd.to_datetime(features[\"timestamp\"])\n",
        "\n",
        "            # Combine existing and new data, drop duplicates based on id and timestamp\n",
        "            combined = pd.concat([existing, features]).drop_duplicates(subset=[\"id\", \"timestamp\"]).reset_index(drop=True)\n",
        "        else:\n",
        "            combined = features\n",
        "\n",
        "        combined.to_csv(output_csv, index=False)\n",
        "        log_info(f\"ML features logged successfully to {output_csv}. Total entries: {len(combined)}\")\n",
        "        return combined\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error during ML feature logging for {indicator_csv}: {e}. Skipping logging.\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on error\n",
        "\n",
        "''')\n",
        "\n",
        "# Now that ml_logger.py exists, try running the scheduler again.\n",
        "# Ensure scheduler.start() is uncommented in scheduler.py.\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently\n",
        "    # For now, we rely on the full_data_collection_job which runs hourly.\n",
        "    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.\n",
        "    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        # compute_indicators expects input_csv and output_csv paths\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        # find_signals expects a DataFrame\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        # log_ml_features expects indicator_csv and output_csv paths\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        # detect_anomalies expects snapshot_csv and output_dir\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        # collect_data returns a DataFrame\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "scheduler.start() # Uncommented to start the scheduler\n",
        "''')\n",
        "\n",
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 7, in <module>\n",
            "    from signal_bot.anomaly_detector import detect_anomalies\n",
            "ModuleNotFoundError: No module named 'signal_bot.anomaly_detector'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7dc8950",
        "outputId": "864e1ec2-8b6d-42a1-be25-b3a70a46b7f2"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs('signal_bot/signals', exist_ok=True)\n",
        "\n",
        "# Write the content to the signal_finder.py file\n",
        "with open('signal_bot/signals/signal_finder.py', 'w') as f:\n",
        "    f.write('''# signal_bot/signals/signal_finder.py\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from ..logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "def find_signals(df):\n",
        "    \"\"\"\n",
        "    Detects trading signals based on technical indicators.\n",
        "\n",
        "    Args:\n",
        "        df (pd.DataFrame): DataFrame with technical indicators (rsi, macd_diff, close, bb_upper, bb_lower).\n",
        "                           Assumes DataFrame is sorted by timestamp.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with 'signal' column added. Returns DataFrame with HOLD signals on error or insufficient data.\n",
        "    \"\"\"\n",
        "    setup_logger() # Ensure logger is setup\n",
        "    log_info(\"Attempting to find signals...\")\n",
        "\n",
        "    # Ensure DataFrame has required columns.\n",
        "    required_cols = [\"rsi\", \"ema_20\", \"macd_diff\", \"close\", \"bb_upper\", \"bb_lower\"]\n",
        "    if not all(col in df.columns for col in required_cols):\n",
        "        missing = [col for col in required_cols if col not in df.columns]\n",
        "        log_error(f\"Missing required columns for signal generation: {missing}. Returning HOLD signals.\")\n",
        "        df[\"signal\"] = \"HOLD\"\n",
        "        return df[['signal']]\n",
        "\n",
        "    if df.empty:\n",
        "        log_info(\"Input DataFrame is empty. Returning HOLD signals.\")\n",
        "        df[\"signal\"] = \"HOLD\"\n",
        "        return df[['signal']]\n",
        "\n",
        "    # Initialize signal column\n",
        "    df[\"signal\"] = \"HOLD\"\n",
        "\n",
        "    # Find the first index where all required indicators are not NaN\n",
        "    df_cleaned = df.dropna(subset=required_cols)\n",
        "\n",
        "    if df_cleaned.empty:\n",
        "        log_info(\"All required indicator values are NaN. Returning HOLD signals.\")\n",
        "        # Set all signals to HOLD if no valid indicator rows are found\n",
        "        df[\"signal\"] = \"HOLD\"\n",
        "        return df[['signal']]\n",
        "\n",
        "    first_valid_index = df_cleaned.index.min()\n",
        "    first_valid_iloc = df.index.get_loc(first_valid_index)\n",
        "\n",
        "    # Apply signal logic based on technical indicators, starting from the first valid index\n",
        "    for i in range(first_valid_iloc, len(df)):\n",
        "        current_row = df.iloc[i]\n",
        "\n",
        "        # Ensure previous row exists and has valid indicator values\n",
        "        prev_row_iloc = i - 1\n",
        "        if prev_row_iloc < first_valid_iloc:\n",
        "             # Previous row does not have valid indicators\n",
        "             df.loc[current_row.name, 'signal'] = 'HOLD'\n",
        "             continue\n",
        "\n",
        "        prev_row = df.iloc[prev_row_iloc]\n",
        "        signals = []\n",
        "\n",
        "        # RSI signals (using current and previous crossing thresholds)\n",
        "        if current_row['rsi'] < 30 and prev_row['rsi'] >= 30:\n",
        "            signals.append(\"BUY_RSI_CROSS\")\n",
        "        elif current_row['rsi'] > 70 and prev_row['rsi'] <= 70:\n",
        "            signals.append(\"SELL_RSI_CROSS\")\n",
        "\n",
        "        # RSI extreme conditions (may add to existing signals)\n",
        "        if current_row['rsi'] < 20:\n",
        "             signals.append(\"EXTREME_OVERSOLD\")\n",
        "        elif current_row['rsi'] > 80:\n",
        "             signals.append(\"EXTREME_OVERBOUGHT\")\n",
        "\n",
        "\n",
        "        # MACD signals (using macd_diff crossing 0)\n",
        "        # Ensure previous macd_diff is not NaN for cross check\n",
        "        if not np.isnan(prev_row['macd_diff']):\n",
        "            if current_row['macd_diff'] > 0 and prev_row['macd_diff'] <= 0:\n",
        "                signals.append(\"BUY_MACD_CROSS\")\n",
        "            elif current_row['macd_diff'] < 0 and prev_row['macd_diff'] >= 0:\n",
        "                signals.append(\"SELL_MACD_CROSS\")\n",
        "\n",
        "        # Bollinger Bands signals (breakouts)\n",
        "        # Ensure previous BB values are not NaN for breakout check\n",
        "        if not np.isnan(prev_row['bb_upper']) and not np.isnan(prev_row['bb_lower']):\n",
        "            if current_row['close'] > current_row['bb_upper'] and prev_row['close'] <= prev_row['bb_upper']:\n",
        "                signals.append(\"BREAKOUT_UPPER\")\n",
        "            elif current_row['close'] < current_row['bb_lower'] and prev_row['close'] >= prev_row['bb_lower']:\n",
        "                signals.append(\"BREAKOUT_LOWER\")\n",
        "\n",
        "        # Update the signal for the current row\n",
        "        df.loc[current_row.name, 'signal'] = \", \".join(signals) if signals else \"HOLD\"\n",
        "\n",
        "\n",
        "    # For rows before the first valid indicator index, set signal to HOLD\n",
        "    if first_valid_iloc > 0:\n",
        "        df.loc[df.index[:first_valid_iloc], 'signal'] = 'HOLD'\n",
        "\n",
        "    log_info(\"Signal finding completed.\")\n",
        "    return df[['signal']] # Return only the signal column as a DataFrame\n",
        "\n",
        "''')\n",
        "\n",
        "# Now that signal_finder.py exists, try running the scheduler again.\n",
        "# Ensure scheduler.start() is uncommented in scheduler.py.\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently\n",
        "    # For now, we rely on the full_data_collection_job which runs hourly.\n",
        "    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.\n",
        "    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        # compute_indicators expects input_csv and output_csv paths\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        # find_signals expects a DataFrame\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        # log_ml_features expects indicator_csv and output_csv paths\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        # detect_anomalies expects snapshot_csv and output_dir\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        # collect_data returns a DataFrame\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "scheduler.start() # Uncommented to start the scheduler\n",
        "''')\n",
        "\n",
        "\n",
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 6, in <module>\n",
            "    from signal_bot.ml_logger import log_ml_features\n",
            "ModuleNotFoundError: No module named 'signal_bot.ml_logger'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7b8703b",
        "outputId": "654914e2-790a-436a-e418-8c7768172198"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs('signal_bot/indicators', exist_ok=True)\n",
        "\n",
        "# Write the content to the ta_utils.py file\n",
        "with open('signal_bot/indicators/ta_utils.py', 'w') as f:\n",
        "    f.write('''# indicators/ta_utils.py\n",
        "import pandas as pd\n",
        "import ta\n",
        "from ..logger import setup_logger, log_info, log_error # Import logger from parent directory\n",
        "\n",
        "\n",
        "def compute_indicators(df_or_path, output_csv=None):\n",
        "    \"\"\"\n",
        "    Computes technical indicators (RSI, EMA, MACD, Bollinger Bands) for price data.\n",
        "\n",
        "    Args:\n",
        "        df_or_path (pd.DataFrame or str): Input data as a DataFrame or path to a CSV file.\n",
        "        output_csv (str, optional): Path to save the DataFrame with indicators to CSV. Defaults to None.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame: DataFrame with technical indicators added. Returns empty DataFrame on error.\n",
        "    \"\"\"\n",
        "    setup_logger() # Ensure logger is setup\n",
        "\n",
        "    try:\n",
        "        if isinstance(df_or_path, pd.DataFrame):\n",
        "            df = df_or_path.copy()\n",
        "            log_info(\"Computing indicators on provided DataFrame.\")\n",
        "        elif isinstance(df_or_path, str) and os.path.exists(df_or_path):\n",
        "            df = pd.read_csv(df_or_path, parse_dates=[\"timestamp\"])\n",
        "            log_info(f\"Computing indicators on data loaded from {df_or_path}.\")\n",
        "        else:\n",
        "            log_error(f\"Invalid input: Expected a DataFrame or valid file path, got {type(df_or_path)}.\")\n",
        "            return pd.DataFrame()\n",
        "\n",
        "        # Ensure close price exists and data is sorted by timestamp\n",
        "        if \"close\" not in df.columns and \"current_price\" in df.columns:\n",
        "             df[\"close\"] = df[\"current_price\"]\n",
        "        elif \"close\" not in df.columns:\n",
        "             log_error(\"Input DataFrame must contain a 'close' or 'current_price' column.\")\n",
        "             return pd.DataFrame()\n",
        "\n",
        "        # Ensure timestamp is datetime and sort\n",
        "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"])\n",
        "        df = df.sort_values(\"timestamp\")\n",
        "\n",
        "        # Compute Indicators - Handle potential errors or insufficient data for windows\n",
        "        if len(df) > 14: # Minimum required for RSI default window\n",
        "            try:\n",
        "                df[\"rsi\"] = ta.momentum.RSIIndicator(df[\"close\"]).rsi()\n",
        "            except Exception as e:\n",
        "                 log_error(f\"Error computing RSI: {e}\")\n",
        "                 df[\"rsi\"] = pd.NA # Assign pandas NA on error or insufficient data\n",
        "\n",
        "            if len(df) > 20: # Minimum required for EMA and BB default window\n",
        "                 try:\n",
        "                     df[\"ema_20\"] = ta.trend.EMAIndicator(df[\"close\"], window=20).ema_indicator()\n",
        "                     bb = ta.volatility.BollingerBands(df[\"close\"], window=20, window_dev=2)\n",
        "                     df[\"bb_upper\"] = bb.bollinger_hband()\n",
        "                     df[\"bb_lower\"] = bb.bollinger_lband()\n",
        "                 except Exception as e:\n",
        "                      log_error(f\"Error computing EMA or Bollinger Bands: {e}\")\n",
        "                      df[\"ema_20\"] = pd.NA\n",
        "                      df[\"bb_upper\"] = pd.NA\n",
        "                      df[\"bb_lower\"] = pd.NA\n",
        "\n",
        "                 try:\n",
        "                    # MACD requires more data than the default window for meaningful values\n",
        "                    # ta.trend.MACD default windows are (12, 26, 9). MACD_diff needs at least 26 data points.\n",
        "                    if len(df) > 26:\n",
        "                         df[\"macd_diff\"] = ta.trend.MACD(df[\"close\"]).macd_diff()\n",
        "                    else:\n",
        "                         log_info(f\"Not enough data points ({len(df)}) for MACD calculation (requires > 26).\")\n",
        "                         df[\"macd_diff\"] = pd.NA\n",
        "                 except Exception as e:\n",
        "                      log_error(f\"Error computing MACD: {e}\")\n",
        "                      df[\"macd_diff\"] = pd.NA\n",
        "\n",
        "\n",
        "            else:\n",
        "                 log_info(f\"Not enough data points ({len(df)}) for EMA, MACD, or Bollinger Bands calculation (requires > 20).\")\n",
        "                 df[\"ema_20\"] = pd.NA\n",
        "                 df[\"bb_upper\"] = pd.NA\n",
        "                 df[\"bb_lower\"] = pd.NA\n",
        "                 df[\"macd_diff\"] = pd.NA # Ensure MACD is also NA\n",
        "\n",
        "        else:\n",
        "            log_info(f\"Not enough data points ({len(df)}) for any indicator calculation (requires > 14 for RSI).\")\n",
        "            # Assign NA to all indicator columns if not enough data for any\n",
        "            df[\"rsi\"] = pd.NA\n",
        "            df[\"ema_20\"] = pd.NA\n",
        "            df[\"bb_upper\"] = pd.NA\n",
        "            df[\"bb_lower\"] = pd.NA\n",
        "            df[\"macd_diff\"] = pd.NA\n",
        "\n",
        "        df[\"processed_timestamp\"] = pd.Timestamp.utcnow().isoformat()\n",
        "\n",
        "\n",
        "        if output_csv:\n",
        "            try:\n",
        "                os.makedirs(os.path.dirname(output_csv), exist_ok=True)\n",
        "                df.to_csv(output_csv, index=False)\n",
        "                log_info(f\"Indicators computed and saved to {output_csv}.\")\n",
        "            except Exception as e:\n",
        "                log_error(f\"Error saving indicators to {output_csv}: {e}\")\n",
        "\n",
        "\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"An unexpected error occurred during indicator computation: {e}\")\n",
        "        return pd.DataFrame() # Return empty DataFrame on unexpected errors\n",
        "\n",
        "''')\n",
        "\n",
        "# Now that ta_utils.py exists, try running the scheduler again.\n",
        "# Ensure scheduler.start() is uncommented in scheduler.py.\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    # The pipeline_job should start with fetching the top 10 data if it's not already scheduled separately more frequently\n",
        "    # For now, we rely on the full_data_collection_job which runs hourly.\n",
        "    # A dedicated top 10 fetch job running more frequently would be better for this pipeline.\n",
        "    # Assuming top10_market_data.csv is generated by some means (e.g., another job or manual run)\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        # compute_indicators expects input_csv and output_csv paths\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        # find_signals expects a DataFrame\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        # log_ml_features expects indicator_csv and output_csv paths\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        # detect_anomalies expects snapshot_csv and output_dir\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        # collect_data returns a DataFrame\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "scheduler.start() # Uncommented to start the scheduler\n",
        "''')\n",
        "\n",
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 5, in <module>\n",
            "    from signal_bot.signals.signal_finder import find_signals\n",
            "ModuleNotFoundError: No module named 'signal_bot.signals'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b923e091",
        "outputId": "8c30c85a-c0d5-4451-8654-67a6281f9e84"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the logger.py file\n",
        "with open('signal_bot/logger.py', 'w') as f:\n",
        "    f.write('''# signal_bot/logger.py\n",
        "import logging\n",
        "import sys\n",
        "\n",
        "def setup_logger():\n",
        "    \"\"\"Sets up a basic logger to output to standard output.\"\"\"\n",
        "    # Ensure logger is not already configured\n",
        "    if not logging.getLogger().handlers:\n",
        "        logging.basicConfig(level=logging.INFO,\n",
        "                            format='%(asctime)s - %(levelname)s - %(message)s',\n",
        "                            stream=sys.stdout)\n",
        "\n",
        "def log_info(message):\n",
        "    \"\"\"Logs an informational message.\"\"\"\n",
        "    logging.info(message)\n",
        "\n",
        "def log_error(message):\n",
        "    \"\"\"Logs an error message.\"\"\"\n",
        "    logging.error(message)\n",
        "''')\n",
        "\n",
        "# Now that logger.py exists, try running the scheduler again.\n",
        "# Ensure scheduler.start() is uncommented in scheduler.py.\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        # compute_indicators expects input_csv and output_csv paths\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        # find_signals expects a DataFrame\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        # log_ml_features expects indicator_csv and output_csv paths\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        # detect_anomalies expects snapshot_csv and output_dir\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        # collect_data returns a DataFrame\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "scheduler.start() # Uncommented to start the scheduler\n",
        "''')\n",
        "\n",
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 4, in <module>\n",
            "    from signal_bot.indicators.ta_utils import compute_indicators\n",
            "ModuleNotFoundError: No module named 'signal_bot.indicators'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b837d4bd",
        "outputId": "eaa3b464-dd85-4e36-9cdb-ce0128931362"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directories if they don't exist\n",
        "os.makedirs('signal_bot/utils', exist_ok=True)\n",
        "\n",
        "# Write the content to the coingecko_api.py file\n",
        "with open('signal_bot/utils/coingecko_api.py', 'w') as f:\n",
        "    f.write('''# utils/coingecko_api.py\n",
        "import requests\n",
        "import pandas as pd\n",
        "import os\n",
        "from ..logger import setup_logger, log_info, log_error # Import logger from parent directory\n",
        "\n",
        "BASE_URL = \"https://api.coingecko.com/api/v3\"\n",
        "\n",
        "def get_top_coins(limit=10):\n",
        "    \"\"\"Fetches market data for top N coins.\"\"\"\n",
        "    setup_logger() # Ensure logger is setup\n",
        "    url = f\"{BASE_URL}/coins/markets\"\n",
        "    params = {\n",
        "        \"vs_currency\": \"usd\",\n",
        "        \"order\": \"market_cap_desc\",\n",
        "        \"per_page\": limit,\n",
        "        \"page\": 1,\n",
        "        \"sparkline\": \"true\"\n",
        "    }\n",
        "    log_info(f\"Fetching top {limit} coins from {url} with params {params}\")\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        log_info(\"Successfully fetched top coins.\")\n",
        "        return response.json()\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log_error(f\"Error fetching top coins: {e}\")\n",
        "        if 'response' in locals() and response is not None:\n",
        "             log_error(f\"Response status code: {response.status_code}\")\n",
        "             log_error(f\"Response text: {response.text}\")\n",
        "        return None\n",
        "\n",
        "def get_coin_history(coin_id, days=\"30\"):\n",
        "    \"\"\"Fetches historical market data for a specific coin.\"\"\"\n",
        "    setup_logger() # Ensure logger is setup\n",
        "    url = f\"{BASE_URL}/coins/{coin_id}/market_chart\"\n",
        "    # Removing \"interval\": \"hourly\" as it's for Enterprise plan\n",
        "    params = {\"vs_currency\": \"usd\", \"days\": days}\n",
        "    log_info(f\"Fetching historical data for {coin_id} from {url} with params: {params}\")\n",
        "    try:\n",
        "        response = requests.get(url, params=params)\n",
        "        response.raise_for_status() # Raise an exception for bad status codes\n",
        "        data = response.json()\n",
        "        log_info(f\"Successfully fetched historical data for {coin_id}. Response keys: {data.keys()}\")\n",
        "        return data\n",
        "    except requests.exceptions.RequestException as e:\n",
        "        log_error(f\"Error fetching historical data for {coin_id}: {e}\")\n",
        "        if 'response' in locals() and response is not None:\n",
        "             log_error(f\"Response status code: {response.status_code}\")\n",
        "             log_error(f\"Response text: {response.text}\")\n",
        "        return None''')\n",
        "\n",
        "# Now that coingecko_api.py exists, try running the scheduler again.\n",
        "# Ensure scheduler.start() is uncommented in scheduler.py.\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        # compute_indicators expects input_csv and output_csv paths\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        # find_signals expects a DataFrame\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        # log_ml_features expects indicator_csv and output_csv paths\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        # detect_anomalies expects snapshot_csv and output_dir\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        # collect_data returns a DataFrame\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "scheduler.start() # Uncommented to start the scheduler\n",
        "''')\n",
        "\n",
        "\n",
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 3, in <module>\n",
            "    from signal_bot.data_collector import collect_data\n",
            "  File \"/content/signal_bot/data_collector.py\", line 4, in <module>\n",
            "    from .utils.coingecko_api import get_top_coins # Import get_top_coins\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/signal_bot/utils/coingecko_api.py\", line 5, in <module>\n",
            "    from ..logger import setup_logger, log_info, log_error # Import logger from parent directory\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'signal_bot.logger'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1454d67d",
        "outputId": "08ba1f21-05c6-44e8-d5be-a213f50e3697"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "os.makedirs('signal_bot/data', exist_ok=True) # Ensure data directory exists\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/data_collector.py', 'w') as f:\n",
        "    f.write('''# signal_bot/data_collector.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from .utils.coingecko_api import get_top_coins # Import get_top_coins\n",
        "from .logger import setup_logger, log_info, log_error # Import logger\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def collect_data(limit=250):\n",
        "    \"\"\"\n",
        "    Fetches a snapshot of market data for a specified number of top coins.\n",
        "\n",
        "    Args:\n",
        "        limit (int): The number of top coins to fetch data for.\n",
        "\n",
        "    Returns:\n",
        "        pd.DataFrame or None: DataFrame with market data if successful, None otherwise.\n",
        "    \"\"\"\n",
        "    setup_logger() # Ensure logger is setup within the function\n",
        "    log_info(f\"Fetching market data for top {limit} coins...\")\n",
        "    try:\n",
        "        data = get_top_coins(limit=limit)\n",
        "        if data is not None:\n",
        "            df = pd.DataFrame(data)\n",
        "            df[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "            full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "            df.to_csv(full_snapshot_path, index=False)\n",
        "            log_info(f\"Full market snapshot fetched and saved to {full_snapshot_path}.\")\n",
        "            return df\n",
        "        else:\n",
        "            log_error(\"Failed to fetch market data for full snapshot.\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error during market data collection: {e}\")\n",
        "        return None\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    # Example of how to run the data collector as a script\n",
        "    collect_data(limit=250) # Fetch data for top 250 coins for the snapshot\n",
        "''')\n",
        "\n",
        "# Now that data_collector.py exists, try running the scheduler again.\n",
        "# Modify the scheduler.py file to ensure scheduler.start() is uncommented.\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "scheduler.start() # Uncommented to start the scheduler\n",
        "''')\n",
        "\n",
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 3, in <module>\n",
            "    from signal_bot.data_collector import collect_data\n",
            "  File \"/content/signal_bot/data_collector.py\", line 4, in <module>\n",
            "    from .utils.coingecko_api import get_top_coins # Import get_top_coins\n",
            "    ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "ModuleNotFoundError: No module named 'signal_bot.utils'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "257660a9",
        "outputId": "87823a08-7810-44de-e0c8-fc923e64ec79"
      },
      "source": [
        "import os\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the file, uncommenting scheduler.start()\n",
        "with open('signal_bot/scheduler.py', 'w') as f:\n",
        "    f.write('''# scheduler.py\n",
        "from apscheduler.schedulers.blocking import BlockingScheduler\n",
        "from signal_bot.data_collector import collect_data\n",
        "from signal_bot.indicators.ta_utils import compute_indicators\n",
        "from signal_bot.signals.signal_finder import find_signals\n",
        "from signal_bot.ml_logger import log_ml_features\n",
        "from signal_bot.anomaly_detector import detect_anomalies\n",
        "import pandas as pd\n",
        "import os\n",
        "from signal_bot.logger import setup_logger, log_info, log_error\n",
        "\n",
        "\n",
        "scheduler = BlockingScheduler()\n",
        "\n",
        "@scheduler.scheduled_job('interval', minutes=10)\n",
        "def pipeline_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running pipeline job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    top10_input_path = os.path.join(DATA_DIR, \"top10_market_data.csv\")\n",
        "    top10_indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "    ml_log_output_path = os.path.join(DATA_DIR, \"ml_training.csv\")\n",
        "    top10_signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "\n",
        "    if not os.path.exists(top10_input_path):\n",
        "        log_info(f\"Warning: {top10_input_path} not found. Skipping pipeline job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Computing indicators for top 10...\")\n",
        "        df_ind_top10 = compute_indicators(top10_input_path, top10_indicators_path)\n",
        "        log_info(\"Indicators computed for top 10.\")\n",
        "\n",
        "        log_info(\"Generatingsignals for top 10...\")\n",
        "        df_signals_top10 = find_signals(df_ind_top10.copy())\n",
        "        df_signals_top10.to_csv(top10_signals_path, index=False)\n",
        "        log_info(\"Signals generated for top 10.\")\n",
        "\n",
        "        log_info(\"Logging ML features for top 10...\")\n",
        "        log_ml_features(top10_indicators_path, ml_log_output_path)\n",
        "        log_info(\"ML features logged for top 10.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in pipeline job: {e}\")\n",
        "\n",
        "    log_info(\"Pipeline job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def anomaly_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running anomaly job...\")\n",
        "\n",
        "    DATA_DIR = 'signal_bot/data'\n",
        "    full_snapshot_path = os.path.join(DATA_DIR, \"full_market_snapshot.csv\")\n",
        "    anomalies_output_dir = DATA_DIR\n",
        "\n",
        "    if not os.path.exists(full_snapshot_path):\n",
        "        log_info(f\"Warning: {full_snapshot_path} not found. Skipping anomaly job.\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        log_info(\"Running Anomaly Detection...\")\n",
        "        path, anomalies = detect_anomalies(full_snapshot_path, anomalies_output_dir)\n",
        "        log_info(f\"Anomaly detection completed. Anomalies saved to {path}. Found {len(anomalies)} anomalies.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in anomaly job: {e}\")\n",
        "\n",
        "    log_info(\"Anomaly job finished.\")\n",
        "\n",
        "\n",
        "@scheduler.scheduled_job('interval', hours=1)\n",
        "def full_data_collection_job():\n",
        "    setup_logger()\n",
        "    log_info(\"Running full data collection job...\")\n",
        "    try:\n",
        "        collected_df = collect_data(limit=250)\n",
        "\n",
        "        if collected_df is not None:\n",
        "             log_info(\"Full market data collection completed.\")\n",
        "        else:\n",
        "             log_error(\"Full market data collection failed.\")\n",
        "\n",
        "    except Exception as e:\n",
        "        log_error(f\"Error in full data collection job: {e}\")\n",
        "\n",
        "    log_info(\"Full data collection job finished.\")\n",
        "\n",
        "\n",
        "print(\"Starting scheduler...\")\n",
        "scheduler.start() # Uncommented to start the scheduler\n",
        "''')\n",
        "\n",
        "# Execute the scheduler script\n",
        "!python -m signal_bot.scheduler"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 88, in _run_code\n",
            "  File \"/content/signal_bot/scheduler.py\", line 3, in <module>\n",
            "    from signal_bot.data_collector import collect_data\n",
            "ModuleNotFoundError: No module named 'signal_bot.data_collector'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "e2ba7230",
        "outputId": "33f80a24-8dd8-4bcc-ace9-cac4b0e4eae5"
      },
      "source": [
        "# Manually run the anomaly job for testing\n",
        "import os\n",
        "from signal_bot.scheduler import setup_logger, anomaly_job\n",
        "\n",
        "# Ensure logger is setup for manual runs\n",
        "setup_logger()\n",
        "\n",
        "# Run the job\n",
        "anomaly_job()"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'signal_bot.data_collector'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-3549855402.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Manually run the anomaly job for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manomaly_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Ensure logger is setup for manual runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/signal_bot/scheduler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# scheduler.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedulers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlockingScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollect_data\u001b[0m \u001b[0;31m# Import collect_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mta_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_indicators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_finder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_signals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'signal_bot.data_collector'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0b1d19eb",
        "outputId": "c06cf9c9-d420-4a0c-f13c-cbae8ef48692"
      },
      "source": [
        "# Execute data_collector.py as a module to generate the full market snapshot\n",
        "!python -m signal_bot.data_collector"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/usr/bin/python3: No module named signal_bot.data_collector\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "46c5d306",
        "outputId": "fdd74536-ce8b-4a25-c3c9-2156c92db4c2"
      },
      "source": [
        "# Manually run the full data collection job for testing\n",
        "import os\n",
        "from signal_bot.scheduler import setup_logger, full_data_collection_job\n",
        "\n",
        "# Ensure logger is setup for manual runs\n",
        "setup_logger()\n",
        "\n",
        "# Run the job\n",
        "full_data_collection_job()"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "error",
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'signal_bot.data_collector'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2751138549.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Manually run the full data collection job for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msetup_logger\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfull_data_collection_job\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Ensure logger is setup for manual runs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/signal_bot/scheduler.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# scheduler.py\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mapscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mschedulers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblocking\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mBlockingScheduler\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata_collector\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcollect_data\u001b[0m \u001b[0;31m# Import collect_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindicators\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mta_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcompute_indicators\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msignal_bot\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignal_finder\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfind_signals\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'signal_bot.data_collector'",
            "",
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"
          ],
          "errorDetails": {
            "actions": [
              {
                "action": "open_url",
                "actionText": "Open Examples",
                "url": "/notebooks/snippets/importing_libraries.ipynb"
              }
            ]
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19f48871",
        "outputId": "0ae1ae0c-fb90-4221-cc69-fe956ac9f8b5"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import glob # Import glob to find files\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "\n",
        "# --- Verify top10_signals.csv ---\n",
        "signals_path = os.path.join(DATA_DIR, \"top10_signals.csv\")\n",
        "print(f\"--- Verifying {signals_path} ---\")\n",
        "if os.path.exists(signals_path):\n",
        "    try:\n",
        "        signals_df = pd.read_csv(signals_path)\n",
        "        print(f\"{signals_path} head:\")\n",
        "        display(signals_df.head().to_markdown(index=False))\n",
        "        print(f\"\\n{signals_path} info:\")\n",
        "        signals_df.info()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or displaying {signals_path}: {e}\")\n",
        "else:\n",
        "    print(f\"File not found: {signals_path}\")\n",
        "\n",
        "# --- Verify latest anomalies_*.csv ---\n",
        "print(\"\\n--- Verifying latest anomalies_*.csv ---\")\n",
        "anomalies_dir = DATA_DIR\n",
        "anomaly_files = glob.glob(os.path.join(anomalies_dir, 'anomalies_*.csv'))\n",
        "anomaly_files.sort(reverse=True) # Sort to get the latest first\n",
        "\n",
        "if anomaly_files:\n",
        "    latest_anomaly_file = anomaly_files[0]\n",
        "    print(f\"Latest anomaly file found: {latest_anomaly_file}\")\n",
        "    try:\n",
        "        anomalies_df = pd.read_csv(latest_anomaly_file)\n",
        "        print(f\"{latest_anomaly_file} head:\")\n",
        "        display(anomalies_df.head().to_markdown(index=False))\n",
        "        print(f\"\\n{latest_anomaly_file} info:\")\n",
        "        anomalies_df.info()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or displaying {latest_anomaly_file}: {e}\")\n",
        "else:\n",
        "    print(f\"No anomaly files found in {anomalies_dir}.\")\n",
        "\n",
        "# --- Verify top10_with_indicators.csv ---\n",
        "print(\"\\n--- Verifying top10_with_indicators.csv ---\")\n",
        "indicators_path = os.path.join(DATA_DIR, \"top10_with_indicators.csv\")\n",
        "if os.path.exists(indicators_path):\n",
        "    try:\n",
        "        indicators_df = pd.read_csv(indicators_path)\n",
        "        print(f\"{indicators_path} head:\")\n",
        "        display(indicators_df.head().to_markdown(index=False))\n",
        "        print(f\"\\n{indicators_path} info:\")\n",
        "        indicators_df.info()\n",
        "    except Exception as e:\n",
        "        print(f\"Error reading or displaying {indicators_path}: {e}\")\n",
        "else:\n",
        "    print(f\"File not found: {indicators_path}\")"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Verifying signal_bot/data/top10_signals.csv ---\n",
            "File not found: signal_bot/data/top10_signals.csv\n",
            "\n",
            "--- Verifying latest anomalies_*.csv ---\n",
            "No anomaly files found in signal_bot/data.\n",
            "\n",
            "--- Verifying top10_with_indicators.csv ---\n",
            "File not found: signal_bot/data/top10_with_indicators.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b901a5d9",
        "outputId": "7036c347-c5de-441e-a17b-43e2e724da9c"
      },
      "source": [
        "# Step 2: Run the main pipeline to generate data files\n",
        "# This will fetch data, compute indicators, generate signals, backtest, and log\n",
        "print(\"Running main pipeline...\")\n",
        "!python -m signal_bot.main\n",
        "print(\"Main pipeline execution completed.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running main pipeline...\n",
            "Traceback (most recent call last):\n",
            "  File \"<frozen runpy>\", line 189, in _run_module_as_main\n",
            "  File \"<frozen runpy>\", line 159, in _get_module_details\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1133, in get_code\n",
            "  File \"<frozen importlib._bootstrap_external>\", line 1063, in source_to_code\n",
            "  File \"<frozen importlib._bootstrap>\", line 488, in _call_with_frames_removed\n",
            "  File \"/content/signal_bot/main.py\", line 188\n",
            "    print(\"\n",
            "          ^\n",
            "SyntaxError: unterminated string literal (detected at line 188)\n",
            "Main pipeline execution completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5df3f104",
        "outputId": "83fe9bb9-f1e5-4498-b76f-44f9eba050de"
      },
      "source": [
        "# Step 1: Install dependencies (including streamlit)\n",
        "# Ensure required libraries are installed\n",
        "!pip install -q -r signal_bot/requirements.txt\n",
        "print(\"Dependencies installed.\")\n",
        "\n",
        "# Optional: Verify streamlit installation\n",
        "!pip show streamlit"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'signal_bot/requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0mDependencies installed.\n",
            "\u001b[33mWARNING: Package(s) not found: streamlit\u001b[0m\u001b[33m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ff6e1b3a"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/main.py', 'w') as f:\n",
        "    f.write('''# main.py\n",
        "import pandas as pd\n",
        "import os\n",
        "from utils.coingecko_api import get_top_coins\n",
        "from indicators.ta_utils import compute_indicators\n",
        "from signals.generate_signals import generate_signal\n",
        "\n",
        "DATA_DIR = 'signal_bot/data'\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "def main():\n",
        "    print(\"Fetching top 10 coin data...\")\n",
        "    data = get_top_coins()\n",
        "    df = pd.DataFrame(data)\n",
        "    df[\"timestamp\"] = pd.Timestamp.utcnow()\n",
        "    df.to_csv(f\"{DATA_DIR}/top10_market_data.csv\", index=False)\n",
        "    print(\"Data fetched and saved.\")\n",
        "\n",
        "    print(\"Computing technical indicators...\")\n",
        "    # For now, we'll use the current price as 'close' for indicator calculation\n",
        "    # In a real scenario, we'd fetch historical data for proper indicator calculation\n",
        "    df[\"close\"] = df[\"current_price\"]\n",
        "    df_ind = compute_indicators(df.copy())\n",
        "    df_ind.to_csv(f\"{DATA_DIR}/top10_with_indicators.csv\", index=False)\n",
        "    print(\"Indicators computed and saved.\")\n",
        "\n",
        "    print(\"Generating signals...\")\n",
        "    # Note: The current generate_signal function expects a single row or a dataframe\n",
        "    # with sufficient historical data. For this initial test with only current data,\n",
        "    # the signals will likely be 'HOLD' or based on limited info.\n",
        "    df_signals = generate_signal(df_ind) # This needs adjustment for processing multiple rows\n",
        "    df_signals.to_csv(f\"{DATA_DIR}/signals.csv\", index=False)\n",
        "    print(\"Signals generated and saved.\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()\n",
        "''')"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1437504"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot/signals', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/signals/generate_signals.py', 'w') as f:\n",
        "    f.write('''# signals/generate_signals.py\n",
        "def generate_signal(df):\n",
        "    signals = []\n",
        "\n",
        "    if df['rsi'].iloc[-1] < 30:\n",
        "        signals.append(\"BUY (RSI oversold)\")\n",
        "\n",
        "    if df['macd'].iloc[-1] > 0 and df['macd'].iloc[-2] < 0:\n",
        "        signals.append(\"BUY (MACD crossover)\")\n",
        "\n",
        "    if df['close'].iloc[-1] > df['ema_20'].iloc[-1]:\n",
        "        signals.append(\"UPTREND\")\n",
        "\n",
        "    return signals''')"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "82bef2a7"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot/indicators', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/indicators/ta_utils.py', 'w') as f:\n",
        "    f.write('''# indicators/ta_utils.py\n",
        "import pandas as pd\n",
        "import ta  # pip install ta\n",
        "\n",
        "def compute_indicators(df):\n",
        "    df = df.copy()\n",
        "    df['rsi'] = ta.momentum.RSIIndicator(df['close']).rsi()\n",
        "    df['ema_20'] = ta.trend.EMAIndicator(df['close'], window=20).ema_indicator()\n",
        "    df['macd'] = ta.trend.MACD(df['close']).macd_diff()\n",
        "    bb = ta.volatility.BollingerBands(df['close'], window=20)\n",
        "    df['bb_upper'] = bb.bollinger_hband()\n",
        "    df['bb_lower'] = bb.bollinger_lband()\n",
        "    return df''')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bb5d6f36"
      },
      "source": [
        "# Create the directory if it doesn't exist\n",
        "import os\n",
        "os.makedirs('signal_bot/utils', exist_ok=True)\n",
        "\n",
        "# Write the content to the file\n",
        "with open('signal_bot/utils/coingecko_api.py', 'w') as f:\n",
        "    f.write('''# utils/coingecko_api.py\n",
        "import requests\n",
        "\n",
        "BASE_URL = \"https://api.coingecko.com/api/v3\"\n",
        "\n",
        "def get_top_coins(limit=10):\n",
        "    url = f\"{BASE_URL}/coins/markets\"\n",
        "    params = {\n",
        "        \"vs_currency\": \"usd\",\n",
        "        \"order\": \"market_cap_desc\",\n",
        "        \"per_page\": limit,\n",
        "        \"page\": 1,\n",
        "        \"sparkline\": \"true\"\n",
        "    }\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json()\n",
        "\n",
        "def get_coin_history(coin_id, days=\"30\"):\n",
        "    url = f\"{BASE_URL}/coins/{coin_id}/market_chart\"\n",
        "    params = {\"vs_currency\": \"usd\", \"days\": days, \"interval\": \"hourly\"}\n",
        "    response = requests.get(url, params=params)\n",
        "    return response.json()''')"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "30cc63b7"
      },
      "source": [
        "import os\n",
        "\n",
        "project_dirs = [\n",
        "    'signal_bot/data',\n",
        "    'signal_bot/indicators',\n",
        "    'signal_bot/models',\n",
        "    'signal_bot/signals',\n",
        "    'signal_bot/utils'\n",
        "]\n",
        "\n",
        "for dir_path in project_dirs:\n",
        "    os.makedirs(dir_path, exist_ok=True)\n",
        "\n",
        "# Create an empty requirements.txt for now\n",
        "with open('signal_bot/requirements.txt', 'w') as f:\n",
        "    pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98f4cb39"
      },
      "source": [
        "# Task\n",
        "Integrate the provided Python code snippets for fetching CoinGecko data, computing technical indicators, and generating basic trading signals into a single executable script to test the core functionality before adding machine learning capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "233ced92"
      },
      "source": [
        "## Create a main script\n",
        "\n",
        "### Subtask:\n",
        "Create a new Python file, likely `main.py`, to orchestrate the process.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7f7d4435"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the `main.py` file and write the initial structure, including necessary imports and the main execution block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9e08d76e"
      },
      "source": [
        "## Import necessary modules\n",
        "\n",
        "### Subtask:\n",
        "Import the functions from `coingecko_api.py`, `ta_utils.py`, and `generate_signals.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "adaffefc"
      },
      "source": [
        "**Reasoning**:\n",
        "Import the necessary functions from the created Python files into the main script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "901fabcf"
      },
      "source": [
        "## Fetch historical data\n",
        "\n",
        "### Subtask:\n",
        "Fetch historical data for a specific coin using the `get_coin_history` function from `coingecko_api.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b0ddf5a1"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the main function to fetch historical data for a selected coin and convert it to a DataFrame suitable for indicator calculation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a5e4779f"
      },
      "source": [
        "**Reasoning**:\n",
        "Correct the syntax error in the string literal by escaping the apostrophe in \"prices\".\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21485d41"
      },
      "source": [
        "## Compute technical indicators\n",
        "\n",
        "### Subtask:\n",
        "Compute technical indicators on the historical data using the `compute_indicators` function from `ta_utils.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "840a4898"
      },
      "source": [
        "## Generate signals\n",
        "\n",
        "### Subtask:\n",
        "Generate signals on the historical data using the `generate_signal` function from `signals.generate_signals.py`.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42b6c37a"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `signal_bot/main.py` file to generate signals using the `history_df_ind` DataFrame and save the output to a new file indicating historical signals. I will locate the section where signals are generated and update it to use `history_df_ind` and change the output filename.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83891e51"
      },
      "source": [
        "## Display results\n",
        "\n",
        "### Subtask:\n",
        "Display the generated signals along with the relevant data.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7d7f7776"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the main.py file to display the head of the historical signals dataframe.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "34741b43"
      },
      "source": [
        "## Refine and test\n",
        "\n",
        "### Subtask:\n",
        "Refine the code in `signal_bot/main.py` and test its execution to ensure it runs without errors and produces the expected output.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "accb313a"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the `signal_bot/main.py` script to test the integration of the different components and observe the output and generated files.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f0d5cb21"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that `get_coin_history` is not defined in `main.py`. I need to add the import for this function from `utils.coingecko_api`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f231f77"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that `get_coin_history` is imported, I need to execute the script again to check if the previous error is resolved and if the rest of the script runs as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dcbfab1"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates a `KeyError: 'prices'`, meaning the `history_data` dictionary returned by `get_coin_history` does not contain a key named 'prices'. I need to inspect the structure of `history_data` to understand what keys are available and how to access the price information. I will add a print statement to the `main` function to inspect the keys.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "de49225c"
      },
      "source": [
        "**Reasoning**:\n",
        "Now that I've added the print statement to inspect the keys of `history_data`, I need to execute the script again to see the output and identify the correct key for the price data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1a74fb82"
      },
      "source": [
        "**Reasoning**:\n",
        "The output shows that `history_data` only contains the key 'status', indicating that the call to `get_coin_history` failed to return the historical price data. This could be due to an issue with the API call itself. I need to investigate why the API call might be failing. I will remove the unnecessary print statement and add a check for the 'prices' key before attempting to create the DataFrame to handle cases where the API call is unsuccessful.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2068681e"
      },
      "source": [
        "**Reasoning**:\n",
        "I have added a check for the 'prices' key in `history_data`. Now I need to execute the script again to see if the error is handled and if the script can proceed or exit gracefully.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f83c182b"
      },
      "source": [
        "**Reasoning**:\n",
        "The script now correctly identifies that historical data is not available but exits the `main` function prematurely. The issue is likely with the `get_coin_history` function or the CoinGecko API itself at this moment. Since the primary goal of this subtask is to refine the code and test its execution flow with the provided components, and the current API response prevents further processing of historical data, I will proceed by focusing on the top 10 coin data, which was fetched successfully. I will modify the script to ensure the signal generation and display parts for the historical data are skipped when historical data is not available, allowing the script to complete the processing of the top 10 coin data. I will remove the previous check and instead wrap the historical data processing in a try-except block to handle the potential `KeyError` more robustly and continue with the top 10 data processing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1900f2c7"
      },
      "source": [
        "## Refine and test retry 1\n",
        "\n",
        "### Subtask:\n",
        "Refine the code in `signal_bot/main.py` and test its execution to ensure it runs without errors and processes at least the top 10 coin data, even if historical data fetching fails.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "09cade20"
      },
      "source": [
        "**Reasoning**:\n",
        "Open the `signal_bot/main.py` file, locate the historical data processing block, wrap it in a try-except block to handle potential errors, ensure `history_df_signals` is initialized in the except block, and add print statements to confirm signal generation and display the head of the dataframes. Finally, save the modified file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "761c398a"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the modified `signal_bot/main.py` script to test the implemented error handling for historical data fetching and processing, and to verify that top 10 data processing and signal generation still occur.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74d67c5f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution showed that the historical data fetching failed with \"Error: Historical data not available.\". The current code in `main.py` exits the `main` function if historical data is not available, preventing the processing of top 10 data and signal generation for the top 10. The subtask requires ensuring that top 10 data is processed even if historical data fetching fails. I need to modify the `main` function to remove the `return` statement within the historical data check and ensure that the code for processing the top 10 data and generating its signals is executed regardless of the historical data fetching outcome. I also need to adjust the try-except block to cover the historical data processing part correctly. Finally, I will execute the script again to verify the fix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ff66653a"
      },
      "source": [
        "**Reasoning**:\n",
        "The input contains a command `code_block` followed by a block of python code.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e46e379"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution resulted in a `SyntaxError: expected 'except' or 'finally' block` on the `try:` line within the historical data processing block. This indicates an issue with the structure of the try-except block or surrounding code after the modification. I need to review the `signal_bot/main.py` file to identify and fix the syntax error. I will read the file content, find the `try:` block, and examine the indentation and structure to ensure it is correctly formed with a corresponding `except` block. I will then rewrite the file with the corrected syntax and execute it again to verify the fix.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1e6019a9"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to fix the syntax error failed because the script could not find the `except` block. This indicates an issue with the logic used to locate the `except` block within the `main` function lines. I need to re-examine the code in `signal_bot/main.py` to understand the current structure and correctly identify the `except` block. I will then rewrite the code to accurately locate the `try` and `except` blocks, extract the historical data processing code, wrap it in a correctly formatted try-except block, and insert it back into the `main` function. I will also ensure that the top 10 data processing and signal generation code remains outside this block and that print statements are added to confirm the execution flow and display the results. Finally, I will execute the script to test the changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "00d0c211"
      },
      "source": [
        "**Reasoning**:\n",
        "The script failed to find the historical data fetch or top 10 signal generation line, which is needed to define the historical processing block. This suggests that the lines identifying these sections were not found in the current content of `signal_bot/main.py`. I need to carefully re-examine the current state of the `main.py` file to identify the exact text of the lines that should mark the beginning and end of the historical data processing block and the start of the top 10 signal generation. Once I have the correct line content, I will modify the script to accurately locate these lines and proceed with wrapping the historical data processing in a try-except block, ensuring top 10 processing is outside, and adding the necessary print statements. Finally, I will execute the script to test the changes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "37e5d91a"
      },
      "source": [
        "**Reasoning**:\n",
        "The output shows the current content of `signal_bot/main.py`, which has nested `try` blocks and duplicated `except` blocks, causing the `SyntaxError`. It also shows that the script could not find the expected lines for modification, likely due to the previous failed attempts altering the file structure. I need to rewrite the `main` function to have a clear structure: fetch top 10 data, compute indicators for top 10, then a `try...except` block for fetching historical data, processing it, computing indicators, and generating signals for historical data. Outside the `try...except` block, generate signals for the top 10 data and add print statements to display the results for both. I will then execute the script to verify the fix and ensure that the top 10 data is processed even if historical data fetching fails.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "77780061"
      },
      "source": [
        "## Refine and test retry 2\n",
        "\n",
        "### Subtask:\n",
        "Refine the code in `signal_bot/main.py` and test its execution to ensure it runs without errors and processes at least the top 10 coin data, even if historical data fetching fails.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeab01f0"
      },
      "source": [
        "**Reasoning**:\n",
        "Rewrite the entire content of `signal_bot/main.py` with the corrected and robust structure as described in the instructions, including imports, the main function, fetching and processing of top 10 data, a try-except block for historical data, and print statements for results. Then, execute the script to test.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "647b241f"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous execution failed because the `generate_signal` function from `signals/generate_signals.py` returns a list of strings, not a DataFrame, which caused an error when trying to call `.to_csv()` and `.head()` on the result. The `generate_signal` function needs to be updated to return a DataFrame with the signals. I will modify the `signals/generate_signals.py` file to return a DataFrame and then re-execute `signal_bot/main.py` to test the fix and ensure the top 10 data is processed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bfc25eff"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed due to a `SyntaxError: incomplete input`. This is likely caused by using triple quotes within the triple-quoted string literal for the file content. I need to fix the syntax in the string literal for `signals/generate_signals.py` by escaping any internal triple quotes or using a different quoting mechanism if necessary. I will rewrite the code block with the corrected string and then re-execute the main script.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dba7939a"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous command failed with a `SyntaxError: invalid syntax` in `signals/generate_signals.py`. This is because the code block included a shell command (`!python signal_bot/main.py`) within the Python code that was being written to the file. Shell commands should not be included in Python scripts like this. I need to remove the shell command from the string literal that writes to `signals/generate_signals.py` and execute the shell command in a separate code block.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2e79a0e0"
      },
      "source": [
        "**Reasoning**:\n",
        "I have fixed the `signals/generate_signals.py` file. Now I need to execute `signal_bot/main.py` to test if the script runs without errors and processes the top 10 data, even if historical data fetching fails.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "340d9e17"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The `main.py` script was successfully created to orchestrate the data fetching, technical indicator computation, and signal generation process.\n",
        "*   The script successfully fetches the top 10 coin data, saves it to a CSV file, computes technical indicators on this data, and saves the result to another CSV.\n",
        "*   The script attempts to fetch historical data for a specific coin (Bitcoin), but this process consistently failed during testing due to the CoinGecko API response not containing the expected 'prices' key.\n",
        "*   Error handling (`try...except`) was implemented around the historical data processing block in `main.py` to prevent the script from crashing if historical data fetching or processing fails.\n",
        "*   The `generate_signal` function was updated to ensure it returns a pandas DataFrame containing only the generated signals.\n",
        "*   The script now successfully runs without errors, processing the top 10 coin data and gracefully handling the historical data fetching failure.\n",
        "*   The heads of both the historical signals DataFrame (which is empty when fetching fails) and the top 10 snapshot signals DataFrame are printed to the console, confirming the execution flow and generated output for the top 10 data.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Investigate the CoinGecko API `get_coin_history` function call or API key (if applicable) to understand why the 'prices' key is missing, as this is crucial for historical analysis.\n",
        "*   Further refine the `generate_signal` function to produce meaningful signals from both historical time-series data and the single-row snapshot data, considering the limitations of using only current price for indicators on the top 10 snapshot.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "18ecd371"
      },
      "source": [
        "# Task\n",
        "Integrate the provided Python code snippets for a CoinGecko signal bot, including modules for data fetching, technical indicators, and basic signal generation. Ensure the code is functional and ready for testing before incorporating machine learning capabilities."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2f52994f"
      },
      "source": [
        "## Investigate coingecko api `get coin history`\n",
        "\n",
        "### Subtask:\n",
        "Investigate the CoinGecko API `get_coin_history` function call or API key (if applicable) to understand why the 'prices' key is missing, as this is crucial for historical analysis.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bc6ea5d1"
      },
      "source": [
        "**Reasoning**:\n",
        "I will examine the `get_coin_history` function in `signal_bot/utils/coingecko_api.py` and the relevant CoinGecko API documentation, and add more robust error handling and logging to the function to help diagnose why the 'prices' key might be missing.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5b9dc482"
      },
      "source": [
        "**Reasoning**:\n",
        "The error message from the previous execution clearly indicates that the `interval=hourly` parameter in the `get_coin_history` function is only available for Enterprise plan customers. The CoinGecko API documentation tips suggest that hourly data can be retrieved automatically for 'days' values between 2 and 90 without specifying the interval. I need to modify the `get_coin_history` function to remove the `interval='hourly'` parameter and update the `main.py` script to handle the case where `get_coin_history` returns `None` due to an API error, preventing the \"argument of type 'NoneType' is not iterable\" error. I will also try fetching historical data for a different coin to further investigate.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61fe8fd7"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates a `SyntaxError: 'continue' not properly in loop`. This is because the `continue` statement was added inside an `if` block within a `try` block, but not within a loop construct. The intention was to skip the rest of the historical data processing block when fetching fails. Instead of using `continue`, I should structure the code to only execute the processing steps if `history_data` is not `None`. I will modify `main.py` to remove the incorrect `continue` and wrap the historical data processing logic in an `if history_data is not None:` block. I will also revert the coin ID back to \"bitcoin\" for consistency in testing the historical data fetching mechanism itself before introducing variations.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7e1a8759"
      },
      "source": [
        "**Reasoning**:\n",
        "The `IndentationError` indicates that the line `print(\"Historical data fetching failed. Skipping historical data processing.\")` is not properly indented within the `if history_data is not None:` block. My previous attempt to modify `main.py` to wrap the historical data processing in an `if` block introduced incorrect indentation. I need to fix the indentation in `main.py` to ensure that the code within the `if history_data is not None:` block is correctly indented and the code outside of it maintains its original indentation. I will rewrite the relevant section of the `main.py` file to ensure correct indentation and logic for handling the `None` case from `get_coin_history`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "98edbecd"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   The initial attempt to fetch historical data using `get_coin_history` failed with a `401 Client Error: Unauthorized`.\n",
        "*   The error message indicated that the parameter `interval=hourly` is restricted to CoinGecko Enterprise plan customers.\n",
        "*   Removing the `interval='hourly'` parameter from the `get_coin_history` function resolved the API error and allowed the historical data, including the crucial 'prices' key, to be successfully retrieved.\n",
        "*   Initial attempts to implement error handling for a failed `get_coin_history` call in `main.py` led to `SyntaxError` and `IndentationError`.\n",
        "*   Correctly restructuring the error handling logic in `main.py` allowed the script to gracefully handle cases where historical data fetching might fail while still processing other data.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   Ensure that API calls adhere to the restrictions of the chosen plan (e.g., free vs. enterprise) to avoid unexpected errors.\n",
        "*   Refine the error handling and logging mechanisms in the `main.py` script to provide more specific information about which part of the data processing pipeline failed.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4efecf1d"
      },
      "source": [
        "# Task\n",
        "Integrate the provided Python code snippets for a CoinGecko signal bot, including data fetching, technical indicator calculation, anomaly detection, and ML feature logging. Refine the signal generation logic and implement a dedicated signal finder module. Explore using a larger window for indicator calculation and add logging to the main script. The code should be structured into modular files as described in the initial plan."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ac2a7ab4"
      },
      "source": [
        "## Refine the `generate signal` function\n",
        "\n",
        "### Subtask:\n",
        "Refine the `generate_signal` function to produce more meaningful signals based on the technical indicators, especially for time-series historical data where indicator values change over time.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "da96b0b7"
      },
      "source": [
        "**Reasoning**:\n",
        "I need to modify the `generate_signal` function in `signal_bot/signals/generate_signals.py` to iterate through the DataFrame and apply signal logic based on technical indicators, returning a DataFrame with signals.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "74825160"
      },
      "source": [
        "**Reasoning**:\n",
        "The `generate_signal` function has been updated. Now I need to test it by executing the main script, which will call this function with historical data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8579fef"
      },
      "source": [
        "**Reasoning**:\n",
        "The output indicates that the `generate_signal` function is returning \"HOLD\" signals for both top 10 snapshot data and historical data, with a warning \"Insufficient data or missing columns for signal generation.\" This suggests that the DataFrame passed to `generate_signal` either has missing required columns or is empty, even though previous steps indicate data was fetched and indicators computed. I need to inspect the DataFrames (`df_ind_top10` and `history_df_ind`) being passed to `generate_signal` in `main.py` to confirm they contain the expected columns and data after indicator computation. I will add print statements in `main.py` before calling `generate_signal` to display the columns and a few rows of these DataFrames.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7c1804bd"
      },
      "source": [
        "**Reasoning**:\n",
        "The print statements show that `df_ind_top10` has all the required columns including technical indicators, but `history_df_ind` has `NaN` values for all technical indicators in the head, and the warning indicates \"Insufficient data or missing columns\". This confirms that the issue is not missing columns, but likely insufficient data points to compute the indicators, specifically for the window size used (20 for EMA, MACD, and Bollinger Bands, and 14 for RSI by default). The historical data fetched is for 30 days, and hourly data is being requested (although the 'interval' parameter was removed, the API might default to hourly for this range). To compute indicators with a window of 20, at least 20 data points are needed. The historical data likely starts with fewer than 20 points with valid indicator values due to the rolling window calculation. The top 10 snapshot data only has one row, which is insufficient for any indicator calculation requiring a window greater than 1.\n",
        "\n",
        "To address this, I need to:\n",
        "1. Acknowledge in `generate_signals.py` that initial rows might have NaN values and the signal generation logic should handle this or start from an index where indicators are valid.\n",
        "2. Explicitly handle the case of insufficient data points for indicator calculation in `generate_signals.py` to avoid the misleading \"missing columns\" warning.\n",
        "3. Accept that for the single-row top 10 snapshot data, meaningful signals based on time-series indicators cannot be generated, and it's appropriate to return \"HOLD\" or a specific message.\n",
        "\n",
        "I will modify `generate_signals.py` to handle NaN values in indicators by checking for them explicitly before applying signal logic and refine the insufficient data check.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cdba9fee"
      },
      "source": [
        "**Reasoning**:\n",
        "I have updated the `generate_signal` function to handle NaN values and insufficient data. Now I need to execute the main script again to test these changes and see if signals are generated for the historical data where indicators are valid.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "739277da"
      },
      "source": [
        "**Reasoning**:\n",
        "The output still shows \"Insufficient data or missing columns for signal generation\" and \"HOLD\" signals, despite the inspection showing that `history_df_ind` has the necessary columns. The warning message likely means the DataFrame passed to `generate_signal` is either empty or has entirely NaN values for the required indicator columns after filtering, or the length check is too strict. However, the head of `history_df_ind` clearly shows some data rows, even if indicators are NaN initially. The check for `len(df) < min_data_points` might be causing the function to exit prematurely for the historical data. I will modify `generate_signal` to remove the strict length check at the beginning and rely solely on dropping NaNs to find the first valid index for signal computation.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "93c6ea0d"
      },
      "source": [
        "**Reasoning**:\n",
        "I have updated the `generate_signal` function to handle NaN values and remove the strict length check. Now I need to execute the main script again to test these changes and see if signals are generated for the historical data starting from the first row with valid indicators.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2580a194"
      },
      "source": [
        "**Reasoning**:\n",
        "The output still shows the \"Insufficient data or missing columns\" warning and all \"HOLD\" signals, which is unexpected given the modifications. The warning message in `generate_signal` needs to be more specific to understand if it's an empty DataFrame, missing columns, or if all indicator values are NaN after filtering. I will refine the warning message in `generate_signal` to pinpoint the exact condition causing the function to return \"HOLD\". This will help diagnose why signals are not being generated for the historical data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ff6fa05"
      },
      "source": [
        "## Implement the `signal finder.py` module\n",
        "\n",
        "### Subtask:\n",
        "Implement the `signal finder.py` module to house more complex signal generation logic, potentially incorporating multiple indicator conditions and strategies.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d96da846"
      },
      "source": [
        "**Reasoning**:\n",
        "Create the `signal_finder.py` file and define the `find_signals` function with basic structure and imports.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54b1a073"
      },
      "source": [
        "# Task\n",
        "Build a highly efficient, precise signal bot for CoinGecko with ML capabilities, integrating data collection, technical indicator calculation, signal generation, anomaly detection, ML model training, and backtesting into a scheduled workflow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8a2dd48d"
      },
      "source": [
        "## Implement full data collection\n",
        "\n",
        "### Subtask:\n",
        "Implement full data collection: Fetch and save the `full_market_snapshot.csv` file required for the anomaly detection job.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b562e99a"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the main function in signal_bot/main.py to fetch a larger market snapshot using get_top_coins with a higher limit and save it to full_market_snapshot.csv.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e19aa466"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the main script to test if the full market snapshot is fetched and saved correctly, and if the rest of the pipeline runs as expected.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f4915645"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates a `SyntaxError: unterminated string literal` in `signal_bot/main.py` at line 153. This suggests an issue with a print statement or string within that line or the surrounding lines. I need to examine the `main.py` file, identify the problematic string literal around line 153, and fix the syntax error, likely by ensuring quotes are properly matched or escaped.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4c87950f"
      },
      "source": [
        "**Reasoning**:\n",
        "The output shows the `main.py` file content is truncated, ending with an incomplete `print(\"No anoma...`. This is the cause of the `SyntaxError: unterminated string literal`. I need to rewrite the entire `main.py` content, ensuring the string literal is complete and correctly formatted, including closing quotes for all print statements.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f7229b14"
      },
      "source": [
        "**Reasoning**:\n",
        "The input text contains a Python code block with the command `!python -m signal_bot.main`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c1d30ee9"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback still shows a `SyntaxError: unterminated string literal` at line 153 in `main.py`, even after rewriting the file content. This is unexpected and suggests the previous rewrite did not correctly fix the string literal or there's a persistent issue. I will attempt to rewrite the `main.py` content again, paying close attention to all string literals, especially around the `print` statements in the results display section, to ensure they are properly closed.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "52cffd74"
      },
      "source": [
        "**Reasoning**:\n",
        "The `SyntaxError` should be resolved after rewriting the `main.py` file with the complete and correctly formatted code. Now I need to execute the script again to verify that it runs without errors and that the full market snapshot is fetched and saved as required by the subtask.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "822511f3"
      },
      "source": [
        "**Reasoning**:\n",
        "The `SyntaxError: unterminated string literal` at line 153 in `main.py` persists despite multiple attempts to rewrite the file content. This is highly unusual and suggests a potential issue with how the string literal is being handled or interpreted in this environment. Since I cannot directly debug the file content line by line within this format, and rewriting the entire string literal repeatedly isn't resolving the issue, I will try a different approach to write the file content. I will write the content of `main.py` piece by piece, carefully constructing the string literal for each section and writing it to the file incrementally. This might help isolate where the string literal is being terminated prematurely.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5945ff75"
      },
      "source": [
        "# Task\n",
        "Integrate the scheduler for automated runs, refine data collector for pagination, explore ML model prediction, refine signal generation logic and backtesting, and consider production deployment for the CoinGecko signal bot."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb580a99"
      },
      "source": [
        "## Integrate the scheduler for automated runs\n",
        "\n",
        "### Subtask:\n",
        "Decide how to run the `scheduler.py` script continuously to automate the scheduled jobs (e.g., as a background process, in a dedicated environment). In a Colab notebook, this typically involves running `!python -m signal_bot.scheduler` in a cell and keeping that cell executing.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "afb71bd4"
      },
      "source": [
        "**Reasoning**:\n",
        "Modify the `signal_bot/scheduler.py` file to uncomment the `scheduler.start()` call and execute the script to start the scheduler.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0e1cfd7a"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the `signal_bot.data_collector` module was not found. I need to create the `signal_bot/data_collector.py` file with the necessary content so that the scheduler can import and use it.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6131deef"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the `signal_bot.utils` module was not found, specifically when `data_collector.py` tried to import `get_top_coins` from `signal_bot.utils.coingecko_api`. This means the `utils` directory or the `coingecko_api.py` file within it is missing or not structured correctly. I need to create the `signal_bot/utils/coingecko_api.py` file with the necessary content, including the `get_top_coins` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ad7a1c93"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the `signal_bot.logger` module was not found, specifically when `coingecko_api.py` tried to import logging functions from it. This means the `signal_bot/logger.py` file is missing. I need to create this file with the necessary logging setup functions.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b817c70b"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the `signal_bot.indicators` module was not found, specifically when `scheduler.py` tried to import `compute_indicators` from `signal_bot.indicators.ta_utils`. This means the `indicators` directory or the `ta_utils.py` file within it is missing or not structured correctly. I need to create the `signal_bot/indicators/ta_utils.py` file with the necessary content, including the `compute_indicators` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0ebedf4f"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the `signal_bot.signals` module was not found, specifically when `scheduler.py` tried to import `find_signals` from `signal_bot.signals.signal_finder`. This means the `signals` directory or the `signal_finder.py` file within it is missing or not structured correctly. I need to create the `signal_bot/signals/signal_finder.py` file with the necessary content, including the `find_signals` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5794ec23"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the `signal_bot.ml_logger` module was not found, specifically when `scheduler.py` tried to import `log_ml_features` from it. This means the `signal_bot/ml_logger.py` file is missing. I need to create this file with the necessary content, including the `log_ml_features` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6f79e471"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback indicates that the `signal_bot.anomaly_detector` module was not found, specifically when `scheduler.py` tried to import `detect_anomalies` from it. This means the `signal_bot/anomaly_detector.py` file is missing. I need to create this file with the necessary content, including the `detect_anomalies` function.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4e7b0acc"
      },
      "source": [
        "# Task\n",
        "Guide the user through setting up a development environment in Google Cloud Shell Editor by creating a GitHub repository, uploading the existing code, cloning the repository into Cloud Shell, setting up the environment, and explaining how to work on the code and push changes back to GitHub."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "499afdfb"
      },
      "source": [
        "## Create a github repository\n",
        "\n",
        "### Subtask:\n",
        "If you don't already have one for this project, create a new public or private repository on GitHub (e.g., `crypto-signal-bot`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f05a7314"
      },
      "source": [
        "## Upload your code to github\n",
        "\n",
        "### Subtask:\n",
        "Upload your code to github: Add all the project files and directories (`signal_bot/`, `.env`, etc.) to this new GitHub repository. You can do this via the GitHub web interface or using Git commands if you're familiar with them (e.g., `git clone`, `git add`, `git commit`, `git push`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cabcb714"
      },
      "source": [
        "## Open google cloud shell editor\n",
        "\n",
        "### Subtask:\n",
        "Open google cloud shell editor\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3e4b7f5"
      },
      "source": [
        "## Clone your github repository in cloud shell\n",
        "\n",
        "### Subtask:\n",
        "Clone your github repository in cloud shell: In the Cloud Shell Editor terminal, use the `git clone` command to clone your project repository from GitHub.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f5ba1d2b"
      },
      "source": [
        "**Reasoning**:\n",
        "Execute the git clone command in the Cloud Shell Editor terminal to clone the GitHub repository.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1c636f0b"
      },
      "source": [
        "## Clone your github repository in cloud shell\n",
        "\n",
        "### Subtask:\n",
        "Clone your github repository in cloud shell: In the Cloud Shell Editor terminal, use the `git clone` command to clone your project repository from GitHub.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1d3bd9f0"
      },
      "source": [
        "## Set up the environment in cloud shell\n",
        "\n",
        "### Subtask:\n",
        "Set up the environment in cloud shell: Navigate into your cloned project directory in the Cloud Shell terminal, ensure Python is installed, install project dependencies, and set up your CoinGecko API key.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f8e99896"
      },
      "source": [
        "## Work on your code in cloud shell editor\n",
        "\n",
        "### Subtask:\n",
        "Work on your code in cloud shell editor: You can now edit your project files directly within the Cloud Shell Editor's browser-based IDE. Use the integrated terminal to run your scripts (e.g., `python -m signal_bot.main`, `python -m signal_bot.scheduler`).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bb3eb4cc"
      },
      "source": [
        "**Reasoning**:\n",
        "The traceback shows a `SyntaxError: unterminated string literal` at line 189 in `signal_bot/main.py`. I need to regenerate the `main.py` file, carefully ensuring all string literals, especially multi-line strings and print statements, are correctly closed and formatted to resolve this syntax error and make the script executable.\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}